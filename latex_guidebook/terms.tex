\chapter{Term dump}

\textcolor{green}{Terms that are important but haven't been placed in the document yet}



\emph{Curse of Dimensionality} --- \r{phenomenon where the feature space becomes increasingly sparse as the number of dimensions/features is increased -- trade off between the density of instances in the feature space and the number of dimensions (number of descriptive features) \r{including too many features can paradoxically, lead to worsening of performance.} \textcolor{green}{TODO: mentioned in paper Bellman 1961}}. \r{The more data and features we have, the ``better'' we should be able to find hidden structures and patterns in the data. However, more data and features, also means training will become more difficult (there are more dimensions and and dimension relationships to explore).}

\emph{dummy variable or dummy coding} --- $0$ or $1$, constrast coding $-1$ or $1$



\emph{$e$} --- \textcolor{blue}{Euler's numbers}

\subsection{Distributions}



\emph{epoch} --- \textcolor{blue}{a complete pass through the entire training set. Every sample in the training set has been seen by the model.} 

\emph{vector} --- \textcolor{blue}{direction, and magnitude (length)}

\emph{eigenvalue} and \emph{eigenvector} --- \textcolor{blue}{`eigen' is a german word for ``belonging to'' or ``particular to''.} 