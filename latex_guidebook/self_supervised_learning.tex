\chapter{Self-Supervised Learning (SSL)}

\r{``rebranded unsupervised''?}

\r{learn from data, even without labels present}

\TD{Evaluating Self-Supervised Pretraining Without Using Labels \cite{Reed2020EvaluatingSP}}

%% contrastive losses

% TODO: this paper likely belongs elsewhere
\TD{Siamese networks \cite{bromley1994signature}}

\TD{The triplet loss: (FaceNet: A Unified Embedding for Face Recognition and Clustering) \cite{DBLP:journals/corr/SchroffKP15}}

\TD{Beyond triplet loss: a deep quadruplet network for person re-identification \cite{DBLP:journals/corr/ChenCZH17}}

\TD{Debiased Contrastive Learning \cite{Chuang2020DebiasedCL}}

\TD{Contrastive Representation Learning: A \cite{DBLP:journals/corr/abs-2010-05113}}

\TD{Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning \cite{Grill2020BootstrapYO}}

% NOTE: I don't understand why RAFT hasn't received more attention, this seems like a very good idea
\TD{Run Away From your Teacher: Understanding BYOL by a Novel Self-Supervised Approach \cite{Shi2020RunAF}}

\TD{SWAV paper ---  Unsupervised Learning of Visual Features by Contrasting Cluster Assignments \cite{Caron2020UnsupervisedLO}}

\TD{SimSiam paper [stop-gradient] --- Exploring Simple Siamese Representation Learning \cite{Chen2020ExploringSS}}

\TD{Barlow Twins: Self-Supervised Learning via Redundancy Reduction [cross-correlation matrix loss] \cite{Zbontar2021BarlowTS}}

\TD{Momentum Contrast for Unsupervised Visual Representation Learning \cite{DBLP:journals/corr/abs-1911-05722}}

\TD{SimCLR --- A Simple Framework for Contrastive Learning of Visual Representations [https://github.com/google-research/simclr] \cite{Chen2020ASF}}

\TD{Moco V2 --- Improved Baselines with Momentum Contrastive Learning \cite{Chen2020ImprovedBW}}

\TD{SimCLRv2 --- Big Self-Supervised Models are Strong Semi-Supervised Learners [https://github.com/google-research/simclr] \cite{Chen2020BigSM}}