% no free lunch
@article{wolpert1997no,
	title={No free lunch theorems for optimization},
	author={Wolpert, David H and Macready, William G and others},
	journal={IEEE transactions on evolutionary computation},
	volume={1},
	number={1},
	pages={67--82},
	year={1997}
}

%%%%% Optimizers

% adam optimizer paper
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% Nadam optimizer paper
@article{dozat2016incorporating,
	title={Incorporating nesterov momentum into adam},
	author={Dozat, Timothy},
	year={2016}
}

% adadelta optimizer
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% adagrad optimizer
@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}

% momentum optimizer
@article{qian1999momentum,
	title={On the momentum term in gradient descent learning algorithms},
	author={Qian, Ning},
	journal={Neural networks},
	volume={12},
	number={1},
	pages={145--151},
	year={1999},
	publisher={Elsevier}
}


%%%%%%%%%%%%%%%%%%% Others

% snapshot ensembles
@article{huang2017snapshot,
	title={Snapshot ensembles: Train 1, get M for free},
	author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
	journal={arXiv preprint arXiv:1704.00109},
	year={2017}
}

% cyclic learning rates
@inproceedings{smith2017cyclical,
	title={Cyclical learning rates for training neural networks},
	author={Smith, Leslie N},
	booktitle={Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
	pages={464--472},
	year={2017},
	organization={IEEE}
}

% sgd with restarts, cosine annealing
@article{loshchilov2016sgdr,
	title={SGDR: stochastic gradient descent with restarts},
	author={Loshchilov, Ilya and Hutter, Frank},
	journal={Learning},
	volume={10},
	pages={3},
	year={2016}
}

% small batch size (<32)
@article{masters2018revisiting,
	title={Revisiting Small Batch Training for Deep Neural Networks},
	author={Masters, Dominic and Luschi, Carlo},
	journal={arXiv preprint arXiv:1804.07612},
	year={2018}
}

% minibatch vs batch, small batches are better
@article{wilson2003general,
	title={The general inefficiency of batch training for gradient descent learning},
	author={Wilson, D Randall and Martinez, Tony R},
	journal={Neural Networks},
	volume={16},
	number={10},
	pages={1429--1451},
	year={2003},
	publisher={Elsevier}
}

% generalization gap, Ghost-BN, not the batch size, the number of updates
@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1729--1739},
	year={2017}
}

% increasing batch size
@article{smith2017don,
	title={Don't Decay the Learning Rate, Increase the Batch Size},
	author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
	journal={arXiv preprint arXiv:1711.00489},
	year={2017}
}

% Pooling/pooling not being used
@article{ruderman2018learned,
	title={Learned Deformation Stability in Convolutional Neural Networks},
	author={Ruderman, Avraham and Rabinowitz, Neil and Morcos, Ari S and Zoran, Daniel},
	journal={arXiv preprint arXiv:1804.04438},
	year={2018}
}

%%%%%%%%%%%%%%%%%%% vizualizations
@Misc{deeplearnjs,
	title        = {deeplearn.js. [{O}nline]},
	howpublished = {\url{https://deeplearnjs.org/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{tf_playground,
	title        = {A Neural Network Playground. [{O}nline]},
	howpublished = {\url{http://playground.tensorflow.org}},
	note         = {Accessed: 2018-06-21},
}

@Misc{convnet_js,
	title        = {ConvNetJS. [{O}nline]},
	howpublished = {\url{http://cs.stanford.edu/people/karpathy/convnetjs/}},
	note         = {Accessed: 2018-06-21},
}

%%%%%%%%%%%%%%%%%%% Cloud Providers

@Misc{cloudHW_amazon_aws,
	title        = {Amazon AWS. [{O}nline]},
	howpublished = {\url{https://aws.amazon.com/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_micro_azure,
	title        = {Microsoft Azure. [{O}nline]},
	howpublished = {\url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_google_cloud,
	title        = {Google Cloud. [{O}nline]},
	howpublished = {\url{https://cloud.google.com/gpu/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_floydhub,
	title        = {FloydHub. [{O}nline]},
	howpublished = {\url{https://www.floydhub.com/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_nvidia_cloud,
	title        = {Nvidia GPU Cloud. [{O}nline]},
	howpublished = {\url{https://www.nvidia.com/en-us/gpu-cloud/}},
	note         = {Accessed: 2018-06-21},
}

%%%%%%%% device placement
% automatic deviceplacement white paper, internal google API
@article{abadi2016tensorflow_device_placement,
	title={Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
	author={Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
	journal={arXiv preprint arXiv:1603.04467},
	year={2016}
}


% math
% derivatives
@book{griewank2008evaluating,
	title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
	author={Griewank, Andreas and Walther, Andrea},
	volume={105},
	year={2008},
	publisher={Siam}
}

% optimizer book
@book{kochenderfer2019algorithms,
	title={Algorithms for Optimization},
	author={Kochenderfer, Mykel J and Wheeler, Tim A},
	year={2019},
	publisher={Mit Press}
}

% reverse accumulation
@article{linnainmaa1970representation,
	title={The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors},
	author={Linnainmaa, Seppo},
	journal={Master's Thesis (in Finnish), Univ. Helsinki},
	pages={6--7},
	year={1970}
}

% backpropagation paper..
@article{alber2018backprop,
	title={Backprop evolution},
	author={Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
	journal={arXiv preprint arXiv:1808.02822},
	year={2018}
}


% neural network paper from 1990
@article{hansen1990neural,
	title={Neural network ensembles},
	author={Hansen, Lars Kai and Salamon, Peter},
	journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
	number={10},
	pages={993--1001},
	year={1990},
	publisher={IEEE}
}

% reverse accumulation --> backpropagation algorithm
@article{rumelhart1988learning,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
	journal={Cognitive modeling},
	volume={5},
	number={3},
	pages={1},
	year={1988}
}

% optimizer, adadelta
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% hypergradient descent, optimizer
@article{baydin2017online,
	title={Online learning rate adaptation with hypergradient descent},
	author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
	journal={arXiv preprint arXiv:1703.04782},
	year={2017}
}

% AMSGrad, optimizer
@article{reddi2019convergence,
	title={On the convergence of adam and beyond},
	author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
	journal={arXiv preprint arXiv:1904.09237},
	year={2019}
}

% augmentation
@article{cubuk2018autoaugment,
	title={Autoaugment: Learning augmentation policies from data},
	author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
	journal={arXiv preprint arXiv:1805.09501},
	year={2018}
}

% augmentation
@article{lim2019fast,
  title={Fast autoaugment},
  author={Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
  journal={arXiv preprint arXiv:1905.00397},
  year={2019}
}

% augmentation - cutout
@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

% augmentation - samplepairing
@article{inoue2018data,
  title={Data augmentation by pairing samples for images classification},
  author={Inoue, Hiroshi},
  journal={arXiv preprint arXiv:1801.02929},
  year={2018}
}

% augmentation - smart augmentation
@article{lemley2017smart,
  title={Smart augmentation learning an optimal data augmentation strategy},
  author={Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
  journal={Ieee Access},
  volume={5},
  pages={5858--5869},
  year={2017},
  publisher={IEEE}
}

% augmentation - GAN
@inproceedings{shrivastava2017learning,
  title={Learning from simulated and unsupervised images through adversarial training},
  author={Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Joshua and Wang, Wenda and Webb, Russell},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2107--2116},
  year={2017}
}

% augmentation - bayesian
@inproceedings{tran2017bayesian,
  title={A bayesian data augmentation approach for learning deep models},
  author={Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
  booktitle={Advances in neural information processing systems},
  pages={2797--2806},
  year={2017}
}

% augmentation - cutmix
@article{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  journal={arXiv preprint arXiv:1905.04899},
  year={2019}
}

% augmentation - mixup
@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

% augmentation - population based augmentation (PBA)
@article{ho2019population,
  title={Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules},
  author={Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
  journal={arXiv preprint arXiv:1905.05393},
  year={2019}
}


% augmentation - object detection
@article{zoph2019learning,
  title={Learning Data Augmentation Strategies for Object Detection},
  author={Zoph, Barret and Cubuk, Ekin D and Ghiasi, Golnaz and Lin, Tsung-Yi and Shlens, Jonathon and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.11172},
  year={2019}
}


% augmentation - NLP
@article{sennrich2015improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
  year={2015}
}


% augmentation - unsupervised
@article{xie2019unsupervised,
  title={Unsupervised data augmentation},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.12848},
  year={2019}
}


% adversarial examples - robustness:
@article{papernot2018deep,
  title={Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning},
  author={Papernot, Nicolas and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1803.04765},
  year={2018}
}

% TODO:
@article{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}

% output regularization
@article{pereyra2017regularizing,
	title={Regularizing neural networks by penalizing confident output distributions},
	author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
	journal={arXiv preprint arXiv:1701.06548},
	year={2017}
}

% output regularization
@inproceedings{szegedy2016rethinking,
	title={Rethinking the inception architecture for computer vision},
	author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2818--2826},
	year={2016}
}

% output regularization
@inproceedings{xie2016disturblabel,
	title={Disturblabel: Regularizing cnn on the loss layer},
	author={Xie, Lingxi and Wang, Jingdong and Wei, Zhen and Wang, Meng and Tian, Qi},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={4753--4762},
	year={2016}
}


% output regularization - TODO: likely in other sections -- model `smalling''
@article{hinton2015distilling,
	title={Distilling the knowledge in a neural network},
	author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	journal={arXiv preprint arXiv:1503.02531},
	year={2015}
}


% output regularization
@article{reed2014training,
	title={Training deep neural networks on noisy labels with bootstrapping},
	author={Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	journal={arXiv preprint arXiv:1412.6596},
	year={2014}
}

@article{reed2014training,
	title={Training deep neural networks on noisy labels with bootstrapping},
	author={Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	journal={arXiv preprint arXiv:1412.6596},
	year={2014}
}

@article{miyato2018virtual,
	title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
	author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={41},
	number={8},
	pages={1979--1993},
	year={2018},
	publisher={IEEE}
}


% universial approximator
@article{hornik1991approximation,
	title={Approximation capabilities of multilayer feedforward networks},
	author={Hornik, Kurt},
	journal={Neural networks},
	volume={4},
	number={2},
	pages={251--257},
	year={1991},
	publisher={Elsevier}
}


% replace pooling layers with convolutions
@article{springenberg2014striving,
	title={Striving for simplicity: The all convolutional net},
	author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	journal={arXiv preprint arXiv:1412.6806},
	year={2014}
}

% train model on own dreams
@article{ha2018world,
	title={World models},
	author={Ha, David and Schmidhuber, J{\"u}rgen},
	journal={arXiv preprint arXiv:1803.10122},
	year={2018}
}

% snapshot ensembles
@article{huang2017snapshot,
	title={Snapshot ensembles: Train 1, get m for free},
	author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
	journal={arXiv preprint arXiv:1704.00109},
	year={2017}
}


% augmentation (flips at test time)
@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}

@inproceedings{xie2017aggregated,
	title={Aggregated residual transformations for deep neural networks},
	author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={1492--1500},
	year={2017}
}

% embeddings
@article{guo2016entity,
	title={Entity embeddings of categorical variables},
	author={Guo, Cheng and Berkhahn, Felix},
	journal={arXiv preprint arXiv:1604.06737},
	year={2016}
}

% weight normalization
@inproceedings{salimans2016weight,
	title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
	author={Salimans, Tim and Kingma, Durk P},
	booktitle={Advances in Neural Information Processing Systems},
	pages={901--909},
	year={2016}
}



%%% initialization

% PRELU, Kaiming Initi
% activation functions, initialization strategies
@inproceedings{he2015delving,
	title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={1026--1034},
	year={2015}
}


% fixup initialization, initialization strategies, init
@article{zhang2019fixup,
	title={Fixup initialization: Residual learning without normalization},
	author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
	journal={arXiv preprint arXiv:1901.09321},
	year={2019}
}


% glorot initialization
@inproceedings{glorot2010understanding,
	title={Understanding the difficulty of training deep feedforward neural networks},
	author={Glorot, Xavier and Bengio, Yoshua},
	booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages={249--256},
	year={2010}
}

@incollection{lecun2012efficient,
	title={Efficient backprop},
	author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
	booktitle={Neural networks: Tricks of the trade},
	pages={9--48},
	year={2012},
	publisher={Springer}
}


% multi task
@inproceedings{zhao2019recommending,
	title={Recommending what video to watch next: a multitask ranking system},
	author={Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
	booktitle={Proceedings of the 13th ACM Conference on Recommender Systems},
	pages={43--51},
	year={2019}
}

% multi-task group tasks
@article{standley2019tasks,
	title={Which Tasks Should Be Learned Together in Multi-task Learning?},
	author={Standley, Trevor and Zamir, Amir R and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
	journal={arXiv preprint arXiv:1905.07553},
	year={2019}
}

% multi-task : possible example of brute force task weighting
@inproceedings{kirillov2019panoptic,
	title={Panoptic feature pyramid networks},
	author={Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={6399--6408},
	year={2019}
}


% black box appraoch - meta learning- external memory
@inproceedings{santoro2016meta,
	title={Meta-learning with memory-augmented neural networks},
	author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	booktitle={International conference on machine learning},
	pages={1842--1850},
	year={2016}
}


% meta-learning - blackbox permutation invariant- external memory
@article{garnelo2018conditional,
	title={Conditional neural processes},
	author={Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J and Eslami, SM},
	journal={arXiv preprint arXiv:1807.01613},
	year={2018}
}


% meta learning - external memory
@inproceedings{munkhdalai2017meta,
	title={Meta networks},
	author={Munkhdalai, Tsendsuren and Yu, Hong},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={2554--2563},
	year={2017},
	organization={JMLR. org}
}


% attention and convolution - meta learning
@article{mishra2017simple,
	title={A simple neural attentive meta-learner},
	author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1707.03141},
	year={2017}
}


%few shot learning
@inproceedings{snell2017prototypical,
	title={Prototypical networks for few-shot learning},
	author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
	booktitle={Advances in neural information processing systems},
	pages={4077--4087},
	year={2017}
}