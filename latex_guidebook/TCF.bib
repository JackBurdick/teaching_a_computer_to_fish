% no free lunch
@article{wolpert1997no,
	title={No free lunch theorems for optimization},
	author={Wolpert, David H and Macready, William G and others},
	journal={IEEE transactions on evolutionary computation},
	volume={1},
	number={1},
	pages={67--82},
	year={1997}
}

%%%%% Optimizers

% adam optimizer paper
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% Nadam optimizer paper
@article{dozat2016incorporating,
	title={Incorporating nesterov momentum into adam},
	author={Dozat, Timothy},
	year={2016}
}

% adagrad optimizer
@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}

% momentum optimizer
@article{qian1999momentum,
	title={On the momentum term in gradient descent learning algorithms},
	author={Qian, Ning},
	journal={Neural networks},
	volume={12},
	number={1},
	pages={145--151},
	year={1999},
	publisher={Elsevier}
}


%%%%%%%%%%%%%%%%%%% Others

% snapshot ensembles
@article{huang2017snapshot,
	title={Snapshot ensembles: Train 1, get M for free},
	author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
	journal={arXiv preprint arXiv:1704.00109},
	year={2017}
}

% cyclic learning rates
@inproceedings{smith2017cyclical,
	title={Cyclical learning rates for training neural networks},
	author={Smith, Leslie N},
	booktitle={Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
	pages={464--472},
	year={2017},
	organization={IEEE}
}

% sgd with restarts, cosine annealing
@article{loshchilov2016sgdr,
	title={SGDR: stochastic gradient descent with restarts},
	author={Loshchilov, Ilya and Hutter, Frank},
	journal={Learning},
	volume={10},
	pages={3},
	year={2016}
}

% small batch size (<32)
@article{masters2018revisiting,
	title={Revisiting Small Batch Training for Deep Neural Networks},
	author={Masters, Dominic and Luschi, Carlo},
	journal={arXiv preprint arXiv:1804.07612},
	year={2018}
}

% minibatch vs batch, small batches are better
@article{wilson2003general,
	title={The general inefficiency of batch training for gradient descent learning},
	author={Wilson, D Randall and Martinez, Tony R},
	journal={Neural Networks},
	volume={16},
	number={10},
	pages={1429--1451},
	year={2003},
	publisher={Elsevier}
}

% generalization gap, Ghost-BN, not the batch size, the number of updates
@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1729--1739},
	year={2017}
}

% increasing batch size
@article{smith2017don,
	title={Don't Decay the Learning Rate, Increase the Batch Size},
	author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
	journal={arXiv preprint arXiv:1711.00489},
	year={2017}
}

% Pooling/pooling not being used
@article{ruderman2018learned,
	title={Learned Deformation Stability in Convolutional Neural Networks},
	author={Ruderman, Avraham and Rabinowitz, Neil and Morcos, Ari S and Zoran, Daniel},
	journal={arXiv preprint arXiv:1804.04438},
	year={2018}
}

%%%%%%%%%%%%%%%%%%% vizualizations
@Misc{deeplearnjs,
	title        = {deeplearn.js. [{O}nline]},
	howpublished = {\url{https://deeplearnjs.org/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{tf_playground,
	title        = {A Neural Network Playground. [{O}nline]},
	howpublished = {\url{http://playground.tensorflow.org}},
	note         = {Accessed: 2018-06-21},
}

@Misc{convnet_js,
	title        = {ConvNetJS. [{O}nline]},
	howpublished = {\url{http://cs.stanford.edu/people/karpathy/convnetjs/}},
	note         = {Accessed: 2018-06-21},
}

%%%%%%%%%%%%%%%%%%% Cloud Providers

@Misc{cloudHW_amazon_aws,
	title        = {Amazon AWS. [{O}nline]},
	howpublished = {\url{https://aws.amazon.com/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_micro_azure,
	title        = {Microsoft Azure. [{O}nline]},
	howpublished = {\url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_google_cloud,
	title        = {Google Cloud. [{O}nline]},
	howpublished = {\url{https://cloud.google.com/gpu/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_nvidia_cloud,
	title        = {Nvidia GPU Cloud. [{O}nline]},
	howpublished = {\url{https://www.nvidia.com/en-us/gpu-cloud/}},
	note         = {Accessed: 2018-06-21},
}

%%%%%%%% device placement
% automatic deviceplacement white paper, internal google API
@article{abadi2016tensorflow_device_placement,
	title={Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
	author={Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
	journal={arXiv preprint arXiv:1603.04467},
	year={2016}
}


% math
% derivatives
@book{griewank2008evaluating,
	title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
	author={Griewank, Andreas and Walther, Andrea},
	volume={105},
	year={2008},
	publisher={Siam}
}

% optimizer book
@book{kochenderfer2019algorithms,
	title={Algorithms for Optimization},
	author={Kochenderfer, Mykel J and Wheeler, Tim A},
	year={2019},
	publisher={Mit Press}
}

% reverse accumulation
@article{linnainmaa1970representation,
	title={The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors},
	author={Linnainmaa, Seppo},
	journal={Master's Thesis (in Finnish), Univ. Helsinki},
	pages={6--7},
	year={1970}
}

% backpropagation paper..
@article{alber2018backprop,
	title={Backprop evolution},
	author={Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
	journal={arXiv preprint arXiv:1808.02822},
	year={2018}
}


% neural network paper from 1990
@article{hansen1990neural,
	title={Neural network ensembles},
	author={Hansen, Lars Kai and Salamon, Peter},
	journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
	number={10},
	pages={993--1001},
	year={1990},
	publisher={IEEE}
}

% reverse accumulation --> backpropagation algorithm
@article{rumelhart1988learning,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
	journal={Cognitive modeling},
	volume={5},
	number={3},
	pages={1},
	year={1988}
}

% optimizer, adadelta
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% hypergradient descent, optimizer
@article{baydin2017online,
	title={Online learning rate adaptation with hypergradient descent},
	author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
	journal={arXiv preprint arXiv:1703.04782},
	year={2017}
}

% AMSGrad, optimizer
@article{reddi2019convergence,
	title={On the convergence of adam and beyond},
	author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
	journal={arXiv preprint arXiv:1904.09237},
	year={2019}
}

% augmentation
@article{cubuk2018autoaugment,
	title={Autoaugment: Learning augmentation policies from data},
	author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
	journal={arXiv preprint arXiv:1805.09501},
	year={2018}
}

% augmentation
@article{lim2019fast,
  title={Fast autoaugment},
  author={Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
  journal={arXiv preprint arXiv:1905.00397},
  year={2019}
}

% augmentation - cutout
@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

% augmentation - samplepairing
@article{inoue2018data,
  title={Data augmentation by pairing samples for images classification},
  author={Inoue, Hiroshi},
  journal={arXiv preprint arXiv:1801.02929},
  year={2018}
}

% augmentation - smart augmentation
@article{lemley2017smart,
  title={Smart augmentation learning an optimal data augmentation strategy},
  author={Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
  journal={Ieee Access},
  volume={5},
  pages={5858--5869},
  year={2017},
  publisher={IEEE}
}

% augmentation - GAN
@inproceedings{shrivastava2017learning,
  title={Learning from simulated and unsupervised images through adversarial training},
  author={Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Joshua and Wang, Wenda and Webb, Russell},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2107--2116},
  year={2017}
}

% augmentation - bayesian
@inproceedings{tran2017bayesian,
  title={A bayesian data augmentation approach for learning deep models},
  author={Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
  booktitle={Advances in neural information processing systems},
  pages={2797--2806},
  year={2017}
}

% augmentation - cutmix
@article{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  journal={arXiv preprint arXiv:1905.04899},
  year={2019}
}

% augmentation - mixup
@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

% augmentation - population based augmentation (PBA)
@article{ho2019population,
  title={Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules},
  author={Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
  journal={arXiv preprint arXiv:1905.05393},
  year={2019}
}


% augmentation - object detection
@article{zoph2019learning,
  title={Learning Data Augmentation Strategies for Object Detection},
  author={Zoph, Barret and Cubuk, Ekin D and Ghiasi, Golnaz and Lin, Tsung-Yi and Shlens, Jonathon and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.11172},
  year={2019}
}


% augmentation - NLP
@article{sennrich2015improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
  year={2015}
}


% augmentation - unsupervised
@article{xie2019unsupervised,
  title={Unsupervised data augmentation},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.12848},
  year={2019}
}


% adversarial examples - robustness:
@article{papernot2018deep,
  title={Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning},
  author={Papernot, Nicolas and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1803.04765},
  year={2018}
}

% TODO:
@article{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}

% output regularization
@article{pereyra2017regularizing,
	title={Regularizing neural networks by penalizing confident output distributions},
	author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
	journal={arXiv preprint arXiv:1701.06548},
	year={2017}
}

% output regularization
@inproceedings{szegedy2016rethinking,
	title={Rethinking the inception architecture for computer vision},
	author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2818--2826},
	year={2016}
}

% output regularization
@inproceedings{xie2016disturblabel,
	title={Disturblabel: Regularizing cnn on the loss layer},
	author={Xie, Lingxi and Wang, Jingdong and Wei, Zhen and Wang, Meng and Tian, Qi},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={4753--4762},
	year={2016}
}


% output regularization - TODO: likely in other sections -- model `smalling''
@article{hinton2015distilling,
	title={Distilling the knowledge in a neural network},
	author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	journal={arXiv preprint arXiv:1503.02531},
	year={2015}
}


% output regularization
@article{reed2014training,
	title={Training deep neural networks on noisy labels with bootstrapping},
	author={Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	journal={arXiv preprint arXiv:1412.6596},
	year={2014}
}

@article{miyato2018virtual,
	title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
	author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={41},
	number={8},
	pages={1979--1993},
	year={2018},
	publisher={IEEE}
}


% universial approximator
@article{hornik1991approximation,
	title={Approximation capabilities of multilayer feedforward networks},
	author={Hornik, Kurt},
	journal={Neural networks},
	volume={4},
	number={2},
	pages={251--257},
	year={1991},
	publisher={Elsevier}
}


% replace pooling layers with convolutions
@article{springenberg2014striving,
	title={Striving for simplicity: The all convolutional net},
	author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	journal={arXiv preprint arXiv:1412.6806},
	year={2014}
}

% train model on own dreams
@article{ha2018world,
	title={World models},
	author={Ha, David and Schmidhuber, J{\"u}rgen},
	journal={arXiv preprint arXiv:1803.10122},
	year={2018}
}

% augmentation (flips at test time)
@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}

@inproceedings{xie2017aggregated,
	title={Aggregated residual transformations for deep neural networks},
	author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={1492--1500},
	year={2017}
}

% embeddings
@article{guo2016entity,
	title={Entity embeddings of categorical variables},
	author={Guo, Cheng and Berkhahn, Felix},
	journal={arXiv preprint arXiv:1604.06737},
	year={2016}
}

% weight normalization
@inproceedings{salimans2016weight,
	title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
	author={Salimans, Tim and Kingma, Durk P},
	booktitle={Advances in Neural Information Processing Systems},
	pages={901--909},
	year={2016}
}



%%% initialization

% PRELU, Kaiming Initi
% activation functions, initialization strategies
@inproceedings{he2015delving,
	title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={1026--1034},
	year={2015}
}


% fixup initialization, initialization strategies, init
@article{zhang2019fixup,
	title={Fixup initialization: Residual learning without normalization},
	author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
	journal={arXiv preprint arXiv:1901.09321},
	year={2019}
}


% glorot initialization
@inproceedings{glorot2010understanding,
	title={Understanding the difficulty of training deep feedforward neural networks},
	author={Glorot, Xavier and Bengio, Yoshua},
	booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages={249--256},
	year={2010}
}

@incollection{lecun2012efficient,
	title={Efficient backprop},
	author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
	booktitle={Neural networks: Tricks of the trade},
	pages={9--48},
	year={2012},
	publisher={Springer}
}


% multi task
@inproceedings{zhao2019recommending,
	title={Recommending what video to watch next: a multitask ranking system},
	author={Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
	booktitle={Proceedings of the 13th ACM Conference on Recommender Systems},
	pages={43--51},
	year={2019}
}

% multi-task group tasks
@article{standley2019tasks,
	title={Which Tasks Should Be Learned Together in Multi-task Learning?},
	author={Standley, Trevor and Zamir, Amir R and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
	journal={arXiv preprint arXiv:1905.07553},
	year={2019}
}

% multi-task : possible example of brute force task weighting
@inproceedings{kirillov2019panoptic,
	title={Panoptic feature pyramid networks},
	author={Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={6399--6408},
	year={2019}
}


% black box appraoch - meta learning- external memory
@inproceedings{santoro2016meta,
	title={Meta-learning with memory-augmented neural networks},
	author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	booktitle={International conference on machine learning},
	pages={1842--1850},
	year={2016}
}


% meta-learning - blackbox permutation invariant- external memory
@article{garnelo2018conditional,
	title={Conditional neural processes},
	author={Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J and Eslami, SM},
	journal={arXiv preprint arXiv:1807.01613},
	year={2018}
}


% meta learning - external memory
@inproceedings{munkhdalai2017meta,
	title={Meta networks},
	author={Munkhdalai, Tsendsuren and Yu, Hong},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={2554--2563},
	year={2017},
	organization={JMLR. org}
}


% attention and convolution - meta learning
@article{mishra2017simple,
	title={A simple neural attentive meta-learner},
	author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1707.03141},
	year={2017}
}


%few shot learning
@inproceedings{snell2017prototypical,
	title={Prototypical networks for few-shot learning},
	author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
	booktitle={Advances in neural information processing systems},
	pages={4077--4087},
	year={2017}
}


% pruning and lottery ticket blog structure
@article{lange2020_lottery_ticket_hypothesis,
	title   = "The Lottery Ticket Hypothesis: A Survey",
	author  = "Lange, Robert Tjarko",
	journal = "https://roberttlange.github.io/year-archive/posts/2020/06/lottery-ticket-hypothesis/",
	year    = "2020",
	url     = "https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/"
}


% survey on metric learning
@article{kaya2019deep,
	title={Deep metric learning: A survey},
	author={Kaya, Mahmut and Bilge, Hasan {\c{S}}akir},
	journal={Symmetry},
	volume={11},
	number={9},
	pages={1066},
	year={2019},
	publisher={Multidisciplinary Digital Publishing Institute}
}

% bleu metric
@inproceedings{papineni2002bleu,
	title={BLEU: a method for automatic evaluation of machine translation},
	author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
	pages={311--318},
	year={2002}
}

% pseudo-label, first proposal?
@inproceedings{lee2013pseudo,
	title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
	author={Lee, Dong-Hyun},
	booktitle={Workshop on challenges in representation learning, ICML},
	volume={3},
	number={2},
	year={2013}
}

% machine learning
@article{samuel1962artificial,
	title={Artificial intelligence: a frontier of automation},
	author={Samuel, Arthur L},
	journal={The Annals of the American Academy of Political and Social Science},
	volume={340},
	number={1},
	pages={10--20},
	year={1962},
	publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

% Generative deep learning book
@book{foster2019generative,
	title={Generative deep learning: teaching machines to paint, write, compose, and play},
	author={Foster, David},
	year={2019},
	publisher={O'Reilly Media}
}

% gan controversy
@article{schmidhuber1992learning,
	title={Learning factorial codes by predictability minimization},
	author={Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={4},
	number={6},
	pages={863--879},
	year={1992},
	publisher={MIT Press}
}

% early CNN, convolution, relu
@incollection{fukushima1982neocognitron,
	title={Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
	author={Fukushima, Kunihiko and Miyake, Sei},
	booktitle={Competition and cooperation in neural nets},
	pages={267--285},
	year={1982},
	publisher={Springer}
}

% dropout
@article{JMLR:v15:srivastava14a,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	number  = {56},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@book{geron2019hands,
	title={Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems},
	author={G{\'e}ron, Aur{\'e}lien},
	year={2019},
	publisher={O'Reilly Media}
}

@misc{cs231n, 
	title={CS231n: Convolutional Neural Networks for Visual Recognition}, 
	url={http://cs231n.stanford.edu/}
}

@inproceedings{wan2013regularization,
	title={Regularization of neural networks using dropconnect},
	author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
	booktitle={International conference on machine learning},
	pages={1058--1066},
	year={2013},
	organization={PMLR}
}

@article{krawczyk2016learning,
	title={Learning from imbalanced data: open challenges and future directions},
	author={Krawczyk, Bartosz},
	journal={Progress in Artificial Intelligence},
	volume={5},
	number={4},
	pages={221--232},
	year={2016},
	publisher={Springer}
}

% t-test, student t test, Gosset
@article{student1908probable,
	title={The probable error of a mean},
	author={Student},
	journal={Biometrika},
	pages={1--25},
	year={1908},
	publisher={JSTOR}
}


% auROC vs PR curves
@inproceedings{davis2006relationship,
	title={The relationship between Precision-Recall and ROC curves},
	author={Davis, Jesse and Goadrich, Mark},
	booktitle={Proceedings of the 23rd international conference on Machine learning},
	pages={233--240},
	year={2006}
}


% ANOVA
@incollection{fisher1992statistical,
	title={Statistical methods for research workers},
	author={Fisher, Ronald Aylmer},
	booktitle={Breakthroughs in statistics},
	pages={66--70},
	year={1992},
	publisher={Springer}
}

% box-cox transform
@article{box1964analysis,
	title={An analysis of transformations},
	author={Box, George EP and Cox, David R},
	journal={Journal of the Royal Statistical Society: Series B (Methodological)},
	volume={26},
	number={2},
	pages={211--243},
	year={1964},
	publisher={Wiley Online Library}
}

% cuped methods
@inproceedings{deng2013improving,
	title={Improving the sensitivity of online controlled experiments by utilizing pre-experiment data},
	author={Deng, Alex and Xu, Ya and Kohavi, Ron and Walker, Toby},
	booktitle={Proceedings of the sixth ACM international conference on Web search and data mining},
	pages={123--132},
	year={2013}
}

% CUPAC
@article{tangcontrol,
	title={Control Using Predictions as Covariates in Switchback Experiments},
	author={Tang, Yixin and Huang, Caixia and Kastelman, David and Bauman, Jared}
}


% entropy, information theory
@article{shannon1948mathematical,
	title={A mathematical theory of communication},
	author={Shannon, Claude E},
	journal={The Bell system technical journal},
	volume={27},
	number={3},
	pages={379--423},
	year={1948},
	publisher={Nokia Bell Labs}
}


% missing data
@book{allison2001missing,
	title={Missing data},
	author={Allison, Paul D},
	year={2001},
	publisher={Sage publications}
}

@book{lakshmanan2021practical,
	title={Practical Machine Learning for Computer Vision: End-To-End Machine Learning for Images},
	author={Lakshmanan, V. and G{\"o}rner, M. and Gillard, R.},
	isbn={9781098102364},
	url={https://books.google.com/books?id=1x5izgEACAAJ},
	year={2021},
	publisher={O'Reilly Media, Incorporated}
}

@inproceedings{power2021grokking,
	title={GROKKING: GENERALIZATION BEYOND OVERFIT-TING ON SMALL ALGORITHMIC DATASETS},
	author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	booktitle={ICLR MATH-AI Workshop},
	year={2021}
}

@article{lecun1998gradient,
	title={Gradient-based learning applied to document recognition},
	author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	journal={Proceedings of the IEEE},
	volume={86},
	number={11},
	pages={2278--2324},
	year={1998},
	publisher={Ieee}
}

% AlexNet
@article{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	journal={Advances in neural information processing systems},
	volume={25},
	pages={1097--1105},
	year={2012}
}

@inproceedings{yim2017gift,
	title={A gift from knowledge distillation: Fast optimization, network minimization and transfer learning},
	author={Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={4133--4141},
	year={2017}
}

@article{schmidhuber1990making,
	title={Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neual Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments},
	author={Schmidhuber, Jiirgen},
	year={1990}
}

@article{pal1978computer,
	title={Computer recognition of vowel sounds using a self-supervised learning algorithm},
	author={Pal, SK and Datta, AK and Majumder, D Dutta},
	journal={J. Anatomical Soc. India},
	volume={6},
	pages={117--123},
	year={1978},
	publisher={Citeseer}
}

@inproceedings{chopra2005learning,
	title={Learning a similarity metric discriminatively, with application to face verification},
	author={Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
	booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
	volume={1},
	pages={539--546},
	year={2005},
	organization={IEEE}
}

@article{weng2019selfsup,
	title={"Self-Supervised Representation Learning"},
	author={"Weng, Lilian"},
	journal={"lilianweng.github.io/lil-log"},
	year={"2019"},
	url={"https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html"}
}

@article{weng2021contrastive,
	title   = {"Contrastive Representation Learning"},
	author  = {"Weng, Lilian"},
	journal = {"lilianweng.github.io/lil-log"},
	year    = {"2021"},
	url     = {"https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html"}
}

@article{jaeger2001echo,
	title={The “echo state” approach to analysing and training recurrent neural networks-with an erratum note},
	author={Jaeger, Herbert},
	journal={Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
	volume={148},
	number={34},
	pages={13},
	year={2001},
	publisher={Bonn}
}

@misc{brownlee2021WeightInit,
	author = {Brownlee, Jason},
	month = {02},
	title = {{Weight Initialization for Deep Learning Neural Networks}},
	url = {https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/},
	year = {2021},
}

@article{sculley2015hidden,
	title={Hidden technical debt in machine learning systems},
	author={Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
	journal={Advances in neural information processing systems},
	volume={28},
	pages={2503--2511},
	year={2015}
}

@inproceedings{riedmiller2005neural,
	title={Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
	author={Riedmiller, Martin},
	booktitle={European conference on machine learning},
	pages={317--328},
	year={2005},
	organization={Springer}
}

@article{mnih2015human,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={nature},
	volume={518},
	number={7540},
	pages={529--533},
	year={2015},
	publisher={Nature Publishing Group}
}

@inproceedings{thrun1993issues,
	title={Issues in using function approximation for reinforcement learning},
	author={Thrun, Sebastian and Schwartz, Anton},
	booktitle={Proceedings of the Fourth Connectionist Models Summer School},
	pages={255--263},
	year={1993},
	organization={Hillsdale, NJ}
}

% describing a ``turing machine'' -- not 100 sure on this citation
@article{church1937turing,
	title={AM Turing. On computable numbers, with an application to the Entscheidungs problcm. Proceedings of the London Mathematical Society, 2 s. vol. 42 (1936--1937), pp. 230--265.},
	author={Church, Alonzo},
	journal={The Journal of Symbolic Logic},
	volume={2},
	number={1},
	pages={42--43},
	year={1937},
	publisher={Cambridge University Press}
}

% Turing machine
@article{turing1936computable,
	title={On computable numbers, with an application to the Entscheidungsproblem},
	author={Turing, Alan Mathison and others},
	journal={J. of Math},
	volume={58},
	number={345-363},
	pages={5},
	year={1936}
}

@article{yuksel2012twenty,
	title={Twenty years of mixture of experts},
	author={Yuksel, Seniha Esen and Wilson, Joseph N and Gader, Paul D},
	journal={IEEE transactions on neural networks and learning systems},
	volume={23},
	number={8},
	pages={1177--1193},
	year={2012},
	publisher={IEEE}
}

@book{chapelle2010semi,
	title={Semi-Supervised Learning},
	author={Chapelle, O. and Scholkopf, B. and Zien, A.},
	isbn={9780262514125},
	lccn={2011288034},
	series={Adaptive Computation and Machine Learning series},
	url={https://books.google.com/books?id=A3ISEAAAQBAJ},
	year={2010},
	publisher={MIT Press}
}

@misc{kazemnejad_2021,
	 title={Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog}, url={https://kazemnejad.com/blog/transformer_architecture_positional_encoding/},
	 journal={Kazemnejad.com},
	 author={Kazemnejad, Amirhossein},
	 year={2021}
}

% accessed 15Dec21
@misc{kernes_2021,
	author = {Kernes, Jonathan},
	month = {02},
	title = {{Master Positional Encoding: Part I - Towards Data Science}},
	url = {https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3},
	year = {2021},
}

% accessed 15Dec21
@misc{kernes_2021B,
	author = {Kernes, Jonathan},
	month = {02},
	title = {{Master Positional Encoding: Part II - Towards Data Science}},
	url = {https://towardsdatascience.com/master-positional-encoding-part-ii-1cfc4d3e7375},
	year = {2021},
}

@article{doya1993universality,
	title={Universality of fully connected recurrent neural networks},
	author={Doya, Kenji},
	journal={Dept. of Biology, UCSD, Tech. Rep},
	year={1993},
	publisher={Citeseer}
}

@inproceedings{jozefowicz2015empirical,
	title={An empirical exploration of recurrent network architectures},
	author={Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
	booktitle={International conference on machine learning},
	pages={2342--2350},
	year={2015},
	organization={PMLR}
}

@article{ring1994continual,
	title={Continual learning in reinforcement environments},
	author={Ring, Mark Bishop and others},
	year={1994},
	publisher={Citeseer}
}

@incollection{thrun1998lifelong,
	title={Lifelong learning algorithms},
	author={Thrun, Sebastian},
	booktitle={Learning to learn},
	pages={181--209},
	year={1998},
	publisher={Springer}
}

@incollection{MCCLOSKEY1989109,
	title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
	editor = {Gordon H. Bower},
	series = {Psychology of Learning and Motivation},
	publisher = {Academic Press},
	volume = {24},
	pages = {109-165},
	year = {1989},
	issn = {0079-7421},
	doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
	url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
	author = {Michael McCloskey and Neal J. Cohen},
}

@article{shao2020normalization,
	title={Is normalization indispensable for training deep neural network?},
	author={Shao, Jie and Hu, Kai and Wang, Changhu and Xue, Xiangyang and Raj, Bhiksha},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={13434--13444},
	year={2020}
}

@article{grosseadaptive,
	title={Adaptive Gradient Methods, Normalization, and Weight Decay},
	author={Grosse, Roger}
}

@article{wang2019transferable,
	title={Transferable normalization: Towards improving transferability of deep neural networks},
	author={Wang, Ximei and Jin, Ying and Long, Mingsheng and Wang, Jianmin and Jordan, Michael I},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

@inproceedings{huang2017centered,
	title={Centered weight normalization in accelerating training of deep neural networks},
	author={Huang, Lei and Liu, Xianglong and Liu, Yang and Lang, Bo and Tao, Dacheng},
	booktitle={Proceedings of the IEEE International Conference on Computer Vision},
	pages={2803--2811},
	year={2017}
}

@inproceedings{vinyals2017model,
	title={“Model vs optimization meta learning},
	author={Vinyals, Oriol},
	booktitle={NIPS 2017 Metalearning Symposium},
	year={2017}
}

@article{Hospedales2021MetaLearningIN,
	title={Meta-Learning in Neural Networks: A Survey},
	author={Timothy M. Hospedales and Antreas Antoniou and Paul Micaelli and Amos J. Storkey},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	year={2021},
	volume={PP}
}

@inproceedings{ni2022contrastive,
	title={Contrastive Learning is Just Meta-Learning},
	author={Renkun Ni and Manli Shu and Hossein Souri and Micah Goldblum and Tom Goldstein},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=gICys3ITSmj}
}


@inproceedings{Koch2015SiameseNN,
	title={Siamese Neural Networks for One-Shot Image Recognition},
	author={Gregory R. Koch},
	year={2015}
}


@inproceedings{Vinyals2016MatchingNF,
	title={Matching Networks for One Shot Learning},
	author={Oriol Vinyals and Charles Blundell and Timothy P. Lillicrap and Koray Kavukcuoglu and Daan Wierstra},
	booktitle={NIPS},
	year={2016}
}

@article{Sung2018LearningTC,
	title={Learning to Compare: Relation Network for Few-Shot Learning},
	author={Flood Sung and Yongxin Yang and Li Zhang and Tao Xiang and Philip H. S. Torr and Timothy M. Hospedales},
	journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	year={2018},
	pages={1199-1208}
}

@article{Satorras2018FewShotLW,
	title={Few-Shot Learning with Graph Neural Networks},
	author={Victor Garcia Satorras and Joan Bruna},
	journal={ArXiv},
	year={2018},
	volume={abs/1711.04043}
}

@article{Shyam2017AttentiveRC,
	title={Attentive Recurrent Comparators},
	author={Pranav Shyam and Shubham Gupta and Ambedkar Dukkipati},
	journal={ArXiv},
	year={2017},
	volume={abs/1703.00767}
}

@inproceedings{Mishra2018ASN,
	title={A Simple Neural Attentive Meta-Learner},
	author={Nikhil Mishra and Mostafa Rohaninejad and Xi Chen and P. Abbeel},
	booktitle={ICLR},
	year={2018}
}

@inproceedings{Garnelo2018ConditionalNP,
	title={Conditional Neural Processes},
	author={Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo Jimenez Rezende and S. M. Ali Eslami},
	booktitle={ICML},
	year={2018}
}

@article{Edwards2017TowardsAN,
	title={Towards a Neural Statistician},
	author={Harrison Edwards and Amos J. Storkey},
	journal={ArXiv},
	year={2017},
	volume={abs/1606.02185}
}


@article{Nichol2018ReptileAS,
	title={Reptile: a Scalable Metalearning Algorithm},
	author={Alex Nichol and John Schulman},
	journal={arXiv: Learning},
	year={2018}
}

@article{Singh2021OnTD,
	title={On the Dark Side of Calibration for Modern Neural Networks},
	author={Aditya Kumar Singh and Alessandro Bay and Biswa Sengupta and Andrea Mirabile},
	journal={ArXiv},
	year={2021},
	volume={abs/2106.09385}
}


@inproceedings{Bengio2009CurriculumL,
	title={Curriculum learning},
	author={Yoshua Bengio and J{\'e}r{\^o}me Louradour and Ronan Collobert and Jason Weston},
	booktitle={ICML '09},
	year={2009}
}

@inproceedings{Yang2019MixtapeBT,
	title={Mixtape: Breaking the Softmax Bottleneck Efficiently},
	author={Zhilin Yang and Thang Luong and Ruslan Salakhutdinov and Quoc V. Le},
	booktitle={NeurIPS},
	year={2019}
}