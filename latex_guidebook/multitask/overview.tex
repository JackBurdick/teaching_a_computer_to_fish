\section{Overview}

\TD{Multi-Task Learning with Deep Neural Networks: A \cite{DBLP:journals/corr/abs-2009-09796}}

\TD{A Survey on Multi-Task Learning \cite{DBLP:journals/corr/ZhangY17aa}}

% multi-task === task relatedness
\TD{Learning Task Relatedness in Multi-Task Learning for Images in Context \cite{DBLP:journals/corr/abs-1904-03011}}

\TD{overview}

\r{learn model with parameters that solve multiple tasks -- don't care about generalizing to new tasks}

\r{may also be refered to as joint learning or learning with auxiliary tasks and is closely related to meta learning \TD{ref} or learning to learn.}

\r{A task here is something we are }

\r{Ruder~\cite{DBLP:journals/corr/Ruder17a} motivates multi-task learning in three different ways:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item biologically -- when learning new tasks we often use knowledge acquired from other tasks
	\item pedagogical -- learn foundational skills before learning more complex skills
	\item machine learning -- MTL may provide a form of inductive transfer, which may cause a model to prefer some hypotheses over others or may cause the model to prefer hypotheses that explain more than one task
\end{itemize}


\subsection{Same loss function, different data distribution}

\r{same loss function across all tasks, different distribution over inputs for each task -- same kind of output tasks -- e.g. per-language handwriting recognition, personalized spam filter -- spam for one person might different from one person to the next}

\r{multi-label learning. loss function and distribution over inputs is the same across tasks -- different kind of output tasks -- e.g. CelebA attribute recognition, is one wearing hat? hair color? scene understanding -- depth? keypoints?}

\subsection{Different loss function}

\r{e.g. one task may be predicting a discrete variable and another is predicting a continuous variable.}

\r{another example may be if you care more about one task than another}

\subsection{training network}

\r{task descriptor -- simplest could be a one hot encoded vector of task index}


\r{multiplicative gating}

\r{opposite ends of the spectrum -- i) single network (but essentially two sub networks), no shared parameters across tasks ii) same network until end, then concatenate task descriptor}

\TD{figure}

\r{task descriptor may be ``added in'' (concateated) at different points in the network -- which would result in more task specific parameters}


\r{shared vs task specific parameters}

\r{split parameter vector into shared and specific parameter vectors --  task specific parameters only optimized with respect to the objective for a given task, whereas shared are optimized over all tasks}

\subsection{Conditioning task descriptor}

\r{``how you condition on the task descriptor is equivalent to choosing where and how to share parameters''}

\r{there are many different choices -- which are currently considered problem dependent and are guided by intuion and knowledge about the problem (e.g. if you know two are similar they may share more information, less similar, maybe share less) -- currently more of an art}

\r{i) concatenation/additive-based conditioning}

\r{concatenation-based conditioning}

\TD{figure}

\r{additive conditioning}

\TD{figure}

\r{but in effect they are the same}
\TD{explain/understand}

\r{determining how similar tasks are to one another}

\r{ii) multi-headed architecture -- (\TD{ruder '17?})}

\r{from youtube paper -- that when correlation betweent tasks is low, it may harm the learning process}

\r{share initial layers then diverge}

\r{iii) multiplicative conditioning}

\r{similar to additive, only you multiply -- more expressive than additive, (multiplicative interactions, not just additive))}

\r{more complex choices -- ``cross stitch networks'' \TD{(Misra, Shrivastava, Gupta, Herber '16)}, ``multi-Task attention Network'' \TD{(Liu, Johns, Davison '18)}, ``Deep Relation Networks ''\TD{Long, Wang, '15}, ``Sluice Networks'' \TD{Ruder, Bingel, Augenstein, Sogaard '17}}

\subsection{optimizing the objective}

%% presented in stanford lecture \TD{cite}
\r{Basic}
\begin{itemize}[noitemsep,topsep=0pt]
	% ensure tasks are sampled uniformly - regardless of data quantity
	% however depending on desires/knowledge this may change
	\item sample mini-batch of tasks
	\item sample mini-bath of datapoints
	\item compute loss on mini-batch
	\item backprop loss to compute gradient
	\item apply gradient with optimizer
\end{itemize}

\r{important to note that task labels should be on the same scale (for instance in the case of regression -- otherwise the loss function will, by default, optimize for one of the tasks more than the other)}

\r{Challenges}
\begin{itemize}[noitemsep,topsep=0pt]
	% there may be a Finn paper on this with Multi-task CIFAR-100
	\item Negative transfer -- data from one task is adversely affecting the training of the other tasks
	\item sample mini-bath of datapoints
	\item compute loss on mini-batch
	\item backprop loss to compute gradient
	\item apply gradient with optimizer
\end{itemize}

\r{negative transfer -- why? -- i) optimization challenges (caused by i) task interference and ii) tasks learning at different rates and ii) limited representational capacity (multi-task networks generally need to be much larger than single task)}

\r{if the data is really unbalanced for different tasks, then it may learn the task with a lot of data and miss out on the task with smaller amounts of data.}


\r{another way we could view multi-task learning is as a form or regularization}

\r{each task has it's own form of supervison -- not necessarily about sharing inputs, about sharing supervision. each task corresponds to different amounts of supervison and those can be used for building more flexible representations}

\r{if there is negative transfer -- share less across tasks. if overfitting, maybe need to share more}

\r{sharing / not sharing is not binary, in that we could do something like soft parameter sharing}

\TD{``recommending what video to watch next''\cite{zhao2019recommending}}



\TD{Sparse mixture of experts (sparse MoE)\cite{shazeer2017outrageously}}
% used by sparse mixture of experts
\TD{Conditional computation \cite{DBLP:journals/corr/abs-1305-2982}}

\TD{from youtube paper ``Multi-gate Mixture-of-Experts (MMoE)'' -- form of soft-parameter sharing}
\begin{itemize}[noitemsep,topsep=0pt]
	\item shared bottom layer
	\item allow different parts of the network to ``specialize'' (expert NNs)
	\item decide which expert to use for some input / task combination
	\item compute features from selected expert
	\item compute output (takes as input the output from the expert)
\end{itemize}

\TD{interesting visualization where they looked at which experts were ``consulted'' for each task.}


%%%%%
\r{one end of the spectrum}
\r{each task may have subtasks}
\r{downsides of multiple single tasks networks:}
\begin{itemize}[noitemsep,topsep=0pt]
	\item expensive at testtime
	\item no feature sharing, potential overfitting
\end{itemize}

\r{benefit: decoupled functionality -- iterate on one thing by holding everything else constant}

\TD{Cite Karpathy youtube}
\r{other end of the spectrum}
\r{benefit: less expensive at test time}
\r{downsides of multiple lightweight heads on single backbone:}
\begin{itemize}[noitemsep,topsep=0pt]
	\item tasks may "fight" for the same shared capacity -- complicated relationship
	\item fully coupled functionality
\end{itemize}

\section{Relationships}

% TODO: this section outline is from the Karpathy youtube

\subsection{Architecture}
\TD{which tasks should be grouped together~\cite{standley2019tasks} -- how many backbones, and how should the heads be allocated?}

\r{task weights -- as hyperparameters -- typically a bruteforce solution -- one example may be \cite{kirillov2019panoptic} -- this is ok for small number of tasks (two or so), but becomes intractable very fast.}

\subsection{loss}

% Karpathy youtube
\r{considerations}
\begin{itemize}[noitemsep,topsep=0pt]
	\item loss function scale (e.g. classification vs regression and and within type scale)
	\item importance
	\item difficulty
	\item data considerations (volume, noise, etc)
\end{itemize}

\subsection{Training Dynamics}

\r{within-task oversampling (example from Karpathy talk is traffic light color -- green/red oversampled largely compared to orange and other)} \TD{attempt to semi-balance the batches to correct for label imbalance}

\r{across-task balances}

\r{Scheduling of batches is complex -- if a dataset has batch that contains a label for multiple objectives -- but by performing ``training'' with the other label, you may be altering the batch distributions you are considering}

\r{what is early stopping for MTL?}

\section{interesting considerations}

\r{collaboration with multiple distributed people on an multi-task model isn't exactly straight forward.}

\section{Parameter Sharing}

\r{hard or soft parameter sharing}

\subsection{Hard parameter sharing}

\r{sharing hidden layers between tasks. This, depending on the capacity of the model and requirements of the task, may require the model to build/find representations that are useful beyond a single task -- reducing the risk of overfitting.}

\r{hard parameter sharing \cite{caruana1993multitask}}

\TD{figure of hard parameter sharing}

\subsection{Soft parameter sharing}

\r{soft parmeter sharing~\cite{duong2015low}}

\r{each task has it's own model, but the parameters between corresponding layers are regularized.}

\TD{figure of soft parameter sharing}

\subsection{Other}

\TD{Deep Relationship Networks~\cite{DBLP:journals/corr/Long015a}, rely on a pre-defined structure but use maxrix priors on fully connected layers, allowing the model to learn the relationship between tasks.}

\TD{dynamically widens a thin network greedily, promoting grouping of related tasks ~\cite{DBLP:journals/corr/LuKZCJF16}}

\TD{Cross-stitch Networks for Multi-task Learning~\cite{DBLP:journals/corr/MisraSGH16} begin as two separate models (the same as if soft parameter sharing was being used) then ``cross-stitch'' units.}

\r{NLP -- task hierarchy. ``“low-level” tasks are better kept at the lowerlayers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks.''\cite{sogaard2016deep}.  \TD{A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks \cite{DBLP:journals/corr/HashimotoXTS16}}}

\r{rather than learning the structure of sharing, \TD{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics \cite{DBLP:journals/corr/KendallGC17}} consider the uncertainty of each task }

\TD{tensor factorization~\cite{DBLP:journals/corr/YangH16}}

\TD{Sluice Networks, learning which layers and subspaces to share~\cite{ruder2019latent}}

\TD{End-to-End Multi-Task Learning with Attention \cite{DBLP:journals/corr/abs-1803-10704}}


\TD{Multi-gate MoE (MMoE) -- explicitly learns to model task relationship from data~\cite{ma2018modeling}}




\section{Mechanisms}

\r{from ruder\cite{DBLP:journals/corr/Ruder17a} citing \TD{Caruana}}



\begin{itemize}[noitemsep,topsep=0pt]
	\item implicit data augmentation -- two tasks that have different noise patterns, may learn better representations though averaging the noise patterns
	\item attention focusing -- e.g. if is very difficult (noisy, or high-dimensional), a model may have trouble differentiating the relevant features and by using additional tasks, the model may be able to use the additional information to determine which features are relevant. This may be dependent on task related-ness?
	\item Eavesdropping -- learning task relevant to a task from another task. In one task, learning a feature may be easier than the other. hints \TD{https://www.sciencedirect.com/science/article/pii/0885064X9090006Y}
	\item representation bias -- MTL biases model to prefer representations that are useful to many tasks, not just one. \TD{A Model of Inductive Bias Learning \cite{DBLP:journals/corr/abs-1106-0245}}
	\item Regularization -- introduces an inductive bias
\end{itemize}

