\chapter{Positional Encodings}

\TD{Positional embedding and positional encoding tend to be used interchangably. However, typically an encoding means ``fixed'' while an embedding means ``learned'' or ``trainable''.}

% TODO: example of how word order matters (not is a good example)

\r{Attention/transformers view the inputs as sets, that is there is no order associated with each input. All information enters the attention block at once. This is in contrast to something like a recurrent model, in which the order of the inputs is implicit.}

\r{trade off: potentially faster (remove the dependancy of doing operations sequentially) and can also possibly help capture longer range dependancies (without additional complexity e.g. skip connections)}

\r{(re)introducing order to the input by including additional information -- the ``positional embedding''.}

\r{NOTE: Great blog posts on this subject~\cite{kazemnejad_2021, kernes_2021, kernes_2021B}}

\section{Positional Values}

\r{why not add linear/progressive value signifying order?}

\r{This would be called an aboslute positional embedding}

\r{Include index information [0, n], where n is the length of the sentance (minus 1). This could lead to magnitude issues. Where the singal from the word embeddings is ``washed out'' by the positional embedding.  Another consideration is that (may or may not be an issue depending on the application) is that you'd like to ensure you have the largest sequence in the training set that you expect to see in evaluation set. For example, if you only see sequences of length $25$ in the training data and then see a sequence of length $32$ during inference. The model will be unsure what to do with values $25 - 31$ (zero indexing). Depending on how you include the positional embedding (e.g. additive or concat), the model may misinterpret the values or be largely/entirely unsure what to make of these previously unseen values.}


\r{To address this you could either increase the magnitude of the word embeddings or normalize/scale the positional embedding.}

\r{However, niether are ideal.}

\r{Increasing the magnitude of the word embeddings would possibly work, though you may consider issues with exploding values in the network, but you'd still have a similar issue to what would happen if you normalized the positional embedding. }

\r{That is, the normalized positional embeddings may encode different information when the sentances are longer or shorter -- the delta between words in a 5 word sentance vs a 20 word sentance doesn't have a consistent meaning}

% NOTE: haven't read this yet (I don't think, though the link is purple...)
\TD{Self-Attention with Relative Position Representations~\cite{DBLP:journals/corr/abs-1803-02155}}

\r{Ideally the embedding would be able to account for all the issues we discussed.}

\begin{itemize}[noitemsep,topsep=0pt]
	\item consistent delta between each position
	\begin{itemize}[noitemsep,topsep=0pt]
		\item regardless of sequence length, if an instance is one instance away from another, the positional encoding should be the same e.g. in a length four sequence the positional encoding should be the same from instances $1$ and $2$ as it is for instances $19$ and $20$ in a length $22$ sequence.
	\end{itemize}
	\item generalize to sequence lengths unseen in training
\end{itemize}

\r{additionally, we'd prefer to have each instance in the sequence be unique. That is the positional encoding for one instance shouldn't be the same as another in the same sequence (e.g. two words in a sentance).}

\paragraph{Positional Encoding Value(s)}

\r{Rather than use a single value, a possible solution is to use an array of values.}

\TD{Relative positional encoding (rather than absolute).}


\TD{What if we were to use a binary array to represent each location?}

\TD{issue with binary}

\subsection{Fixed vs Learned and Relative vs Absolute}

\r{positional information can be included as either absolute or relative.}

\r{Additionally, the included positional information can either be fixed or learned.}

\subsection{Absolute}

\subsubsection{Fixed}

\paragraph{Sinusoidal}

\TD{include figure with multiple frequencies and points on the x and y axis leading to embeddings}

\TD{Sinusoidal used in orginal~\cite{DBLP:journals/corr/VaswaniSPUJGKP17}}

\subsubsection{Learned}

\TD{BERT:~\cite{DBLP:journals/corr/abs-1810-04805}}

% fixed and learned
\TD{Fixed and learned, uses sinusoidal, but learns the frequency, On Position Embeddings in BERT~\cite{Wang2021OnPE}}

\TD{FLOATER --- Learning to Encode Position for Transformer with Continuous Dynamical Model~\cite{DBLP:journals/corr/abs-2003-09229}}

\subsection{Relative}

% TODO: read - https://www.youtube.com/watch?v=7XHucAvHNKs

\TD{Represent positional relationship between tokens, rather than sequence as a whole}

\TD{Included in the keys and values}

\TD{offset between key and query?}

\TD{InDIGO --- Insertion-based Decoding with Automatically Inferred Generation Order~\cite{Gu2019InsertionbasedDW}}

\TD{Music Transformer~\cite{Huang2018MusicT}}

\TD{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer~\cite{Raffel2020ExploringTL}}

\TD{Transformer-XL: Attentive Language Models beyond a Fixed-Length Context~\cite{Dai2019TransformerXLAL}}

\TD{DeBERTa: Decoding-enhanced BERT with Disentangled Attention~\cite{He2021DeBERTaDB}}


\subsubsection{Fixed}

\subsubsection{Learned}

\TD{relative~\cite{DBLP:journals/corr/abs-1803-02155}}


\subsubsection{Use in Linear Transformers}

\TD{whole attention matrix is needed for classical relative positional encodings}

\TD{Relative Positional Encoding for Transformers with Linear Complexity~\cite{DBLP:journals/corr/abs-2105-08399}}

\section{Hybrid approaches}

\TD{ \textbf{T}ransformer with \textbf{U}ntied \textbf{P}ositional \textbf{E}ncoding (TUPE) --- Rethinking Positional Encoding in Language Pre-training~\cite{Ke2021RethinkingPE}}

\TD{RoPE --- RoFormer: Enhanced Transformer with Rotary Position Embedding~\cite{Su2021RoFormerET}}





%%%%%%%%%%%%%%%%%%%%%
\section{TO INCLUDE}

\TD{What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding~\cite{DBLP:journals/corr/abs-2010-04903}}


\TD{CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings~\cite{DBLP:journals/corr/abs-2106-03143}}

% NOTE: possibly relevant: https://aclanthology.org/2021.emnlp-main.266.pdf


% possibly useful: https://theaisummer.com/positional-embeddings/


% Unclear, are these positional embeddings learned?
\TD{Convolutional Sequence to Sequence Learning~\cite{DBLP:journals/corr/GehringAGYD17}}




\section{Including Positional Information}

% someones thoughts on  additive vs concat: https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/exs7d08/

\subsection{Additive}

\TD{saves memory (over concatenation -- less dimensions)}

\TD{figure}

\subsection{Concatenation}

\TD{figure}


\subsection{Multiple inclusions}

\TD{It's possible the information from the positional values lose their significance after each layer.}

\TD{The following papers include positional information at each layer.}

\TD{learned per-layer positional embedding --- Character-Level Language Modeling with Deeper Self-Attention~\cite{DBLP:journals/corr/abs-1808-04444}}

\TD{also per-layer addition of a positional embedding~\cite{Guo2019LowRankAL, DBLP:journals/corr/abs-2003-09229}}



\section{Beyond One Dimension}


\subsection{Two Dimensional}

\TD{Extend sinusoidal to 2D~\cite{DBLP:journals/corr/abs-1802-05751}}

\TD{An Intriguing Failing of Convolutional Neural Networks and the CoordConv
	Solution~\cite{DBLP:journals/corr/abs-1807-03247}}

\TD{Attention Augmented Convolutional Networks~\cite{DBLP:journals/corr/abs-1904-09925}}
