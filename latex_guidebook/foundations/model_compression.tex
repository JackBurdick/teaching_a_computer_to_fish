\chapter{Model ``Compression'' or ``Distillation''}
% this section will likely be moved around
motivation

\TD{Distilling the Knowledge in a Neural Network \cite{Hinton2015DistillingTK}}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Model (output) performance
	\begin{itemize}[noitemsep,topsep=0pt]
		\item regularization (through reduction of number of parameters)
	\end{itemize}
	\item Model performance
	\begin{itemize}[noitemsep,topsep=0pt]
		\item memory/storage --- by creating physically smaller networks
		\item energy --- computation, latency, which supports edge depolyments
	\end{itemize}
\end{itemize}

\section{Quantization}


\r{reduces the precision of parameters in a model}

\r{latency, memory}

\r{some fixed-point accellerators become available in edge settings}

\subsection{When}

\subsubsection{during training}

\subsubsection{post-training}

\r{calibration data}

\section{Weight Pruning}

%TODO: https://github.com/he-y/Awesome-Pruning -- repo of recent pruning work

% intial skeleton influence by:
\TD{The initial structure and citations were heavily influenced by \cite{lange2020_lottery_ticket_hypothesis}}

\TD{SNIP: Single-shot Network Pruning based on Connection Sensitivity \cite{DBLP:journals/corr/abs-1810-02340}}

\r{reduces the overall number of parameters}

\r{in practice, this often refers to setting the parameters of a particular ``node'' to zero and making it untrainable during training.}

\TD{Comparing Rewinding and Fine-tuning in Neural Network Pruning \cite{Renda2020ComparingRA} --- Learning rate rewinding}

\TD{Deconstructing Lottery Tickets: Zero \cite{DBLP:journals/corr/abs-1905-01067} --- SuperMasks}

\TD{The Early Phase of Neural Network Training \cite{Frankle2020TheEP}}

% `` identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data''
\TD{Synaptic Flow Pruning (SynFlow) --- Pruning neural networks without any data by iteratively conserving synaptic flow \cite{Tanaka2020PruningNN}}

% code: https://github.com/varungohil/Generalizing-Lottery-Tickets
\TD{One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers \cite{Morcos2019OneTT}}



\TD{Pruning Neural Networks at Initialization: Why are We Missing the Mark? \cite{Frankle2020PruningNN}}


\subsubsection{What to remove}



\paragraph{relationship of nodes to nodes pruned}

\subparagraph{unstructured}

\r{no consideration for relationship between pruned weights}

\subparagraph{structured}

\r{prunes ``groups'' of weights.}

\paragraph{relationship of nodes to architecture}

\subparagraph{local}

\r{enforcment of a percent of weights pruned at each layer}

\subparagraph{global}

\r{total percent of weights are pruned, no restriction layer wise.}

\subsubsection{Deciding what to remove}

\r{common thought: large magnitude parameters are ``more important'' and thus should be removed less --- which is counterintuitive to concepts such as l2 regularization which penalize large magnitude parameters.}

\TD{other techniques}

\subsubsection{When to prune}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Before training
	\item During training
	\item After training
\end{itemize}

\subsubsection{Pruning mechanism}

\begin{itemize}[noitemsep,topsep=0pt]
	\item One shot (prune all at once)
	\item Iterative
\end{itemize}

\section{Topology}

\r{more efficient model topology}

\subsection{Distillation}

% TODO: these are popular citations in the space, I haven't read many of them yet. putting them here for a start when I come back to the topic
\TD{Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks \cite{Wang2020KnowledgeDA}}
\TD{Like What You Like: Knowledge Distill via Neuron Selectivity Transfer \cite{DBLP:journals/corr/HuangW17a}}
\TD{FitNets: Hints for Thin Deep Nets \cite{Romero2015FitNetsHF}}
\TD{Similarity-Preserving Knowledge Distillation \cite{DBLP:journals/corr/abs-1907-09682}}
\TD{Correlation Congruence for Knowledge Distillation \cite{DBLP:journals/corr/abs-1904-01802}}
\TD{A gift from knowledge distillation: Fast optimization, network minimization and transfer learning\cite{yim2017gift}}
\TD{Relational Knowledge Distillation \cite{DBLP:journals/corr/abs-1904-05068}}
\TD{Paraphrasing Complex Network: Network Compression via Factor Transfer \cite{DBLP:journals/corr/abs-1802-04977}}
\TD{Contrastive Representation Distillation \cite{DBLP:journals/corr/abs-1910-10699}}


\paragraph{Knowledge Transfer}

% TODO: I'm unclear on the distinction here, is knowlege transfer the transfer of knowledge (like transfer learning but student/teacher?) and knowledge distillation the process of distilling that knowledge?

%% knowledge transfer -- but really distillation is kt? unclear here. need to read more
\TD{Probabilistic Knowledge Transfer for Deep Representation Learning \cite{DBLP:journals/corr/abs-1803-10837}}
\TD{Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons \cite{DBLP:journals/corr/abs-1811-03233}}
\TD{Variational Information Distillation for Knowledge Transfer \cite{DBLP:journals/corr/abs-1904-05835}}

\subsection{Tensor Decomposition}
% Not sure what this is -- from coursera


% TODO: this fits somewhere in this chapter, but not necessarily `here'
\subsection{The Lottery Ticket Hypothesis}

% RigL -- training sparse networks
\TD{Rigging the Lottery: Making All Tickets Winners \cite{Evci2019RiggingTL}}


\TD{Original paper --- The Lottery Ticket Hypothesis: Training Pruned Neural Networks \cite{DBLP:journals/corr/abs-1803-03635} --- iterative pruning. Dense network at initialization contains a number of ``winning tickets''}


\TD{Stabilizing the Lottery Ticket Hypothesis \cite{DBLP:journals/corr/abs-1903-01611}}

\TD{IMP with rewind}

% CODE: https://github.com/RICE-EIC/Early-Bird-Tickets
\TD{Drawing early-bird tickets: Towards more efficient training of deep networks \cite{You2020DrawingET}}

\TD{Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP \cite{Yu2020PlayingTL}}

