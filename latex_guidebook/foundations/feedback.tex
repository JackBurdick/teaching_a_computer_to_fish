\section{Feedback or Recurrent}

\textcolor{green}{TODO: Overview}

\r{RNNs or ``\textit{\textbf{r}}ecurrent \textit{\textbf{n}}eural \textit{\textbf{n}}etworks'' are used for a variety of purposes but are typically designed with sequences of data as an input in mind. They are similar in concept to a standard/feed-forward netowrk, with the major distinction being that they also have connections that point ``backwards'' i.e. they have connections that feed into themselves.}

\r{Are capable fo working on sequences of arbitrary lengths, rather than fixed-sized inputs}

\r{universal approximator~\cite{doya1993universality}}

\r{``superiority of gated models over vanilla RNN models is almost exclusively driven by trainability''~\cite{Collins2017CapacityAT}, `` Our results suggest that, contrary to common
	belief, the capacity of RNNs to remember their input history is not a practical limiting factor on their
	performance.'' ~\cite{Collins2017CapacityAT}}

% TODO: I'm not sure where the other citation for this is... but somewhere...
\TD{systematically removing pieces of an LSTM~\cite{DBLP:journals/corr/GreffSKSS15}}

\TD{An empirical exploration of recurrent network architectures~\cite{jozefowicz2015empirical}}
\TD{``Thus we recommend adding a bias of 1 to the forget gate of every LSTM in every application''~\cite{jozefowicz2015empirical}}
\TD{``if there are [RNN] architectures that are much better than the LSTM, then they are not trivial to find''~\cite{jozefowicz2015empirical}}


\subsection{Foundation}

\r{An example of an RNN diagram is shown in \TD{fig}. However, this representation is misleading since it does not show ``every'' connection in the model --- most notably, the recurrent connections.  RNNs may also be often represented in diagrams as ``unrolled'' (\TD{fig}). The unrolled RNN is easier to visualize how these recurrent connections are included.  This makes it easier to understand how each timestep is dependent on not only the current input (at the particular time step), but also dependent on ``all'' previous time steps. It is often stated that at a certain timestep (n), the output has ``memory'' since it is a function of all the previous time steps.}


\footnotetext{the term ``all'' is emphasized here since it is the goal to include information from all previous time steps. This is true in theory, however, this is not always the case in practice. This is discussed further in \ALR{}}

\subsection{Simple RNN and Recurrent Neuron}

\TD{Diagram of the inside of a RNN neuron}


\subparagraph{Overview}

\TD{todo}


\section{Common Problems}

Two well known main problems with RNNs.

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Maintaining states are expensive
	\item Vanishing and/or exploding gradients
\end{enumerate}

\TD{hardware acceleration}

\subsection{Maintaining States}


\subsection{Addressing Vanishing and Exploding Gradients}

\r{Propagating signals through a long/deep network without loosing (vanishing gradient) or overamplifying the signal (exploding gradient) is difficult.  There have been a few advances to address this issue.}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Architecture (different cell types, memory schemes)
	\item Initialization Strategies
	\item Activation Function
\end{enumerate}

