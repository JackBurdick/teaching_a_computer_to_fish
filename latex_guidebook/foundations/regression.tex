\section{Regression}

\textcolor{blue}{Three cases of the {generalized linear model}\index{generalized linear model}, simple, multiple, and polynomial linear regression.}

\textcolor{blue}{TODO: define generalized linear model}

\subsection{Simple Linear Regression}

% C2 of Mastering ML
\textcolor{blue}{Model a \emph{linear} relationship between a response variable and a feature representing an explanatory variable. The relationship is modeled with a linear surface called a hyperplane \ALR.}

\textcolor{blue}{Simple linear regression consists of two total dimensions (a dimension for the response variable and another for the explanatory) -- the hyperplane, as explained above, has one dimension (line)}

\textcolor{blue}{May also be called univariate regression (one variable).}

\textcolor{red}{convex loss function}

\textcolor{red}{history: ``method of least squares'', Legendre and Gauss, astronomy --- 1936 Fisher proposed ``linear discriminant analysis) --- 1940s (various authors - logistic regression) --- 1970s (Nelder and Wedderburn ``generalized linear models'' == entire class of statistical learning methods) (TODO: verify, find papers, ISLRp6) --- 1980s (Breinman, Friedman, Olshen, and Stone) == ``classification and regression trees''. 1986 (Hastie and Tibshirani == ``generalized additive models'' == non-linear extensions to generalized linear models)}

\begin{equation}
{Y \approx \beta_0 + \beta_1 x}
\label{eq:slr_ex}
\end{equation}

\textcolor{blue}{$\approx$ can be read as ``\emph{is approximately modeled as}''. $Y$ is a quantitative response (output/prediction) and $X$ predictor variable(input/feature). $\beta_0$ and $\beta_1$ are two unknown constants representing the intercept and slope, respectively. These unknown values that determine the behavior of the model are known as the model \emph{parameters} or \emph{coefficients}}

\subsubsection{OLS}

\TD{weighted sum of features plus a bias}


\begin{equation}
	\begin{split}
		\hat{y} & = \theta + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n + x_n  \\
		  \textrm{pred} & = \textrm{bias} + \textrm{feature weight} * \textrm{feature value}
	\end{split}
\end{equation}

\textcolor{blue}{{Ordinary Lease Squares (OLS)}\index{Ordinary Lease Squares (OLS)}, or {Linear Least Squares}\index{Linear Least Squares} is a method for estimating the parameters for a simple linear regression model.}

\textcolor{blue}{Solving OLS for simple linear regression ($y=\beta_0 + \beta_1 x$).}

\textcolor{blue}{First we'll solve for the slope $\beta_1$, where $\beta_1$ is can be found using Eq.\ref{eq:slr_ols_slope}.}

\begin{equation}
{\beta_1 =  \frac{cov(x,y)}{var(x)}}
\label{eq:slr_ols_slope}
\end{equation}

%% TODO: JACK -- these two def var and covar need to be moved

\textcolor{blue}{Variance (Eq.\ref{eq:variance_def}) is the measure of how far the set of values are spread apart -- if all the numbers in a set were equal, their variance would be zero.}

\begin{equation}
{var(x) = \frac{\sum_{i=1}^{n}(x_i - \hat{x})^2}{n-1}}
\label{eq:variance_def}
\end{equation}


\textcolor{blue}{Covariance (Eq.\ref{eq:covariance_def}) is the measure of how much two variable change together -- if two variables increase together, their covariance is positive}

\begin{equation}
{cov(x) = \frac{\sum_{i=1}^{n}(x_i - \hat{x})(y_i - \hat{y})}{n-1}}
\label{eq:covariance_def}
\end{equation}

\textcolor{blue}{After solving for $\beta_1$, $\beta_0$ can be found by rearranging the original equation and \textcolor{red}{substituting in the means of $x$ and $y$}($y=\beta_0 + \beta_1 X$) to become Eq.\ref{eq:slr_ols_intercept}}

\begin{equation}
{\beta_0 =  \bar{y} - \beta_1 \bar{x}}
\label{eq:slr_ols_intercept}
\end{equation}

\subsubsection{Cost}

\textcolor{blue}{Cost or loss function (See \textcolor{red}{local ref?}) is used to define and quantitatively measure the error of the model -- the differences between the predicted and ground truth values. The differences between the training is called the residuals\index{residuals} or training errors where as the differences observed between the test predictions and ground truths are called the prediction or test errors.}

\textcolor{blue}{A common measure of the models fitness may be the {residual sum of squares (RSS)}\index{residual sum of squares (RSS)} (Eq.\ref{eq:rss_def}, where $y_i$ is the observed value and $f(x_i)$ is the predicted value)}

\begin{equation}
{\sum_{i=1}^{n}{(y_i - f(x_i))^2}}
\label{eq:rss_def}
\end{equation}

% see p62 of ISL for more

\subsubsection{Evaluation}

\textcolor{blue}{Several methods exist for measuring the models predictive capability (see \textcolor{red}{local ref?} for more details.)}


\subsection{Multiple Linear Regression}

\textcolor{blue}{Using $n$ predictors:}

\textcolor{blue}{generalization of simple linear regression. Uses multiple features to predict the response variable}

\textcolor{blue}{linear regression with multiple variables may be called "multivariate linear regression"}



\begin{equation}
{Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n}
\label{eq:mlr_ex}
\end{equation}


\subsection{Polynomial Regression}

% TODO: this needs to be written more clearly

\textcolor{blue}{Special case of multiple linear regression, models a linear relationship between a response variable and polynomial feature terms}

\textcolor{blue}{a linear model that, using polynomial feature terms, can model non-linear relationships.}

\textcolor{blue}{It is important to note that when representing features as polynomials, feature scaling becomes increasingly important. e.g. if a feature is on a 0-100 scale and the feature is cubed, the value is now on a 0-1000000 scale.}

\textcolor{blue}{Quadratic regression (second-order polynomial) shown in equation (EQ\ref{eq:quad_regression_def})}

\begin{equation}
{Y \approx \alpha + \beta_1 X + \beta_2 X^2}
\label{eq:quad_regression_def}
\end{equation}








