\chapter{Generative}

\r{Nice resource: "Generative Deep Learning"\cite{foster2019generative} and \TD{An Introduction to Deep Generative Modeling \cite{DBLP:journals/corr/abs-2103-05180}}}

\section{Generative Adversarial Networks (GANs)}

\TD{Generative Adversarial Networks \cite{Goodfellow2014GenerativeAN}}

\TD{NIPS 2016 Tutorial: Generative Adversarial Networks \cite{DBLP:journals/corr/Goodfellow17}}

\TD{``Inverse PM -- semi-famous interaction, stemming from this review describing/inquiring about the relation to https://web.archive.org/web/20160411075236/http://media.nips.cc/nipsbooks/nipspapers/paper\_files/nips27/reviews/1384.html '' "predictability minimisation" or PM\cite{schmidhuber1992learning}}

% TODO: how as this not been done yet? I think I've written some pretty nice text on this before
\r{Discriminator and generator}

\r{Tries to learn the underlying structure of the data}

%TODO:

\TD{Self-Attention Generative Adversarial Networks \cite{Zhang2019SelfAttentionGA} uses attention\cite{DBLP:journals/corr/abs-1711-07971}}

\TD{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \cite{Radford2015UnsupervisedRL}}

\TD{Mode collapse}

% TODO: wgan
\TD{Wasserstein GAN \cite{Arjovsky2017WassersteinG}}
\TD{Improved Training of Wasserstein GANs \cite{DBLP:journals/corr/GulrajaniAADC17}}
% TODO: DCGAN
\TD{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \cite{Radford2016UnsupervisedRL}}

\section{Variational AutoEncoder}

% TODO: I've written on this somewhere.... 9Oct21

\TD{Auto-Encoding Variational Bayes \cite{Kingma2014AutoEncodingVB}}

\TD{Tutorial on Variational Autoencoders \cite{Doersch2016TutorialOV}}

% consistency in latent space w/without augementations in VAE
\TD{Consistency Regularization for Variational Auto-Encoders \cite{DBLP:journals/corr/abs-2105-14859}}