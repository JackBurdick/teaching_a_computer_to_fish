\section{research to include}



%% 
\TD{synthetic petri dish -- inner and outer loop \cite{rawal2020synthetic}}


% use style gan to create augmented data.
\TD{DermGAN: Synthetic Generation of Clinical Skin Images with Pathology \cite{Ghorbani2019DermGANSG}}

\TD{Visual attention \cite{DBLP:journals/corr/XuBKCCSZB15}}

% TODO: index for ptr-nets / pointer networks
\TD{Pointer Networks (Ptr-Nets) \cite{Vinyals2015PointerN}}

\TD{Non-local Neural Networks \cite{DBLP:journals/corr/abs-1711-07971}}

% augmented AI
\TD{Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance \cite{Bansal2020DoesTW}}


% blog: https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html 
\TD{SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization \cite{Du2019SpineNetLS}}


% TODO: https://arxiv.org/abs/2006.16668 --- tomorrow?

% https://twitter.com/zacharynado/status/1276252197915942927
% ~``improve calibration on dataset shift''
\TD{Evaluating Prediction-Time Batch Normalization for Robustness under Covariate Shift \cite{Nado2020EvaluatingPB}}

% 1000s of tasks with little forgetting: https://twitter.com/Mitchnw/status/1278711255977492482
\TD{Supermasks in Superposition \cite{Wortsman2020SupermasksIS}}


% no normalization or skip connections - image classification, https://github.com/HaozhiQi/ISONet
\TD{Deep Isometric Learning for Visual Recognition \cite{Qi2020DeepIL}}


% training on synthetic data
\TD{Synthetic Data for Deep Learning \cite{Nikolenko2019SyntheticDF}}


% Sparsely-Gated MoE > 600B -- significant: 1T weights, but issues --
\TD{GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding \cite{Lepikhin2020GShardSG}}


% 
\TD{Bag of Tricks for Image Classification with Convolutional Neural Networks \cite{DBLP:journals/corr/abs-1812-01187}}




% TODO: group convolutions: https://colah.github.io/posts/2014-12-Groups-Convolution/


% TODO: create section for Test-Time Augmentation (TTA)
% TODO: index TTA
\TD{Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation \cite{Molchanov2020GreedyPS} (TTA)}

\TD{Training independent subnetworks for robust prediction \cite{DBLP:journals/corr/abs-2010-06610} --- MIMO, achieve the benefits of ensembled neural networks ``without the multiple forward passes''.}


% NOTE: possible paper on hyper parameter optimization
\TD{Bayesian Optimization for Selecting Efficient Machine Learning Models \cite{Wang2020BayesianOF}}

% 
\TD{The Hardware Lottery \cite{Hooker2020TheHL}}



\TD{pretraining with \textit{random} images, i.e. not Imagenet --- Self-supervised Pretraining of Visual Features in the Wild \cite{Goyal2021SelfsupervisedPO}}

\TD{RegNet --- Designing Network Design Spaces \cite{Radosavovic2020DesigningND}}



\TD{Neural Processes \cite{DBLP:journals/corr/abs-1807-01622}}

\TD{Why AI is Harder Than We Think \cite{DBLP:journals/corr/abs-2104-12871} ``everyone hurridly changes the names of their research projects to something else. This condition is called `AI winter' '' four fallacies}




\TD{Torch.manual\_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision \cite{Picard2021Torchmanual} ``even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average''. Shows that pretraining reduces variance, but it is still present. Limitations worth noting, but likely won't chage the high level message (HP are finiky, results can vary)}

\TD{Data and its (dis)contents: A survey of dataset development and use in machine learning research \cite{Paullada2020DataAI} ``Survey the many concerns raised about the way we collect and use data in machine learning and advocate that a more cautious and thorough understanding of data is necessary to address several of the practical and ethical issues of the field'' Bring up legal issues of creating datasets (Montreal data license)}



% TODO: ~improve reliability by multiple individual submodels
\TD{Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift \cite{Ovadia2019CanYT}}


% TODO: ~show that MoEs and ensembles complementary features and work well together
\TD{Sparse MoEs meet Efficient Ensembles \cite{Allingham2021SparseMM}}


% TODO: DenseNet
\TD{Densely Connected Convolutional Networks \cite{DBLP:journals/corr/HuangLW16a}}



% TODO: justification of DL concepts mathematically (haven't read this yet...)
\TD{Mathematics of Deep Learning \cite{DBLP:journals/corr/abs-1712-04741}}


% TODO: these likely should be cited together
% TODO: MultiModel
\TD{One Model To Learn Them All \cite{DBLP:journals/corr/KaiserGSVPJU17}}
\TD{Perceiver: General Perception with Iterative Attention \cite{DBLP:journals/corr/abs-2103-03206}}
\TD{Perceiver IO: \cite{DBLP:journals/corr/abs-2107-14795}}
% Multi modal multi task learning model
\TD{OmniNet: \cite{DBLP:journals/corr/abs-1907-07804}}




% TODO: neural ODE
\TD{Neural Ordinary Differential Equations \cite{DBLP:journals/corr/abs-1806-07366}}

\TD{BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 \cite{DBLP:journals/corr/CourbariauxB16}}

% TODO: there should be a "Additional papers to be aware of" section

% TODO: CycleGan and cycle consistency
% I've written about this before, but I'm unsure where it is...
\TD{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \cite{DBLP:journals/corr/ZhuPIE17}}


% TODO: read https://openai.com/blog/deep-double-descent/
\TD{Deep Double Descent: Where Bigger Models and More Data Hurt \cite{Nakkiran2020DeepDD}}

\TD{GROKKING: GENERALIZATION BEYOND OVERFIT-TING ON SMALL ALGORITHMIC DATASETS \cite{power2021grokking}}


% tf lattice
\TD{Monotonic Calibrated Interpolated Look-Up Tables \cite{DBLP:journals/corr/GuptaCPVCMM15}}


%% vision transformers
\TD{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \cite{DBLP:journals/corr/abs-2010-11929}}
% TODO: still need to read
\TD{Vision Transformers are Robust Learners \cite{DBLP:journals/corr/abs-2105-07581}}






% student teacher, transfering attention from one network to another, transfer learning?
\TD{Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer \cite{DBLP:journals/corr/ZagoruykoK16a}}


% possibly self-supervised?
\TD{Objects that Sound \cite{DBLP:journals/corr/abs-1712-06651}}

% optimization, gradient clipping, batch norm
\TD{Adaptive Gradient Clipping  --- High-Performance Large-Scale Image Recognition Without Normalization \cite{DBLP:journals/corr/abs-2102-06171}}


% TODO: to consider
\TD{Carbon Emissions and Large Neural Network Training \cite{DBLP:journals/corr/abs-2104-10350}}


% vision transformers and patches
\TD{Aggregating Nested Transformers \cite{DBLP:journals/corr/abs-2105-12723}}
\TD{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows \cite{DBLP:journals/corr/abs-2103-14030}}



\TD{Robust fine-tuning of zero-shot models \cite{Wortsman2021RobustFO}}


\TD{Learning Disentangled Representations with Semi-Supervised Deep Generative Models \cite{Narayanaswamy2017LearningDR}}


% I thought I already included this...
\TD{Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets~\cite{Power2022GrokkingGB}}

\TD{Deep Double Descent: Where Bigger Models and More Data Hurt \cite{DBLP:journals/corr/abs-1912-02292}}

\TD{Toward Trustworthy AI \cite{DBLP:journals/corr/abs-2004-07213}}

% weak supervision
% https://ai.stanford.edu/blog/weak-supervision/


\TD{Second-Order Neural ODE~\cite{DBLP:journals/corr/abs-2109-14158}}


% leslie smith tweet to paper
% https://twitter.com/lnsmith613/status/1480666855048159235?s=20
\TD{Machine Learning Application Development: Practitioners' Insights~\cite{Rahman2021MachineLA}}


% conv nets strike back??
% manual fine tuning >?  automl?
\TD{A ConvNet for the 2020s~\cite{Liu2022ACF}}

% TODO:
\TD{GOPHER: Scaling Language Models: Method~\cite{DBLP:journals/corr/abs-2112-11446}}
\TD{RETRO: Improving language models by retrieving from trillions of tokens~\cite{DBLP:journals/corr/abs-2112-04426}}

%%%%%
% reinforcement learning = sequence modeling problem ?
\TD{Decision Transformer: Reinforcement Learning via Sequence Modeling~\cite{DBLP:journals/corr/abs-2106-01345}}
\TD{Reinforcement Learning as One Big Sequence Modeling Problem~\cite{DBLP:journals/corr/abs-2106-02039}}
%%%%%



%
\TD{Deep Information Propagation~\cite{Schoenholz2017DeepIP}}

% self supervised learning
\TD{FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence~\cite{DBLP:journals/corr/abs-2001-07685}}


% TODO: synthetic gradients --- how I have I not included these?? they're my favorite
\TD{Gradients without Backpropagation~\cite{Baydin2022GradientsWB}}
\TD{Decoupled Neural Interfaces using Synthetic Gradients~\cite{DBLP:journals/corr/JaderbergCOVGK16}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\TD{importance of early stages in training neural networks.}
\TD{Time Matters in Regularizing Deep Networks: Weight Decay and Data  Augmentation Affect Early Learning Dynamic~\cite{DBLP:journals/corr/abs-1905-13277}}
\TD{The Early Phase of Neural Network Training~\cite{DBLP:journals/corr/abs-2002-10365}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% This maybe includes positional embeddings at each layer?
% talk of weight sharing and conditional computation in transformers
\TD{Universal Transformers~\cite{DBLP:journals/corr/abs-1807-03819}}



% Masks and permutation invariance, 
\TD{Transformer Dissection: An Unified Understanding for Transformerâ€™s Attention via the Lens of Kernel~\cite{Tsai2019TransformerDA}}



\TD{Factorized Attention: Self-Attention with Linear Complexities~\cite{DBLP:journals/corr/abs-1812-01243}}
