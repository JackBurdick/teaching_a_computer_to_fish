% TODO: I'm still not sure how/where to structure this

\section{Dense}

\TD{TODO}

\section{Convolutions}

% this reads strangely --> DNN on an image may not take advantage of the ``stationarity'' (statistics) of an image.

\r{When using a standard dense layer, all inputs are treated independently. However, adjacent pixels, on average, are highly highly correlated. For example, if there is a texture in the image, a similar pattern of pixels may occur repeatedly. Convolutions architecturally build in an implicit spatial structure to consider these spatial.}

% TODO: I'm not sure how I'm going to structure these yet or where I'll be placing them

% TODO: https://arxiv.org/abs/1904.11486
% https://www.youtube.com/watch?v=HjewNBZz00w


\TD{LeNet-5 \cite{lecun1998gradient}}

\r{Convolutions are built upon a lie -- that is we refer to the opperation as a convolution, yet it is in fact a cross-correlation operation since we don't rotate the kernel 180$\deg$. However, it is convention to refer to the operation as a convolution. For more, please see section \ref{conv_vs_cross}}

\r{translational invariance --- a property that relates to how a systems decisions are insensitive to the location of a features within an input. That is, if we're looking for an object or feature, our system shouldn't change if the object is in different locations within the input}

\TD{``Filter factorization'' (not the exact same definition of mathematical factorization)-- one $5\times5$ filter vs $2$ $3\times3$ filters stacked.  in the $5\times5$ there are $5\times5 = 25$ parameters, in the $3\times3$, there are $3\times3 \times 2 = 18$ learnable parameters, resulting in a ``cheaper'' operation.}

\TD{Neocognitron -- CNN paper prior to ``CNN''\cite{fukushima1982neocognitron}}

% Survey on CNNs
% TODO: a lot here -- good read
\TD{A Survey of the Recent Architectures of Deep Convolutional Neural Networks \cite{DBLP:journals/corr/abs-1901-06032}}


\TD{Squeeze-and-Excitation Networks \cite{DBLP:journals/corr/abs-1709-01507}}


% Graham Taylor
\r{weighted averaging operation in time or space}


\r{translation equivariant --- }

\TD{BlurPool --- ``fix is anti-aliasing by low-pass filtering before downsampling'' ---Making Convolutional Networks Shift-Invariant Again \cite{DBLP:journals/corr/abs-1904-11486}}


\r{spatial hierarchies --- \TD{TODO: figure raw data, abstract edges+, then more distinct images, then closer output to the output, then the final label}}


\r{typcially a feature extraction phase (consisting of convolutional and pooling layers) followed by a classifier block (dense layers).}

%%%% popular layer types
\textcolor{green}{TODO: feature maps, (height, width, and depth (also called channels axis)). Stride, filter size, depth. talk about parameters}

\r{The output feature map (every dimension in the depth axis is a feature/filter) --- after a convolution operation the depth of a layer is no longer representative of a color channel (like RGB), it is now representative of a feature extracted by the convolutional operation, these are called filters.}

\TD{Strided Convolution\cite{springenberg2014striving}}

\TD{Dilated Convolution --- `atrous' convolution. (famously used by wavenet), which is convenient in time series analysis.}

\r{weight tieing}


\textcolor{green}{TODO: figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example of convolution operation on 2d image \textcolor{green}{TODO}}
	\label{fig:conv_2d_example_calc}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-b}\hfil
	\caption{Figure example of convolution operation on 3d image \textcolor{green}{TODO}}
	\label{fig:conv_2d_depth_example_calc}
\end{figure}

\textcolor{green}{TODO: examples of how different filter values and strides can effect the output dimensions.}




\section{Pooling}

\TD{TODO: examples of max vs average pooling}

%%%%%% research
\textcolor{blue}{Pooling may not fully determine learned deformation stability -- possibly filter smoothness\cite{ruderman2018learned}}

\r{downsampling}

\r{Why? importance of reducing the number of params.}

\TD{L2-pooling}

\TD{L2-pooling over the features or channels.}

\TD{additional --- learned/parameterized pooling}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example of max pooling operation on 2d image \textcolor{green}{TODO: I want this figure to be basic 2d}}
	\label{fig:pooling_max_2d_ex_a}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-b}\hfil
	\caption{Figure example of average pooling operation on 3d image \textcolor{green}{TODO: I want this figure to be 3d}}
	\label{fig:pooling_avg_3d_ex_a}
\end{figure}


\r{may be better to use convolutional layers in place of the pooling layers\cite{springenberg2014striving}}

\section{Recurrent Cells}

% TODO: read this
% Recurrent / Echo state networks / ESN
\TD{The ``echo state'' approach to analysing and training recurrent neural networks-with an erratum note \cite{jaeger2001echo}}
\TD{Deep Echo State Network (DeepESN): A \cite{DBLP:journals/corr/abs-1712-04323}}

\subsection{Cell Advancements}

\subsubsection{LSTM}

% TODO: Nice overview of LSTMs: https://colah.github.io/posts/2015-08-Understanding-LSTMs/

Introduced in 1997 %\cite{hochreiter1997long}

\r{detect long term dependencies in sequence}

\r{two state vectors, short and long term}

\r{Main motivation: learning what to store in the long-term state and what to ``forget''.}

\r{at each time step, some information is ``stored'' and some information is ``forgotten''.}

\paragraph{variants}

\TD{Depth-Gated LSTM \cite{DBLP:journals/corr/YaoCVDD15}}

\TD{A Clockwork RNN \cite{DBLP:journals/corr/KoutnikGGS14}}

\TD{LSTM: A Search Space Odyssey \cite{DBLP:journals/corr/GreffSKSS15} --- survey of LSTM variants --- all variants are essentially equal.}


\paragraph{other directions}

% interesting paper on ``grid LSTMs'' -- not sure why they never become popular
\TD{Grid Long Short-Term Memory \cite{Kalchbrenner2016GridLS}}

\paragraph{Fully Connected Layers}


\begin{enumerate}[noitemsep,topsep=0pt]
	\item Main
	\item \textit{Gate Controllers}
	\begin{enumerate}[noitemsep,topsep=0pt]
		\item Forget
		\item Input
		\item Output
	\end{enumerate}
\end{enumerate}

\r{The gate controllers use a logistic activation fuction (output a range from 0 to 1). This output is then fed through an element-wise multiplication function and thus if the value is $0$, the gate is ``closed'', and $1$ if the gate is ``open''.}

\r{These gates are able to potentially:}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Recognize an important input
	\item Store the important input in a long-term state ()
	\item Preserve the information for as long as it's needed
	\item Extract the important information when needed
\end{enumerate}


\subparagraph{Main}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Main Layer DIAGRAM}}
	%\label{}
\end{figure}

\r{This allows for the same basic functionality as a ``standard'' RNN cell --- however, the output, rather than being only sent to the next cell, is now partially stored in the long-term state.}


\subparagraph{Forget}

\r{Determines which part of the long-term state is forgotten/erased.}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Forget Layer DIAGRAM}}
	%\label{}
\end{figure}



\subparagraph{Input}

\r{Determines which part of the output from the \textbf{main layer} are kept in the long-term state.}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Input Layer DIAGRAM}}
	%\label{}
\end{figure}

\subparagraph{Output}

\r{Determines which part of the long term state is ``relevant'' (read and output).}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Output Layer DIAGRAM}}
	%\label{}
\end{figure}


\paragraph{Other}

\subparagraph{Peephole Connections}

\r{In basic LSTM cells, the gate controller can only look at the input and previous short-term state. Peephole connections, proposed in 2000 \TD{cite gers2000recurrent} add an extra connection that allows for the gate controller to also see information from the long term state as well. }

\r{The previous long-term state also becomes an input to the forget and input gate. The current long-term state becomes an intput to the output gate.}



\subsubsection{GRU}

\r{The GRU (gated recurrent unit) is a varient of the LSTM cell \TD{cite - cho2014learning}. The main modifications include:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Both state vectors are merged into one state vector
	\item A single gate controller determines the \textbf{Forget} and \textbf{Input} gate
	\begin{itemize}[noitemsep,topsep=0pt]
		\item If the gate output is a 1, the input is open and the forget gate is closed. If the gate output is 0, the input gate is closed and the forget gate is open
	\end{itemize}
	\item \r{The output gate is removed and a new controller exists that controls which part of ht previous state will be ``shown'' to the main layer}. At each timestep the full state vector is output.
\end{itemize}

\subsection{Notes -- add}

\r{A recent paper \TD{greff2017lstm}, compares three LSTM variants and makes three main observations:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item no significant architecture improvements over LSTMs
	\item forget gate and the output activation function are the most critical components
	\item \TD{hyperparams...}
\end{itemize}




\section{Capsule Networks}

% TODO: capsule networks
\TD{Dynamic Routing Between Capsules \cite{DBLP:journals/corr/abs-1710-09829}}

\section{Attention}

\r{``An attention function can be described as mapping a query and a set of key-value pairs to an output,
	where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
	of the values, where the weight assigned to each value is computed by a compatibility function of the
	query with the corresponding key.'' \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}

\TD{Self-attention Does Not Need $O(n^{2})$ Memory~\cite{Rabe2021SelfattentionDN}}

%TODO: another blog to checkout https://distill.pub/2016/augmented-rnns/

\r{overview can be found here\cite{weng2018attention}}


\TD{The original attention mechanism is introduced\cite{Bahdanau2015NeuralMT}.}

% TF attention implementation (https://www.tensorflow.org/tutorials/text/nmt_with_attention)

\TD{Effective Approaches to Attention-based Neural Machine Translation \cite{DBLP:journals/corr/LuongPM15}}

\TD{Massive Exploration of Neural Machine Translation Architectures \cite{DBLP:journals/corr/BritzGLL17}}

% TODO: index for transformer
% 'self-attention'
\TD{Attention Is All You Need -- Transformer network --- multi-head self-attention mechanism, key-value pairs \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}

% self-attention \TD{Self-attention, less commonly intra-attention}
\TD{Long Short-Term Memory-Networks for Machine Reading \cite{DBLP:journals/corr/ChengDL16}}


%\TD{Nice table comparing mechanisms https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}

\TD{in above post\cite{weng2018attention}: soft vs hard attention and global vs local attention}

% ``heads learn redundant key/query projections'' --> share
% https://github.com/epfml/collaborative-attention
\TD{Multi-Head Attention: Collaborate Instead of Concatenate \cite{Cordonnier2020MultiHeadAC}}

% soft vs hard and global vs local

\TD{Describes two variants: a ``hard'' stochastic attention mechanism (trainable via ``maximizing an approximate variational lower bound'' or REINFORCE) and a ``soft'' deterministic attention mechanism(trainable by standard back-propagation) \cite{DBLP:journals/corr/XuBKCCSZB15}. Soft attention --- scores to all entities (is differenetiable but expensive) and hard attention --- only selects one entity (non-differentiable (and complicated, reinforcement learning), but requires less computation at inference)}


% TODO: does this make sense?
\TD{Non-linear projection for K,Q, and V~\cite{DBLP:journals/corr/abs-2111-10017}}


\subsubsection{Scoring Functions}

% TODO: https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#summary
\TD{table from \cite{weng2018attention}}


\subsection{Self-Attention}

\r{sometimes refered to as ``intra-attention''\cite{DBLP:journals/corr/VaswaniSPUJGKP17}. Keys, queries and values are all derived from the same sequence. \TD{Self-attention transforms a sequence to create a representation of itself.}}



\subsection{transformers}

% possibly useful: http://nlp.seas.harvard.edu/2018/04/03/attention.html

\TD{survey of recent transformer architectures \TD{Efficient Transformers: A Survey \cite{Tay2020EfficientTA}}}


% Factorized Attention to self-attention
\TD{Generating Long Sequences with Sparse Transformers \cite{DBLP:journals/corr/abs-1904-10509}}

% include reccurence:  "enables learning dependency beyond a fixed length" + "relative position encodings"
\TD{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context \cite{DBLP:journals/corr/abs-1901-02860}}

% extends DBLP:journals/corr/abs-1901-02860 -- 
% https://github.com/guolinke/TUPE
\TD{Compressive Transformers for Long-Range Sequence Modeling \cite{Rae2020CompressiveTF}}

% linear attention
\TD{Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention \cite{Katharopoulos2020TransformersAR}}

% 
\TD{Transformer with Untied Positional Encoding (TUPE) --- Rethinking Positional Encoding in Language Pre-training \cite{Ke2020RethinkingPE}}



\TD{Reformer: The Efficient Transformer \cite{Kitaev2020ReformerTE}}


% TODO: top-down attention
% related to self-attention
% https://twitter.com/thomaskipf/status/1277570203665170432
\TD{Object-Centric Learning with Slot Attention \cite{Locatello2020ObjectCentricLW}}

\TD{Recurrent Independent Mechanisms \cite{Goyal2019RecurrentIM}}

% DETR -- also object detection
\TD{End-to-End Object Detection with Transformers \cite{Carion2020EndtoEndOD}}


% TODO: read https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html


\subsection{Positional Encodings}

\TD{Positional embedding and positional encoding tend to be used interchangably. However, typically an encoding means ``fixed'' while an embedding means ``learned'' or ``trainable''.}

% TODO: example of how word order matters (not is a good example)

\r{Attention/transformers view the inputs as sets, that is there is no order associated with each input. All information enters the attention block at once. This is in contrast to something like a recurrent model, in which the order of the inputs is implicit.}

\r{trade off: potentially faster (remove the dependancy of doing operations sequentially) and can also possibly help capture longer range dependancies (without additional complexity e.g. skip connections)}

\r{(re)introducing order to the input by including additional information -- the ``positional embedding''.}

\r{NOTE: Great blog posts on this subject~\cite{kazemnejad_2021, kernes_2021, kernes_2021B}}

\subsubsection{Positional Encoding Value}

\r{why not add linear/progressive value signifying order?}

\r{This would be called an aboslute positional embedding}

\r{Include index information [0, n], where n is the length of the sentance (minus 1). This could lead to magnitude issues. Where the singal from the word embeddings is ``washed out'' by the positional embedding.  Another consideration is that (may or may not be an issue depending on the application) is that you'd like to ensure you have the largest sequence in the training set that you expect to see in evaluation set. For example, if you only see sequences of length $25$ in the training data and then see a sequence of length $32$ during inference. The model will be unsure what to do with values $25 - 31$ (zero indexing). Depending on how you include the positional embedding (e.g. additive or concat), the model may misinterpret the values or be largely/entirely unsure what to make of these previously unseen values.}
	
	
\r{To address this you could either increase the magnitude of the word embeddings or normalize/scale the positional embedding.}

\r{However, niether are ideal.}

\r{Increasing the magnitude of the word embeddings would possibly work, though you may consider issues with exploding values in the network, but you'd still have a similar issue to what would happen if you normalized the positional embedding. }

\r{That is, the normalized positional embeddings may encode different information when the sentances are longer or shorter -- the delta between words in a 5 word sentance vs a 20 word sentance doesn't have a consistent meaning}

% NOTE: haven't read this yet (I don't think, though the link is purple...)
\TD{Self-Attention with Relative Position Representations~\cite{DBLP:journals/corr/abs-1803-02155}}

\r{Ideally the embedding would be able to account for all the issues we discussed.}

\begin{itemize}[noitemsep,topsep=0pt]
	\item consistent delta between each position
		\begin{itemize}[noitemsep,topsep=0pt]
			\item regardless of sequence length, if an instance is one instance away from another, the positional encoding should be the same e.g. in a length four sequence the positional encoding should be the same from instances $1$ and $2$ as it is for instances $19$ and $20$ in a length $22$ sequence.
		\end{itemize}
	\item generalize to sequence lengths unseen in training
\end{itemize}

\r{additionally, we'd prefer to have each instance in the sequence be unique. That is the positional encoding for one instance shouldn't be the same as another in the same sequence (e.g. two words in a sentance).}

\paragraph{Positional Encoding Value(s)}

\r{Rather than use a single value, a possible solution is to use an array of values.}

\TD{Relative positional encoding (rather than absolute).}


\TD{What if we were to use a binary array to represent each location?}

\TD{issue with binary}

\TD{}


\TD{CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings~\cite{DBLP:journals/corr/abs-2106-03143}}

% NOTE: possibly relevant: https://aclanthology.org/2021.emnlp-main.266.pdf

\paragraph{Sinusoidal}

\TD{include figure with multiple frequencies and points on the x and y axis leading to embeddings}

\subsection{Positional Embeddings (learned ``encodings'')}

% possibly useful: https://theaisummer.com/positional-embeddings/

\TD{Learning to Encode Position for Transformer with Continuous Dynamical Model~\cite{DBLP:journals/corr/abs-2003-09229}}


\TD{What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding~\cite{DBLP:journals/corr/abs-2010-04903}}

\subsubsection{Including Positional Embeddings}

% someones thoughts on  additive vs concat: https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/exs7d08/

\paragraph{Additive}

\TD{saves memory (over concatenation -- less dimensions)}

\TD{figure}

\paragraph{Concatenation}

\TD{figure}


\section{MLP-Mixer}

\r{MLPs that are used to ``mix'' tokens (spatial) and ``mix'' channels (features)}

% possible blog: https://wandb.ai/wandb_fc/pytorch-image-models/reports/Is-MLP-Mixer-a-CNN-in-Disguise---Vmlldzo4NDE1MTU

% MLP resurgence
\TD{Do You Even Need Attention? A \cite{DBLP:journals/corr/abs-2105-02723}}

\TD{gMLP (Pay Attention to MLPs) \cite{DBLP:journals/corr/abs-2105-08050}}

\TD{MLP-Mixer: An all-MLP Architecture for Vision \cite{DBLP:journals/corr/abs-2105-01601}}

\TD{RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition \cite{DBLP:journals/corr/abs-2105-01883}}

\TD{ResMLP: Feedforward networks for image classification with data-efficient training \cite{DBLP:journals/corr/abs-2105-03404}}
Conncurrent papers released looking to replace attention with MLPs.

\TD{Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet \cite{MelasKyriazi2021DoYE}}




\section{Mixture of Experts (MoE)}

\TD{Breaking down a problem (task) into multiple sub-problems (sub-tasks), training and expert in each sub-problem, then learning a meta/gating model that routes information to a specific expert and combines outputs}

% Divide and conquer vs meta-learning approach


\TD{High level steps}
\begin{itemize}[noitemsep,topsep=0pt]
	\item Decompose task into subtasks
	\item Learn ``expert'' for each subtask 
	\item Decide which expert to use (gating model or gating expert)
	\item Combine outputs as needed (pool/aggregate/select)
\end{itemize}

\TD{``20 years MoE''~\cite{yuksel2012twenty}}

\TD{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer~\cite{shazeer2017outrageously}}

