% TODO: I'm still not sure how/where to structure this

\section{Dense}

\TD{TODO}

\section{Convolutions}

% this reads strangely --> DNN on an image may not take advantage of the ``stationarity'' (statistics) of an image.

\r{When using a standard dense layer, all inputs are treated independently. However, adjacent pixels, on average, are highly highly correlated. For example, if there is a texture in the image, a similar pattern of pixels may occur repeatedly. Convolutions architecturally build in an implicit spatial structure to consider these spatial.}

% TODO: I'm not sure how I'm going to structure these yet or where I'll be placing them

% TODO: https://arxiv.org/abs/1904.11486
% https://www.youtube.com/watch?v=HjewNBZz00w


\TD{LeNet-5 \cite{lecun1998gradient}}

\r{Convolutions are built upon a lie -- that is we refer to the opperation as a convolution, yet it is in fact a cross-correlation operation since we don't rotate the kernel 180$\deg$. However, it is convention to refer to the operation as a convolution. For more, please see section \ref{conv_vs_cross}}

\r{translational invariance --- a property that relates to how a systems decisions are insensitive to the location of a features within an input. That is, if we're looking for an object or feature, our system shouldn't change if the object is in different locations within the input}

\TD{``Filter factorization'' (not the exact same definition of mathematical factorization)-- one $5\times5$ filter vs $2$ $3\times3$ filters stacked.  in the $5\times5$ there are $5\times5 = 25$ parameters, in the $3\times3$, there are $3\times3 \times 2 = 18$ learnable parameters, resulting in a ``cheaper'' operation.}

\TD{Neocognitron -- CNN paper prior to ``CNN''\cite{fukushima1982neocognitron}}

% Survey on CNNs
% TODO: a lot here -- good read
\TD{A Survey of the Recent Architectures of Deep Convolutional Neural Networks \cite{DBLP:journals/corr/abs-1901-06032}}


\TD{Squeeze-and-Excitation Networks \cite{DBLP:journals/corr/abs-1709-01507}}


% Graham Taylor
\r{weighted averaging operation in time or space}


\r{translation equivariant --- }

\TD{BlurPool --- ``fix is anti-aliasing by low-pass filtering before downsampling'' ---Making Convolutional Networks Shift-Invariant Again \cite{DBLP:journals/corr/abs-1904-11486}}


\r{spatial hierarchies --- \TD{TODO: figure raw data, abstract edges+, then more distinct images, then closer output to the output, then the final label}}


\r{typcially a feature extraction phase (consisting of convolutional and pooling layers) followed by a classifier block (dense layers).}

%%%% popular layer types
\textcolor{green}{TODO: feature maps, (height, width, and depth (also called channels axis)). Stride, filter size, depth. talk about parameters}

\r{The output feature map (every dimension in the depth axis is a feature/filter) --- after a convolution operation the depth of a layer is no longer representative of a color channel (like RGB), it is now representative of a feature extracted by the convolutional operation, these are called filters.}

\TD{Strided Convolution\cite{springenberg2014striving}}

\TD{Dilated Convolution --- `atrous' convolution. (famously used by wavenet), which is convenient in time series analysis.}

\r{weight tieing}


\textcolor{green}{TODO: figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example of convolution operation on 2d image \textcolor{green}{TODO}}
	\label{fig:conv_2d_example_calc}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-b}\hfil
	\caption{Figure example of convolution operation on 3d image \textcolor{green}{TODO}}
	\label{fig:conv_2d_depth_example_calc}
\end{figure}

\textcolor{green}{TODO: examples of how different filter values and strides can effect the output dimensions.}




\section{Pooling}

\TD{TODO: examples of max vs average pooling}

%%%%%% research
\textcolor{blue}{Pooling may not fully determine learned deformation stability -- possibly filter smoothness\cite{ruderman2018learned}}

\r{downsampling}

\r{Why? importance of reducing the number of params.}

\TD{L2-pooling}

\TD{L2-pooling over the features or channels.}

\TD{additional --- learned/parameterized pooling}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example of max pooling operation on 2d image \textcolor{green}{TODO: I want this figure to be basic 2d}}
	\label{fig:pooling_max_2d_ex_a}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-b}\hfil
	\caption{Figure example of average pooling operation on 3d image \textcolor{green}{TODO: I want this figure to be 3d}}
	\label{fig:pooling_avg_3d_ex_a}
\end{figure}


\r{may be better to use convolutional layers in place of the pooling layers\cite{springenberg2014striving}}

\section{Recurrent Cells}

% TODO: read this
% Recurrent / Echo state networks / ESN
\TD{The ``echo state'' approach to analysing and training recurrent neural networks-with an erratum note \cite{jaeger2001echo}}
\TD{Deep Echo State Network (DeepESN): A \cite{DBLP:journals/corr/abs-1712-04323}}

\subsection{Cell Advancements}

\subsubsection{LSTM}

% TODO: Nice overview of LSTMs: https://colah.github.io/posts/2015-08-Understanding-LSTMs/

Introduced in 1997 %\cite{hochreiter1997long}

\r{detect long term dependencies in sequence}

\r{two state vectors, short and long term}

\r{Main motivation: learning what to store in the long-term state and what to ``forget''.}

\r{at each time step, some information is ``stored'' and some information is ``forgotten''.}

\paragraph{variants}

\TD{Depth-Gated LSTM \cite{DBLP:journals/corr/YaoCVDD15}}

\TD{A Clockwork RNN \cite{DBLP:journals/corr/KoutnikGGS14}}

\TD{LSTM: A Search Space Odyssey \cite{DBLP:journals/corr/GreffSKSS15} --- survey of LSTM variants --- all variants are essentially equal.}


\paragraph{other directions}

% interesting paper on ``grid LSTMs'' -- not sure why they never become popular
\TD{Grid Long Short-Term Memory \cite{Kalchbrenner2016GridLS}}

\paragraph{Fully Connected Layers}


\begin{enumerate}[noitemsep,topsep=0pt]
	\item Main
	\item \textit{Gate Controllers}
	\begin{enumerate}[noitemsep,topsep=0pt]
		\item Forget
		\item Input
		\item Output
	\end{enumerate}
\end{enumerate}

\r{The gate controllers use a logistic activation fuction (output a range from 0 to 1). This output is then fed through an element-wise multiplication function and thus if the value is $0$, the gate is ``closed'', and $1$ if the gate is ``open''.}

\r{These gates are able to potentially:}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Recognize an important input
	\item Store the important input in a long-term state ()
	\item Preserve the information for as long as it's needed
	\item Extract the important information when needed
\end{enumerate}


\subparagraph{Main}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Main Layer DIAGRAM}}
	%\label{}
\end{figure}

\r{This allows for the same basic functionality as a ``standard'' RNN cell --- however, the output, rather than being only sent to the next cell, is now partially stored in the long-term state.}


\subparagraph{Forget}

\r{Determines which part of the long-term state is forgotten/erased.}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Forget Layer DIAGRAM}}
	%\label{}
\end{figure}



\subparagraph{Input}

\r{Determines which part of the output from the \textbf{main layer} are kept in the long-term state.}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Input Layer DIAGRAM}}
	%\label{}
\end{figure}

\subparagraph{Output}

\r{Determines which part of the long term state is ``relevant'' (read and output).}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Output Layer DIAGRAM}}
	%\label{}
\end{figure}


\paragraph{Other}

\subparagraph{Peephole Connections}

\r{In basic LSTM cells, the gate controller can only look at the input and previous short-term state. Peephole connections, proposed in 2000 \TD{cite gers2000recurrent} add an extra connection that allows for the gate controller to also see information from the long term state as well. }

\r{The previous long-term state also becomes an input to the forget and input gate. The current long-term state becomes an intput to the output gate.}



\subsubsection{GRU}

\r{The GRU (gated recurrent unit) is a varient of the LSTM cell \TD{cite - cho2014learning}. The main modifications include:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Both state vectors are merged into one state vector
	\item A single gate controller determines the \textbf{Forget} and \textbf{Input} gate
	\begin{itemize}[noitemsep,topsep=0pt]
		\item If the gate output is a 1, the input is open and the forget gate is closed. If the gate output is 0, the input gate is closed and the forget gate is open
	\end{itemize}
	\item \r{The output gate is removed and a new controller exists that controls which part of ht previous state will be ``shown'' to the main layer}. At each timestep the full state vector is output.
\end{itemize}

\subsection{Notes -- add}

\r{A recent paper \TD{greff2017lstm}, compares three LSTM variants and makes three main observations:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item no significant architecture improvements over LSTMs
	\item forget gate and the output activation function are the most critical components
	\item \TD{hyperparams...}
\end{itemize}




\section{Capsule Networks}

% TODO: capsule networks
\TD{Dynamic Routing Between Capsules \cite{DBLP:journals/corr/abs-1710-09829}}

\section{Attention}

%TODO: another blog to checkout https://distill.pub/2016/augmented-rnns/

\r{overview can be found here\cite{weng2018attention}}


\TD{The original attention mechanism is introduced\cite{Bahdanau2015NeuralMT}.}

% TF attention implementation (https://www.tensorflow.org/tutorials/text/nmt_with_attention)

\TD{Effective Approaches to Attention-based Neural Machine Translation \cite{DBLP:journals/corr/LuongPM15}}

\TD{Massive Exploration of Neural Machine Translation Architectures \cite{DBLP:journals/corr/BritzGLL17}}

% TODO: index for transformer
% 'self-attention'
\TD{Attention Is All You Need -- Transformer network --- multi-head self-attention mechanism, key-value pairs \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}

% self-attention \TD{Self-attention, less commonly intra-attention}
\TD{Long Short-Term Memory-Networks for Machine Reading \cite{DBLP:journals/corr/ChengDL16}}


%\TD{Nice table comparing mechanisms https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}

\TD{in above post\cite{weng2018attention}: soft vs hard attention and global vs local attention}

% ``heads learn redundant key/query projections'' --> share
% https://github.com/epfml/collaborative-attention
\TD{Multi-Head Attention: Collaborate Instead of Concatenate \cite{Cordonnier2020MultiHeadAC}}

\subsection{transformers}

\TD{survey of recent transformer architectures \TD{Efficient Transformers: A Survey \cite{Tay2020EfficientTA}}}


% Factorized Attention to self-attention
\TD{Generating Long Sequences with Sparse Transformers \cite{DBLP:journals/corr/abs-1904-10509}}

% include reccurence:  "enables learning dependency beyond a fixed length" + "relative position encodings"
\TD{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context \cite{DBLP:journals/corr/abs-1901-02860}}

% extends DBLP:journals/corr/abs-1901-02860 -- 
% https://github.com/guolinke/TUPE
\TD{Compressive Transformers for Long-Range Sequence Modeling \cite{Rae2020CompressiveTF}}

% linear attention
\TD{Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention \cite{Katharopoulos2020TransformersAR}}

% 
\TD{Transformer with Untied Positional Encoding (TUPE) --- Rethinking Positional Encoding in Language Pre-training \cite{Ke2020RethinkingPE}}



\TD{Reformer: The Efficient Transformer \cite{Kitaev2020ReformerTE}}


% TODO: top-down attention
% related to self-attention
% https://twitter.com/thomaskipf/status/1277570203665170432
\TD{Object-Centric Learning with Slot Attention \cite{Locatello2020ObjectCentricLW}}

\TD{Recurrent Independent Mechanisms \cite{Goyal2019RecurrentIM}}

% DETR -- also object detection
\TD{End-to-End Object Detection with Transformers \cite{Carion2020EndtoEndOD}}


% TODO: read https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html


\section{NTM (Neural Turing Machines)}
% nerual network + external memory storage
% controller + memory
\TD{Neural Turing Machines \cite{DBLP:journals/corr/GravesWD14}}
