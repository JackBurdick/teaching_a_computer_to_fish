

\textcolor{blue}{tensorflow's XLA compiler}

\textcolor{blue}{checkpoints -- allow: save/stop/resume training, resume on failure, predict from point}


%%%% may talk about in production
% Google Cloud MLE


%%%
% stats vs ML -- in ML you may keep outliers and build models for them. in ML outliers may be collapsed (capped) and in statistics they may be removed
% ML is used to learn the ``long tail'', make fine grained predictions, not just gobal averages

%% why try to stay in linear (like with feature crosses)
% NN with many layers are non-convex
% optimizing linear models is a convex problem (much easier)

% how have I not talked about transfer learning yet?

% optimizing is an NP-hard, non-convex optimization problem (coursera.need to double check)
\textcolor{blue}{L0-norm (the count of non-zero weights).}


%%%%


%%%
\textcolor{blue}{Each layer in a DNN is a composition of the previous layer. i.e. if layer 1 = f(x), then layer 1 = g(f(x)), layer three is h(g(f(x)) \textcolor{green}{TODO: create diagram}}



%%%% rough...
%\textcolor{red}{Multi-headed inference. Useful when using the same model for different tasks. e.g. using a model for one task, then later deciding to perform a similar task on the same data -- rather than train an entirely new model, the original model may be performing may of the same computations. }



%%%  preprocessing, this wasn't already somewhere? % plus index
\textcolor{blue}{whitening}

%%% unsupervised method
\textcolor{blue}{NFM (Non-Negative Matrix Factorization)}

%%%
\textcolor{blue}{TODO: parameter calculation -- use VGG example (conv + dense)}
\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{5}{|c|}{\textbf{Number of VGG-16 Parameters}}     \\ \hline
		Layer & Out Shape & Weights & Bias & Total  \\ \hline
		\emph{Convolution}        & & $(in)\times(h\times w)\times(out)$ & $(out)$ & $weights+bias$    \\ \hline
		Conv3-64          & $224\times224\times64$ & $3\times(3\times3)\times64$ & $64$ & 1792    \\ \hline
		Conv3-64 (p)      & $112\times112\times64$ & $64\times(3\times3)\times64$ & $64$ & 36928    \\ \hline
		Conv3-128         & $112\times112\times128$ & $64\times(3\times3)\times128$ & $128$ & 73856     \\ \hline
		Conv3-128 (p)     & $56\times 56\times 128$ & $128\times(3\times3)\times128$ & $128$ & 147584   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $128\times(3\times3)\times256$ & $256$ & 295168   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-256 (p)     & $28\times 28\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $256\times(3\times3)\times512$ & $512$ & 1180160  \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $7\times 7\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808    \\ \hline
		\emph{dense}         & & $(in)\times(num)$ & $(out)$ & $weights+bias$    \\ \hline
		fc1 (4096)        & 4096 &$(512\times7\times7)\times4096$ & $4096$ & $102764544$    \\ \hline
		fc2 (4096)        & 4096 &$(4096)\times4096$ & $4096$ & $16781312$    \\ \hline
		fc3 (1000)        & 1000 &$(4096)\times1000$ & $1000$ & $4097000$    \\ \hline
		\emph{Total} & & & & 138,357,544 \\ \hline
	\end{tabular}
	\caption{Calculation of VGG parameters. (p) denotes that the layer is followed by a pooling layer (which does not affect the parameter count)}
	\label{tab:vgg_parameter_count}
\end{table}




%%%
\textcolor{green}{TODO: NN from scratch in appendix}

%%%
\textcolor{green}{TODO: CNN without layers API -- in github}

%%% interactive
\textcolor{blue}{The TensorFlow playground~\cite{tf_playground} (\textcolor{green}{TODO: screen shot}).  There are two other notable projects --- deeplearn.js~\cite{deeplearnjs} and ConvNetJS~\cite{convnet_js}.}


%%% distributed
\textcolor{blue}{Distributed TensorFlow Section}


%%% CUDA
\textcolor{blue}{CUDA (Compute Unified Device Architecture) library --- computation}
\textcolor{blue}{cuDNN (CUDA Deep Neural Network) library --- library of GPU-accelerated DNN primitives (activations, normalization, convolutions, pooling)}


%% model quantization
\textcolor{blue}{drop the parameter float precision from 32 bits (\code{tf.float32}) to 16 bits (\code{tf.bfloat16})}

%%%%
\textcolor{blue}{performance metrics --- Bayes error}

\textcolor{blue}{performance metrics --- Ability to perform on new, previously unseen data, generalization error (test error)}


\textcolor{green}{TODO: figure similar to DL pipeline in my thesis and similar to figure 1.5 in DL book. DL automates feature engineering}


\textcolor{green}{MNIST: described by Geoffrey Hinton as ``the drosophilia'' of machine learning. --- fruitflies are often used in biology as controlled laboratory experiments. Also considered the ``hello world'' of deep learning. The dataset is created from grayscale images ($28 \times 28 \times 1$) and has 10 labels 0-9.  There exist 60,000 training images and 10,000 test images. Created by the National Institute of Standards and Technology.}


\textcolor{green}{Two camps of statistics: frequentist estimators and bayesian inference.}


% TODO: figure -- [tensorflow ] -> cudnn -> cuda -> GPU | -> cuda -> GPU | -> [lib] -> CPU


%%
\textcolor{blue}{nonstationary problems --- unsolvable --- need data that makes sense for the problem. Predicting rentals of snowmobiles doesn't make as much sense if you only have data from summer. predicting cloth purchases during summer doesn't make sense if you only have data from winter.}


%%%
\textcolor{green}{Visualizing the output of a CNN}

\begin{enumerate}
	\item Filters: \r{help understand what visual patter a filter is receptive to \textcolor{green}{TODO: lots of examples and refs to implementations}}
	\item Intermediate outputs: \r{help understand the hierarchy of what is important to the classification task \textcolor{green}{TODO: lots of examples and refs to implementations}}
	\item Heatmaps of activation: \r{Help understand what parts (and by relatively how much) of an image were in its classification. \textcolor{green}{TODO: lots of examples and refs to implementations}}
\end{enumerate}


%%
\r{NOTE: somewhere about terms 'higher' and 'lower' levels of an architecture and how they're meaningless/interchangable and depend on context e.g. a diagram vs concept of lower=being closer to raw input.}

%%
\r{dataset == a ``sample'' in statistics}

%%
\r{decision boundary -- separation of classes}

%%

\r{prior knowledge --- additional knowledge about the desired form of a solution that is not obvious in the training data.  Inclusion of prior knowledge may influence the design of a solution, usually though preprocessing.}


\r{scale invariance --- a property that relates to how a systems decisions are insensitive to a uniform resizing of a feature within an input }

%%
\r{Intrisic dimensionality --- }


%%
\r{predictions on new data --- good ``generalization'' -- error/performance metrics}

%%
\r{controlling the complexity of a model = controlling the number of parameters. controlling the \textit{effective complexity} is optimizing the generalization performance of the model (using a penalty term (regularization)).}


\r{radial basis function --- }


% page 16 or neural networks (p31 on tablet)
\r{\textcolor{green}{(Barron 1993)} -- \textcolor{red}{``neural networks offer a dramatic advantage for function approximation in spaces of many dimensions''} --- (+) efficient scaling with dimensionality --- (-) now a non-linear optimiation problem = computationally intensive and may include many minimal in the error function}


%%
\r{\textit{joint probability} --- $x_a$ and $x_b$. \textit{conditional probability} --- P of $x_b$, given $x_a$}

%% TODO: stylegan
\TD{https://arxiv.org/abs/1812.04948}


%% BERT - GPT2

%%
\TD{\cite{smith2018disciplined}}


%%
\r{read this https://ai.googleblog.com/2019/06/innovations-in-graph-representation.html}

%% curiousity and "...allow the agent to create rewards for itself..."
\r{- (link: https://www.frontiersin.org/articles/10.3389/neuro.12.006.2007/full)
	- https://pathak22.github.io/noreward-rl/
	- (link: https://arxiv.org/abs/1705.05363) arxiv.org/abs/1705.05363
	- (link: https://arxiv.org/abs/1810.02274) arxiv.org/abs/1810.02274)
	- https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html}


%%
\TD{Kohonen networks (self-organizing maps) \TD{Improving Self-Organizing Maps with Unsupervised Feature Extraction \cite{Khacef2020ImprovingSM}}}

%%
% p.20 of Neural Networks 
\r{``the outputs of neural networks can be interpreted as (approximations to) posterior probabilities''} \r{two stages in a classification process 1) \textit{inference} --- data is used to determine the values for the posterior probablities 2) \textit{decision making} --- the probabilities are used to make new decisions}


%%
\textcolor{green}{TODO: Bayesian vs frequentist --- see page 21 of neural networks (p36 on tablet)}

%%
\r{sigmoid == `S-shaped' == the logistic form of sigmoid maps $(-\infty, \infty)$ to $(0, 1)$}

%%
\r{linearly seperable --- where a dataset can be correctly seperated by a linear (hyperplanar) decision boundry}
\r{non linearly separable --- two-dimensional excllusive OR problem. --- generalized to d-dimensions when it is knowns as the d-bit parity problem.}

%%
\r{small changes in inputs, ideally, should not generally lead to dramatic changes in the outputs --- leading to a mapping that is represented relatively smoothly.}

%%
\r{colinear --- \textcolor{green}{TODO}}

%%
\TD{robbins-monro procedure}

%%
\r{\TD{perceptron convergence theorem} --- ``for any linearly separable data set, the learning rule (see fig 3.68 in NN - p100), is guaranteed to find a solution in a finite number of steps.'')}

\TD{the ``pocket algorithm'' --- finding solutions to problems which are not linearly separable.}

%%
\r{``projection pursuit regression'' --- similar to a two-layer feed-forward neural network --- typcially the parameters, rather than being all updated simultaneously such as in a nerual network ,are optimized cyclically in groups. Training takes place for one hidden unit at a time}
% another framework for non-linear regression
\r{``generalized additive models \TD{Hastie and Tibshirani, 1990} --- restrictive class of models since it does not allow for interactions between the input variables. === a generalization that does allow for interactions is the ``multi-variate adaptive regression splies (MARS) \TD{(Friedman, 1991)}}


%%
% this is pretty interesting... checkout p136 of NN_bishop, p152 tablet
\TD{Kolmogorov's Theorem}

%%
% p148(163) of NN Bishop
\TD{Jacobian Matrix}
% p150(165) of NN Bishop
\TD{Hessian Matrix}
% diagonal approximation
% outer product approximation
% inverse hessian
% finite differences
% exact evaluation of the hessian
\TD{Radial Basis Functions}


%% Look into
\TD{Max norm constraints --- enforce an absolute upper bound.}


%% Unsupervised pretraining - 2006, Hinton
\TD{greedy layer-wise unsupervised pretraining. greedy since each portion of the network is trained independently. Typically, today, layers are trained jointly using backpropagation}



\subsection{Restricted Boltzmann Machines}
\r{Restricted Boltzmann machines (RBMs) -- no output layer}

\TD{Deep Belief Networks (DBN) --- RBMs linked together to form multistage neural netowrk. Each RBM generates a representation of the data that the subsequent layer builds upon.  --- may be used as feature detectors}

\TD{neural network paper\cite{hansen1990neural}}

\TD{Bracketing: in the context of optimizers? may be work mentioning?}

%%
\r{code resuse and \code{tf.reuse}}

%%
\r{trying to learn the activation function -- however, this is partially already done by using multiple layers}

%%
\r{``learning is cast as an optimization problem -- minimize the loss with regularization''}

%% debugging
\TD{practical advice --- 1) looking at the norm of the outputs? 2) looking at the gradients at different layers and making sure they're relatively in the same magnitude i.e. you wouldn't want very small gradients at one level and super large gradients in another layer. 3) finite-difference approximation of the gradient (``gradient checking''). forward pass + epsilon vs forward pass - epsilon should be roughly the same.  $\frac{f(x+\epsilon) - f(x-\epsilon)}{2\epsilon}$ 4) ensure overfitting is possible on a small dataset, this should reach ``perfect'' (at least for classification on say a sample size of 50)}



%% Hugo talk
\r{relu may act as a gate, in which values are on or off}

%% increasing generalization
\r{dropout on the input layer of an autoencoder}


%%
\r{previously thought that the neural networkworks were difficult to train because they often get stuck in local minima --- however, this is now thought to be untrue and that many of the local minima points are saddle points. \TD{CITATION?}}

\TD{flat minima generalize better --- 1997 paper Hock}


%% padding -- how has this not been written about yet?
\TD{padding --- padding examples --- half(same), full / valid, no padding, reflect padding.}


\TD{mixed effect is a combination of fixed and random effects}

% in relation to imagery/spatial data
\r{local contrast normalization}

%%% upsampling
\r{\begin{itemize}[noitemsep,topsep=0pt]
		\item nearest neighbors
		\item ``bed of nails'' - copy to one position
		\item ``max unpooling'' -- remember which position the value came from
		\item learnable (transpose convolution) --- fractional stride -- ratio of in to out, stride of 1 on input and n on output.
	\end{itemize}
}


\TD{In vs Out of memory data}


% Resources:
% 1. https://medium.com/tensorflow/introducing-tensorflow-model-analysis-scaleable-sliced-and-full-pass-metrics-5cde7baf0b7b
\r{A \textit{dynamic placer} algorithm is presented in the {TensorFlow whitepaper}~\cite{abadi2016tensorflow_device_placement} that is capable of automatically distributing operations across devices. This algorithm takes into consideration  estimates of the sizes of input and output tensors, computation time for each node, \textcolor{green}{OTHERS - TODO:read entire paper}.}

% TODO: see slides I downloaded by Andrew Zisserman
\r{self-supervised learning}


%% shuffling
\r{include shuffling information}

\r{multinomial distribution}

\r{``probabalistic matrix factorization''}

\r{``affine functions''}

\TD{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer~\cite{shazeer2017outrageously}}


\TD{``weight norm'' or weight normalization \cite{salimans2016weight}}


\r{Show a graph of traditional ML, small - large DL and how in data limited scenarios the ordering is difficult (but need to include something about effort) and tends to skew toward ML, but in large data, tends to scew towards large DL}

\TD{trend in moving from intermediate representations (hand engineered features) and toward fully learned/raw data to predictions.}



\subsection{Greek letters}
\TD{basic Greek letters}


\subsection{Basic Log Math}
\TD{include basic log math}

\subsection{Basic Matrix Math}
\TD{include basic log math}

\r{matrix decomposition}

\subsection{Basic Linear Algebra}
\TD{basic LA}


\subsection{Representation Learning}

% p.4 of DL
\r{Representation learning: learn the representation from input to output, not just the mapping.}
\r{form low(er) dimensional features}
\r{latent space}


\r{snapshot ensembles \cite{huang2017snapshot} - obtain parameters at each lowest point and ensemble}


\r{entity embeddings \cite{guo2016entity}}

\subsection{others}

% external memory
\r{external memory \cite{santoro2016meta, garnelo2018conditional, munkhdalai2017meta}}


% few shot learning
\r{Protoypical networks \cite{snell2017prototypical}}


% what it is, etc.
\TD{Metric learning}

% TODO: possibly useful: https://towardsdatascience.com/metric-learning-loss-functions-5b67b3da99a5
\TD{metric learning survey \cite{kaya2019deep}}


% https://weightagnostic.github.io/
\TD{Weight Agnostic Neural Networks \cite{DBLP:journals/corr/abs-1906-04358}}


\r{optimization trajectories of neural networks \TD{On the Spectral Bias of Neural Networks \cite{Rahaman2019OnTS}} \TD{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks \cite{Saxe2013ExactST}}}


%%%%%%%%%%%%%%%%%%%%%

%
\r{variable length encoding. ``prefix property'' --- no codeword should be the prefix of another codeword}



% TODO:
\TD{Energy Functions}