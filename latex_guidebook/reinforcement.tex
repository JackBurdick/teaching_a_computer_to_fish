\chapter{Reinforcement Learning}

\r{I'm far less experienced in this space than others. Please don't judge the details (or lackthereof) in other sections based on this section.}


\chapter{Dump Space / papers to include}

% \TD{https://karpathy.github.io/2016/05/31/rl/}

% TODO: Udacity papers
\TD{Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method \cite{riedmiller2005neural}}

\TD{Introduce Deep Q-learning algorithm --- Human-level control through deep reinforcement learning \cite{mnih2015human}}

% Q tends to overestimate
\r{Deep Q-learning overestimates action values\cite{thrun1993issues}}

\r{improvements to standard DQN}
\TD{Deep Reinforcement Learning with Double Q-learning \cite{DBLP:journals/corr/HasseltGS15}}
\TD{Improve sampling from replay --- Prioritized Experience Replay \cite{Schaul2016PrioritizedER}}
\TD{Dueling Network Architectures for Deep Reinforcement Learning \cite{DBLP:journals/corr/WangFL15}}

% Learning from multi-step bootstrap targets
\TD{Asynchronous Methods for Deep Reinforcement Learning \cite{DBLP:journals/corr/MnihBMGLHSK16}}
\TD{Distributional DQN --- A Distributional Perspective on Reinforcement Learning \cite{DBLP:journals/corr/BellemareDM17}}
\TD{Noisy DQN --- Noisy Networks for Exploration \cite{DBLP:journals/corr/FortunatoAPMOGM17}}

\TD{Combine ideas from prior work\cite{DBLP:journals/corr/HasseltGS15,Schaul2016PrioritizedER,DBLP:journals/corr/WangFL15,DBLP:journals/corr/MnihBMGLHSK16,DBLP:journals/corr/BellemareDM17,DBLP:journals/corr/FortunatoAPMOGM17} Rainbow: Combining Improvements in Deep Reinforcement Learning \cite{DBLP:journals/corr/abs-1710-02298}}