%% techniques
\chapter{Augmentation Techniques}
\label{app_aug_techniques}

\r{Including Imagery}

\r{flip, rotate. color/channel manipulation}

\r{mixup\cite{zhang2017mixup}}

\r{cutout\cite{devries2017improved}}

\r{cutmix\cite{yun2019cutmix}}

\TD{language -- back translation\cite{sennrich2015improving}}

% TODO: blog about this: https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html
\TD{unsupervised augmentation\cite{xie2019unsupervised}}


%% learning augmentation

% TODO: not sure this belongs here
\r{Sample Pairing\cite{inoue2018data}}

\r{Smart Augmentation\cite{lemley2017smart}}

\r{GAN\cite{shrivastava2017learning}}

\r{population based augmentation (PBA)\cite{ho2019population}}

\r{Bayesian data augmentation\cite{tran2017bayesian}}


%% autoaugment

\r{AutoAugment\cite{cubuk2018autoaugment}}

\r{Comment that AutoAugment can be applied directly to a dataset as well as transfer the learned policies to new datasets.}

\TD{figure of loop. controller, strategy, child network, update controller}

\r{Fast AutoAugment\cite{lim2019fast} improves upon the original search strategy in the original AutoAugment paper.}

\r{Unsupervised Augmentation}

\TD{autoaugment for object detection \cite{zoph2019learning}}

\section{Data Imbalance}
\label{app_data_imbalance}

% TODO: special case of augmentation??

\TD{A Survey of Predictive Modelling under Imbalanced Distributions \cite{DBLP:journals/corr/BrancoTR15}}

\r{An ``imbalance in the data'' may have many meanings. That is, the labels could be imbalanced (e.g. in a binary classifier, there may be n times the number of instances with the label p when compared to the label q), the features may be imbalanced, (e.g. facial recognition is being performed on collected images that are composed of overwhemingly white, male, brown hair, clean shaven, hazel eye individuals), or it may mean a combination of the above.}

\r{this poses a problem, as typically the optimization process treats all samples individually and equally, which may (often does) pose problems when creating predictions on imbalanced data}

\r{Two main high level approaches to addressing this issue. You can either modify the (or utilize a combination of the listed):}
\begin{itemize}[noitemsep,topsep=0pt]
	\item data
	\item model
	\item post-processing
\end{itemize}

\r{data modification approaches aim to create a quisi-balanced representation, often through some re-sampling scheme. Simple example for would be to down-sampling the overrepresented class and upsampling the underrepresented class -- where class here might mean target variable or feature attribute.}

\r{when discussing class imbalance in reference to a continuous variable, the term skewed is often used to describe the data, whereas unbalanced or imbalanced is used for discrete variables}

\TD{relevance function --- }

\subsection{Methods}

\TD{paper to read: \TD{Self-paced Ensemble for Highly Imbalanced Massive Data Classification \cite{Liu2020SelfpacedEF}}}

\subsubsection{Data}

\r{as mentioned, under/oversampling scheme}

\TD{Methods -- SMOTE, SMOGN}

\subsubsection{Model}

\TD{Utility-based Regression --- penalty based on \TD{relevance function}}

\subsubsection{Post processing}

\subsection{Evaluation}

\r{Difficult to discuss class imbalance without discussing the importance of having appropriate metrics in place to evaluate the methods. \TD{point to example of disease test where 1 out of N are positive --- acc is very high, yet...}}

\TD{point to metrics section and specific metrics that may be useful for various scenarios}
