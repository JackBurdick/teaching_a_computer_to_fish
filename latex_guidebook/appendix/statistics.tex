\chapter{Statsy Stuff}

% TOOD: decide if/where this belongs, organization, titles, etc

\emph{Collinearity} --- When two or more predictor variables are closely related to one another (highly correlated to one another) they are said to be collinear.

\emph{Population vs Sample} -- the population (usually denoted $N$) is the collection of all the items of interest in a study where as the sample is a subset of a population (usually denoted $n$). The numbers obtained when working with a population are called the `parameters' and the numbers obtained when working with a sample are a called `statistics'. \r{a random sample is obtained when each member of the sample is chosen from the population by chance and accurately reflects the population}


\TD{Probability Density Function (PDF)}


\section{Experimental Design}

\r{Basic Principals}

\r{
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Randomization
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Every treatment has the same probability
		\end{itemize}
		\item Replication
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Repeat the experiment ``enough'' times
		\end{itemize}
		\item Control
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Reduce/eliminate variance from other factors
		\end{itemize}
	\end{itemize}
}

\section{``tests''}

\r{accept or reject null hypothesis}

\TD{degrees of freedom, \r{number of outcomes, minus 1?}}

\r{critical values \r{typically use the 0.05 column, i.e. $95$ sure you're accepting or rejecting}}

\subsection{T test}

% TODO: read https://medium.com/@wyess/demystifying-statistical-analysis-2-the-independent-t-test-expressed-in-linear-regression-434a3b302289

\TD{Gosset -- published under "student". student t test\cite{student1908probable}}

\r{a ratio of signal to noise --- is the differnce in means between two groups significant }

\r{use t-table -- one or two-tailed, with degrees of freedom, to obtain critical value}

\r{degrees of freedom}

\TD{Assumptions: randomly selected}
\r{\begin{itemize}[noitemsep,topsep=0pt]
		\item Measurement scale is coninuous/ordinal scale
		\item randomly selected (representative of population)
		\item normal distribution,
		\item similar variance
	\end{itemize}
}
% 		\item similar size of datapoints

\r{independent}

\r{dependent (paired) -- example same population multiple times}


\r{Correlated (or paired) T-Test}
\begin{equation}
	{\textrm{t-value} = \frac{ \textrm{diff in means}}{\textrm{variability between groups}} = \frac{ |\bar{X}_1 - \bar{X}_2|}{ 
			\sqrt{ \frac{ {std_1}^2 }{ n_1 } + \frac{ {std_2}^2 }{ n_2 }}}}
	\label{eq:paired_t_test}
\end{equation}
\r{source: https://magoosh.com/statistics/how-to-perform-an-independent-sample-t-test/}


\subsection{Chi-Squared Test}

\r{Carl Pearsons Chi-Squared Test. is the variation in your data due to chance?}


\section{Power Analysis}

\r{often used to determine the size of the experiment necessary to detect a given effect/treatment. May also be used determine the power of an experiment after it has been performed (given the effect/treatment size and the size of the experiement)}

\r{power --- the probability of detecting an effect/treatment, provided an effect/treatment is present}

\r{\textit{e.g.} a power value of $0.7$ would signify that $70\%$ of the time, the there is a significant difference between the effect/treatment group and the control group}

\r{power, effect size, sample size, and alpha}


\section{Analysis of Variance (ANOVA)}

\r{Splits and observed aggregate into systematic factors and random factors. Determine the influence that an independent variable has. Fisher analysis of variance\cite{fisher1992statistical}}

\r{if no true variance exists, the ANOVA's F-ratio should be 1 or near 1.}

\begin{equation}
	{\textrm{ANOVA coefficient} = \frac{ \textrm{MST}}{\textrm{MSE}} = \frac{  \textrm{Mean Sum of Squares due to Treatment}}{ 
			 \textrm{Mean Sum of Squres due to Error}}}
	\label{eq:anova}
\end{equation}


\section{Transforms}

\TD{Box-Cox transformation\cite{box1964analysis}: George Box and David Cox}

\r{lambda $\lambda$, which varies from $\neg5$ to $5$}

\r{lambda is selected on which value gives the best approximation of a normal distribution}


\r{Formula for positive data}
\begin{equation}
	\begin{cases} 
		\frac
		{y^\lambda - 1}
		{\lambda},  & \lambda \neq 0; \\
	    \log y, & \lambda = 0. \\
	\end{cases}
\end{equation}



\r{Formula that could be used for negative y-values}
\begin{equation}
	\begin{cases} 
		\frac
		{({{y + \lambda_2}})^{\lambda_1} - 1}
		{\lambda_1},  & {\lambda_1} \neq 0; \\
		\log ({y+{\lambda_2}}), & {\lambda_1} = 0. \\
	\end{cases}
\end{equation}



\section{Variance Reduction Methods}

\TD{An experiement who's treatment effect is small relative to the metrics variance is considered weaker/underpowered, when compared to an experiment who's variance is smaller -- (this explanation needs work)}

% \includegraphics[width=0.5\textwidth]{example-image-a}\hfil
\TD{add X and Y names (Effect and Variance)}
\begin{figure}
	\begin{subfigure}{6cm}
		\centering\includegraphics[width=5cm]{example-image-a}
		\caption{Low Effect, High Variance: Two wide mostly overlapping distributions}
	\end{subfigure}
	\begin{subfigure}{6cm}
		\centering\includegraphics[width=5cm]{example-image-b}
		\caption{High Effect, High Variance: Two wide mostly non-overlapping overlapping distributions}
	\end{subfigure}
	
	\begin{subfigure}{6cm}
		\centering\includegraphics[width=5cm]{example-image-c}
		\caption{Low Effect, Low Variance: Two narrow mostly overlapping overlapping distributions}
	\end{subfigure}
	\begin{subfigure}{6cm}
		\centering\includegraphics[width=5cm]{example-image-c}
		\caption{High Effect, Low Variance: Two narrow mostly non-overlapping overlapping distributions}
	\end{subfigure}
\end{figure}

\r{CUPED methods (\textbf{C}ontrolled \textbf{U}sing \textbf{P}re-\textbf{E}xperiment \textbf{D}ata)\cite{deng2013improving}}

\r{Out of DoorDash --- CUPAC methods (\textbf{C}ontrol \textbf{U}sing \textbf{P}redictions as \textbf{C}ovariates)\cite{tangcontrol}}

\r{reduction of pre-experiment variance to help strengthen experiments power}


\section{Distributions}

\r{http://www.math.wm.edu/~leemis/chart/UDR/UDR.html}

\TD{https://medium.com/@srowen/common-probability-distributions-347e6b945ce4}

\subsection{Common Distributions}

\r{There are many, many different types of distributions. However, there are a number of distributions that are commonly used}

\subsubsection{Normal Distribution / Gaussian Distribution}

\emph{Normal Distribution} --- \TD{Normal, or Guassian, distribution. Data is symmetrical where half the values are greater than the mean and half the values are less than the mean. The median, mode, and mean are all equal}

\subsubsection{Uniform Distribution}

\TD{Uniform}

\r{For example, rolling a single dice}

\subsubsection{Bernoulli Distribution}

\TD{Bernoulli}

\subsubsection{Poisson Distribution}

\TD{Bernoulli}