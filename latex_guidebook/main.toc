\contentsline {part}{I\hspace {1em}Background}{1}%
\contentsline {chapter}{\numberline {1}Introduction}{3}%
\contentsline {section}{\numberline {1.1}Motivation for this text}{3}%
\contentsline {section}{\numberline {1.2}What}{3}%
\contentsline {subsection}{\numberline {1.2.1}Rule based vs ``learning''}{4}%
\contentsline {subsection}{\numberline {1.2.2}High Level Overview}{4}%
\contentsline {subsection}{\numberline {1.2.3}Deep Learning}{4}%
\contentsline {subsubsection}{\numberline {1.2.3.1}Why Now?}{4}%
\contentsline {section}{\numberline {1.3}Why ML}{6}%
\contentsline {section}{\numberline {1.4}Notes}{6}%
\contentsline {subsection}{\numberline {1.4.1}Target Audience}{6}%
\contentsline {subsection}{\numberline {1.4.2}How to Read this Book}{6}%
\contentsline {chapter}{\numberline {2}Resources and Communities}{7}%
\contentsline {section}{\numberline {2.1}Online Communities}{7}%
\contentsline {section}{\numberline {2.2}Blogs}{7}%
\contentsline {section}{\numberline {2.3}Online Courses}{7}%
\contentsline {section}{\numberline {2.4}Text Books}{7}%
\contentsline {chapter}{\numberline {3}Prerequisites}{9}%
\contentsline {section}{\numberline {3.1}Math Notation}{9}%
\contentsline {section}{\numberline {3.2}Calculus}{10}%
\contentsline {subsection}{\numberline {3.2.1}Derivatives}{12}%
\contentsline {subsection}{\numberline {3.2.2}Chain Rule}{12}%
\contentsline {section}{\numberline {3.3}Boolean Logic}{12}%
\contentsline {section}{\numberline {3.4}Linear Algebra}{13}%
\contentsline {subsection}{\numberline {3.4.1}Overview}{13}%
\contentsline {subsubsection}{\numberline {3.4.1.1}Scalars (0D tensors)}{13}%
\contentsline {subsubsection}{\numberline {3.4.1.2}Vectors (1D tensors)}{13}%
\contentsline {subsubsection}{\numberline {3.4.1.3}Matrices (2D tensors)}{13}%
\contentsline {subsection}{\numberline {3.4.2}Matrix Arithmetic}{13}%
\contentsline {subsubsection}{\numberline {3.4.2.1}Matrices}{14}%
\contentsline {paragraph}{\numberline {3.4.2.1.1}Addition, Subtraction}{14}%
\contentsline {paragraph}{\numberline {3.4.2.1.2}Multiplication, Division}{14}%
\contentsline {subsubsection}{\numberline {3.4.2.2}Scalar}{14}%
\contentsline {paragraph}{\numberline {3.4.2.2.1}Addition, Subtraction}{14}%
\contentsline {paragraph}{\numberline {3.4.2.2.2}Multiplication, Division}{14}%
\contentsline {section}{\numberline {3.5}Graph Theory}{14}%
\contentsline {part}{II\hspace {1em}Data}{15}%
\contentsline {section}{\numberline {3.6}Bias}{18}%
\contentsline {subsection}{\numberline {3.6.1}Types of Bias}{18}%
\contentsline {subsubsection}{\numberline {3.6.1.1}Interaction Bias}{18}%
\contentsline {subsubsection}{\numberline {3.6.1.2}Latent Bias}{18}%
\contentsline {subsubsection}{\numberline {3.6.1.3}Selection Bias}{18}%
\contentsline {subsubsection}{\numberline {3.6.1.4}Recency Bias}{18}%
\contentsline {subsection}{\numberline {3.6.2}Evaluation}{18}%
\contentsline {section}{\numberline {3.7}Data Acquisition}{18}%
\contentsline {subsection}{\numberline {3.7.1}Resources}{18}%
\contentsline {subsection}{\numberline {3.7.2}Public Indexing}{18}%
\contentsline {subsection}{\numberline {3.7.3}Notable Database Sites}{18}%
\contentsline {subsection}{\numberline {3.7.4}Datasets to be familiar with}{19}%
\contentsline {subsubsection}{\numberline {3.7.4.1}Common}{19}%
\contentsline {subsection}{\numberline {3.7.5}Problems}{19}%
\contentsline {subsection}{\numberline {3.7.6}Data Portal Search}{19}%
\contentsline {subsection}{\numberline {3.7.7}Generating Fake Data}{19}%
\contentsline {section}{\numberline {3.8}Data Types}{21}%
\contentsline {subsection}{\numberline {3.8.1}Numerical (quantitative)}{21}%
\contentsline {subsubsection}{\numberline {3.8.1.1}Discrete}{21}%
\contentsline {subsubsection}{\numberline {3.8.1.2}Continuous}{21}%
\contentsline {subsection}{\numberline {3.8.2}Categorical (qualitative)}{21}%
\contentsline {subsubsection}{\numberline {3.8.2.1}Nominal}{21}%
\contentsline {subsubsection}{\numberline {3.8.2.2}Ordinal}{21}%
\contentsline {section}{\numberline {3.9}Data Preparation}{21}%
\contentsline {subsection}{\numberline {3.9.1}Data Pre-processing}{22}%
\contentsline {subsection}{\numberline {3.9.2}Handling Missing Data}{22}%
\contentsline {subsubsection}{\numberline {3.9.2.1}Filtering Out}{22}%
\contentsline {subsubsection}{\numberline {3.9.2.2}Filling In}{23}%
\contentsline {subsubsection}{\numberline {3.9.2.3}Handling Categorical Data}{23}%
\contentsline {paragraph}{\numberline {3.9.2.3.1}Encoding}{23}%
\contentsline {subparagraph}{binarization}{23}%
\contentsline {subparagraph}{Target Encoding}{24}%
\contentsline {subsubsection}{\numberline {3.9.2.4}Feature Scaling, Normalization}{24}%
\contentsline {paragraph}{\numberline {3.9.2.4.1}Mean Normalization}{24}%
\contentsline {paragraph}{\numberline {3.9.2.4.2}Min-Max scaling (Normalization)}{24}%
\contentsline {paragraph}{\numberline {3.9.2.4.3}Standardization}{24}%
\contentsline {subparagraph}{Robust Scaler}{24}%
\contentsline {subsubsection}{\numberline {3.9.2.5}Others}{25}%
\contentsline {paragraph}{\numberline {3.9.2.5.1}Removing Duplicates}{25}%
\contentsline {paragraph}{\numberline {3.9.2.5.2}Outliers}{25}%
\contentsline {paragraph}{\numberline {3.9.2.5.3}Discretization and Binning}{25}%
\contentsline {subsubsection}{\numberline {3.9.2.6}Where to do preprocessing}{25}%
\contentsline {subsubsection}{\numberline {3.9.2.7}Feature Engineering}{25}%
\contentsline {subsection}{\numberline {3.9.3}Data Types}{26}%
\contentsline {subsection}{\numberline {3.9.4}Imagery}{26}%
\contentsline {subsection}{\numberline {3.9.5}Time series}{26}%
\contentsline {subsection}{\numberline {3.9.6}Text data}{26}%
\contentsline {section}{\numberline {3.10}Feature Extraction from Various Datatypes}{26}%
\contentsline {subsection}{\numberline {3.10.1}Feature Engineering}{26}%
\contentsline {subsubsection}{\numberline {3.10.1.1}Kernel}{26}%
\contentsline {subsubsection}{\numberline {3.10.1.2}Feature Crosses}{28}%
\contentsline {subsection}{\numberline {3.10.2}2D Images}{28}%
\contentsline {subsection}{\numberline {3.10.3}3D Imagery}{28}%
\contentsline {subsection}{\numberline {3.10.4}Video}{29}%
\contentsline {subsection}{\numberline {3.10.5}Natural Language}{29}%
\contentsline {subsubsection}{\numberline {3.10.5.1}Terminology}{29}%
\contentsline {subsubsection}{\numberline {3.10.5.2}Pre-processing}{29}%
\contentsline {paragraph}{\numberline {3.10.5.2.1}Stop Word Filtering}{29}%
\contentsline {paragraph}{\numberline {3.10.5.2.2}Tokenization}{29}%
\contentsline {paragraph}{\numberline {3.10.5.2.3}Lemmatization}{30}%
\contentsline {paragraph}{\numberline {3.10.5.2.4}Stemming}{30}%
\contentsline {subparagraph}{Porter Stemming}{30}%
\contentsline {subsubsection}{\numberline {3.10.5.3}Encoding}{30}%
\contentsline {paragraph}{\numberline {3.10.5.3.1}Encoding Methods}{30}%
\contentsline {subparagraph}{Bag-of-Words}{30}%
\contentsline {paragraph}{\numberline {3.10.5.3.2}tf-idf}{30}%
\contentsline {subsubsection}{\numberline {3.10.5.4}Embedding}{30}%
\contentsline {subparagraph}{glove}{31}%
\contentsline {subparagraph}{word2vec}{31}%
\contentsline {subsubsection}{\numberline {3.10.5.5}Other Notes}{31}%
\contentsline {subsection}{\numberline {3.10.6}Audio}{31}%
\contentsline {section}{\numberline {3.11}Feature Selection}{31}%
\contentsline {subsection}{\numberline {3.11.1}why}{31}%
\contentsline {subsection}{\numberline {3.11.2}methods}{31}%
\contentsline {section}{\numberline {3.12}Partitioning Data}{32}%
\contentsline {subsection}{\numberline {3.12.1}Types of Splits}{32}%
\contentsline {subsection}{\numberline {3.12.2}k-Fold Cross Validation}{34}%
\contentsline {subsubsection}{\numberline {3.12.2.1}k-Fold Validation with shuffling}{34}%
\contentsline {subsection}{\numberline {3.12.3}Sampling}{35}%
\contentsline {paragraph}{\numberline {3.12.3.0.1}Representative Data}{35}%
\contentsline {paragraph}{\numberline {3.12.3.0.2}Representative Data}{35}%
\contentsline {part}{III\hspace {1em}Foundations}{37}%
\contentsline {chapter}{\numberline {4}Basics}{39}%
\contentsline {section}{\numberline {4.1}Overview}{39}%
\contentsline {section}{\numberline {4.2}Workflow Overview / Blue Print}{39}%
\contentsline {section}{\numberline {4.3}Some Terms}{40}%
\contentsline {section}{\numberline {4.4}Type of Learning}{41}%
\contentsline {subsection}{\numberline {4.4.1}Supervised vs Unsupervised}{42}%
\contentsline {subsection}{\numberline {4.4.2}Classification vs Regression}{42}%
\contentsline {subsubsection}{\numberline {4.4.2.1}Regression}{42}%
\contentsline {subsubsection}{\numberline {4.4.2.2}Classification}{42}%
\contentsline {subsection}{\numberline {4.4.3}Multi-class}{42}%
\contentsline {subsection}{\numberline {4.4.4}Multi-label}{42}%
\contentsline {subsubsection}{\numberline {4.4.4.1}Approaches: Problem transformation}{43}%
\contentsline {paragraph}{\numberline {4.4.4.1.1}Unique set/combination of labels}{43}%
\contentsline {paragraph}{\numberline {4.4.4.1.2}Many Binary Classifiers}{43}%
\contentsline {subsubsection}{\numberline {4.4.4.2}Evaluating Multi-label Classification}{43}%
\contentsline {section}{\numberline {4.5}Training}{43}%
\contentsline {subsection}{\numberline {4.5.1}Performance}{44}%
\contentsline {subsubsection}{\numberline {4.5.1.1}Cost, Loss Function}{44}%
\contentsline {subsubsection}{\numberline {4.5.1.2}Metrics}{44}%
\contentsline {subsection}{\numberline {4.5.2}Error Functions}{44}%
\contentsline {subsubsection}{\numberline {4.5.2.1}forward pass}{45}%
\contentsline {subsubsection}{\numberline {4.5.2.2}backward pass}{45}%
\contentsline {subsubsection}{\numberline {4.5.2.3}Least-squares techniques}{45}%
\contentsline {paragraph}{\numberline {4.5.2.3.1}Sum-of-squares error function}{45}%
\contentsline {paragraph}{\numberline {4.5.2.3.2}Normal Equation}{45}%
\contentsline {paragraph}{\numberline {4.5.2.3.3}Singular Value Decomposition (SVD)}{46}%
\contentsline {paragraph}{\numberline {4.5.2.3.4}Gradient Descent}{46}%
\contentsline {subsubsection}{\numberline {4.5.2.4}Global vs Local Minima}{46}%
\contentsline {section}{\numberline {4.6}Quality of Fit}{46}%
\contentsline {subsection}{\numberline {4.6.1}Regression Example}{46}%
\contentsline {subsection}{\numberline {4.6.2}Classification Example}{47}%
\contentsline {section}{\numberline {4.7}Describing Learners}{47}%
\contentsline {subsection}{\numberline {4.7.1}Parametric and non-parametric}{47}%
\contentsline {subsubsection}{\numberline {4.7.1.1}parametric}{47}%
\contentsline {subsubsection}{\numberline {4.7.1.2}nonparametric}{48}%
\contentsline {subsubsection}{\numberline {4.7.1.3}parametric vs nonparametric}{48}%
\contentsline {subsection}{\numberline {4.7.2}Eager vs Lazy Learners}{49}%
\contentsline {subsubsection}{\numberline {4.7.2.1}Eager Learners}{49}%
\contentsline {subsubsection}{\numberline {4.7.2.2}Lazy Learners}{49}%
\contentsline {subsection}{\numberline {4.7.3}Generative vs Discriminative Models}{49}%
\contentsline {subsubsection}{\numberline {4.7.3.1}Discriminative Models}{49}%
\contentsline {paragraph}{\numberline {4.7.3.1.1}Probabilistic Discriminative}{49}%
\contentsline {paragraph}{\numberline {4.7.3.1.2}Non-probabilistic Discriminative}{50}%
\contentsline {subsubsection}{\numberline {4.7.3.2}Generative Models}{50}%
\contentsline {subsection}{\numberline {4.7.4}Strong vs Weak Learners}{50}%
\contentsline {section}{\numberline {4.8}Online Learning}{50}%
\contentsline {section}{\numberline {4.9}Kernel Methods}{51}%
\contentsline {subsection}{\numberline {4.9.1}Kernel Trick}{51}%
\contentsline {subsection}{\numberline {4.9.2}Kernels}{51}%
\contentsline {subsubsection}{\numberline {4.9.2.1}Polynomial}{51}%
\contentsline {subsubsection}{\numberline {4.9.2.2}Gaussian / RBF (Radial Basis Function)}{51}%
\contentsline {subsection}{\numberline {4.9.3}Less Common Kernels}{52}%
\contentsline {section}{\numberline {4.10}Hyper-Parameters}{52}%
\contentsline {subsection}{\numberline {4.10.1}Parameters: "tuning knobs"}{52}%
\contentsline {subsubsection}{\numberline {4.10.1.1}Learning Rate}{52}%
\contentsline {paragraph}{\numberline {4.10.1.1.1}Schedule}{52}%
\contentsline {paragraph}{\numberline {4.10.1.1.2}Descriminative/Differential}{52}%
\contentsline {paragraph}{\numberline {4.10.1.1.3}research}{52}%
\contentsline {subsubsection}{\numberline {4.10.1.2}Batch size}{53}%
\contentsline {subsection}{\numberline {4.10.2}Hyper-Parameter Optimization}{53}%
\contentsline {subsubsection}{\numberline {4.10.2.1}Coordinate Descent}{53}%
\contentsline {subsubsection}{\numberline {4.10.2.2}Grid Search}{54}%
\contentsline {subsubsection}{\numberline {4.10.2.3}Randomized Search}{54}%
\contentsline {subsubsection}{\numberline {4.10.2.4}Other Methods: Automated / Model-based Methods}{54}%
\contentsline {paragraph}{\numberline {4.10.2.4.1}Bayesian Methods}{54}%
\contentsline {chapter}{\numberline {5}Estimating Model Parameters}{55}%
\contentsline {section}{\numberline {5.1}Initialization}{55}%
\contentsline {subsection}{\numberline {5.1.1}Parameter types (the initialization of)}{55}%
\contentsline {paragraph}{\numberline {5.1.1.0.1}Weights}{55}%
\contentsline {paragraph}{\numberline {5.1.1.0.2}Biases}{56}%
\contentsline {subsection}{\numberline {5.1.2}Normal Vs Uniform}{56}%
\contentsline {subsection}{\numberline {5.1.3}Strategies}{56}%
\contentsline {subsubsection}{\numberline {5.1.3.1}Glorot or Xavier}{56}%
\contentsline {subsubsection}{\numberline {5.1.3.2}he}{56}%
\contentsline {subsubsection}{\numberline {5.1.3.3}Implementation}{56}%
\contentsline {chapter}{\numberline {6}Optimization}{57}%
\contentsline {section}{\numberline {6.1}Parameterized}{57}%
\contentsline {subsection}{\numberline {6.1.1}Descent Direction Methods}{57}%
\contentsline {subsection}{\numberline {6.1.2}First-order}{58}%
\contentsline {subsubsection}{\numberline {6.1.2.1}Gradient Descent}{58}%
\contentsline {paragraph}{\numberline {6.1.2.1.1}Batch Gradient Descent}{58}%
\contentsline {paragraph}{\numberline {6.1.2.1.2}Stochastic Gradient Descent}{58}%
\contentsline {paragraph}{\numberline {6.1.2.1.3}Mini-batch Gradient Descent}{59}%
\contentsline {subsubsection}{\numberline {6.1.2.2}Conjugate Gradient}{59}%
\contentsline {subsubsection}{\numberline {6.1.2.3}Momentum Descent}{59}%
\contentsline {subsubsection}{\numberline {6.1.2.4}Nesterov Momentum Descent}{59}%
\contentsline {subsubsection}{\numberline {6.1.2.5}Adagrad Descent}{60}%
\contentsline {paragraph}{\numberline {6.1.2.5.1}Adagrad Extensions: (RMSProp, Adadelta, Adam)}{60}%
\contentsline {subparagraph}{RMSProp}{60}%
\contentsline {subparagraph}{Adadelta}{60}%
\contentsline {subparagraph}{Adam}{60}%
\contentsline {subsubsection}{\numberline {6.1.2.6}AdaMax}{60}%
\contentsline {subsubsection}{\numberline {6.1.2.7}Hypergradient Descent}{60}%
\contentsline {subsubsection}{\numberline {6.1.2.8}FTRL}{61}%
\contentsline {subsection}{\numberline {6.1.3}Nadam}{61}%
\contentsline {subsubsection}{\numberline {6.1.3.1}AMSGrad}{61}%
\contentsline {subsection}{\numberline {6.1.4}To Include}{61}%
\contentsline {subsection}{\numberline {6.1.5}second-order}{61}%
\contentsline {subsubsection}{\numberline {6.1.5.1}Newton's Method}{62}%
\contentsline {subsubsection}{\numberline {6.1.5.2}Secant Method}{62}%
\contentsline {subsubsection}{\numberline {6.1.5.3}Quasi-Newton Method}{62}%
\contentsline {section}{\numberline {6.2}Non-Parameterized}{62}%
\contentsline {subsection}{\numberline {6.2.1}Direct methods}{62}%
\contentsline {subsection}{\numberline {6.2.2}Stochastic methods}{62}%
\contentsline {subsection}{\numberline {6.2.3}Population methods}{62}%
\contentsline {subsection}{\numberline {6.2.4}Further optimization information}{62}%
\contentsline {subsection}{\numberline {6.2.5}Parallelizing and distributing SGD}{62}%
\contentsline {chapter}{\numberline {7}Losses}{63}%
\contentsline {section}{\numberline {7.1}losses}{63}%
\contentsline {subsection}{\numberline {7.1.1}fit somewhere}{63}%
\contentsline {subsubsection}{\numberline {7.1.1.1}Contrastive Losses}{63}%
\contentsline {subsection}{\numberline {7.1.2}Discrete}{63}%
\contentsline {subsubsection}{\numberline {7.1.2.1}Cross-Entropy}{63}%
\contentsline {subsection}{\numberline {7.1.3}label smoothing}{65}%
\contentsline {subsection}{\numberline {7.1.4}Continuous}{65}%
\contentsline {subsubsection}{\numberline {7.1.4.1}Losses}{65}%
\contentsline {subsection}{\numberline {7.1.5}Distribution}{65}%
\contentsline {chapter}{\numberline {8}Genetic Algorithms}{67}%
\contentsline {chapter}{\numberline {9}Evaluation}{69}%
\contentsline {subsection}{\numberline {9.0.1}Creating a Test Set}{69}%
\contentsline {subsection}{\numberline {9.0.2}Qualitative Evaluation}{70}%
\contentsline {subsubsection}{\numberline {9.0.2.1}(Over$|$Under)fitting and Capacity}{70}%
\contentsline {paragraph}{\numberline {9.0.2.1.1}Overfitting}{70}%
\contentsline {paragraph}{\numberline {9.0.2.1.2}Underfitting}{71}%
\contentsline {subparagraph}{Solution}{71}%
\contentsline {subsubsection}{\numberline {9.0.2.2}Bias Variance Trade-off}{71}%
\contentsline {paragraph}{\numberline {9.0.2.2.1}Variance}{71}%
\contentsline {paragraph}{\numberline {9.0.2.2.2}Bias}{71}%
\contentsline {paragraph}{\numberline {9.0.2.2.3}Trade-Off}{72}%
\contentsline {section}{\numberline {9.1}Evalution beyond aggregated score}{73}%
\contentsline {section}{\numberline {9.2}Qualitative Evalutation: Performance Metrics}{73}%
\contentsline {subsection}{\numberline {9.2.1}discrete}{74}%
\contentsline {subsubsection}{\numberline {9.2.1.1}Common Metrics}{74}%
\contentsline {subsubsection}{\numberline {9.2.1.2}Confusion Matrix}{74}%
\contentsline {subsubsection}{\numberline {9.2.1.3}Classification Metrics}{74}%
\contentsline {subsubsection}{\numberline {9.2.1.4}AUC (Area Under the Curve)}{76}%
\contentsline {subsubsection}{\numberline {9.2.1.5}Precision-Recall curve}{78}%
\contentsline {subsection}{\numberline {9.2.2}continuous}{78}%
\contentsline {subsubsection}{\numberline {9.2.2.1}Common Metrics}{78}%
\contentsline {subsubsection}{\numberline {9.2.2.2}Additional Metrics}{79}%
\contentsline {paragraph}{\numberline {9.2.2.2.1}Linear Evaluation}{79}%
\contentsline {paragraph}{\numberline {9.2.2.2.2}Distance Metrics}{80}%
\contentsline {paragraph}{\numberline {9.2.2.2.3}Multi-label Classification}{81}%
\contentsline {subsubsection}{\numberline {9.2.2.3}Choosing the ``right'' metrics}{81}%
\contentsline {chapter}{\numberline {10}Improving Generalizability}{83}%
\contentsline {section}{\numberline {10.1}Data}{84}%
\contentsline {subsection}{\numberline {10.1.1}Data Collection}{84}%
\contentsline {subsubsection}{\numberline {10.1.1.1}Data Labeling}{84}%
\contentsline {paragraph}{\numberline {10.1.1.1.1}Semi-supervised}{84}%
\contentsline {paragraph}{\numberline {10.1.1.1.2}Active Learning}{84}%
\contentsline {subparagraph}{Margin Sampling}{85}%
\contentsline {subparagraph}{Cluster Based Sampling}{85}%
\contentsline {subparagraph}{Query-by-Committee}{85}%
\contentsline {subparagraph}{Region-based Sampling}{85}%
\contentsline {paragraph}{\numberline {10.1.1.1.3}Weak Supervision}{85}%
\contentsline {subsection}{\numberline {10.1.2}Augmentation}{85}%
\contentsline {subsection}{\numberline {10.1.3}Sampling}{86}%
\contentsline {section}{\numberline {10.2}Architecture}{86}%
\contentsline {section}{\numberline {10.3}Training Pattern}{86}%
\contentsline {subsection}{\numberline {10.3.1}Early Stopping}{86}%
\contentsline {subsection}{\numberline {10.3.2}Stochastic Behavior}{86}%
\contentsline {subsubsection}{\numberline {10.3.2.1}Dropout}{86}%
\contentsline {subsubsection}{\numberline {10.3.2.2}Others}{89}%
\contentsline {subsection}{\numberline {10.3.3}Parameter Regularization}{89}%
\contentsline {subsubsection}{\numberline {10.3.3.1}Types of Regularization}{89}%
\contentsline {subsubsection}{\numberline {10.3.3.2}Parameter Norm Penalties}{90}%
\contentsline {paragraph}{\numberline {10.3.3.2.1}L2 Regularization}{90}%
\contentsline {paragraph}{\numberline {10.3.3.2.2}L1 Regularization}{90}%
\contentsline {paragraph}{\numberline {10.3.3.2.3}Elastic Net Regularization}{91}%
\contentsline {subsection}{\numberline {10.3.4}Ensemble Methods}{91}%
\contentsline {subsection}{\numberline {10.3.5}Adversarial Training}{91}%
\contentsline {subsection}{\numberline {10.3.6}Normalization}{91}%
\contentsline {paragraph}{\numberline {10.3.6.0.1}Proxy Normalization}{92}%
\contentsline {subsubsection}{\numberline {10.3.6.1}Activation-Based Layers}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.1}Instance normalization}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.2}Layer Normalization}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.3}Group Normalization}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.4}Filter Response Normalization}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.5}Variance Normalization}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.6}EvoNorm BO and EvoNoRMSO}{92}%
\contentsline {paragraph}{\numberline {10.3.6.1.7}Batch normalization}{93}%
\contentsline {subsubsection}{\numberline {10.3.6.2}Parametric Layers}{95}%
\contentsline {paragraph}{\numberline {10.3.6.2.1}Weight Normalization}{95}%
\contentsline {paragraph}{\numberline {10.3.6.2.2}Scaled Weight Standardization}{95}%
\contentsline {section}{\numberline {10.4}Output regularization}{95}%
\contentsline {chapter}{\numberline {11}Distributed Methods}{97}%
\contentsline {section}{\numberline {11.1}Data Parallelism}{97}%
\contentsline {section}{\numberline {11.2}Model Parallelism}{97}%
\contentsline {section}{\numberline {11.3}Federated learning}{97}%
\contentsline {chapter}{\numberline {12}Federated}{99}%
\contentsline {part}{IV\hspace {1em}Algorithms}{101}%
\contentsline {chapter}{\numberline {13}Foundational Methods}{103}%
\contentsline {section}{\numberline {13.1}Regression}{103}%
\contentsline {subsection}{\numberline {13.1.1}Simple Linear Regression}{103}%
\contentsline {subsubsection}{\numberline {13.1.1.1}OLS}{104}%
\contentsline {subsubsection}{\numberline {13.1.1.2}Cost}{105}%
\contentsline {subsubsection}{\numberline {13.1.1.3}Evaluation}{105}%
\contentsline {subsection}{\numberline {13.1.2}Multiple Linear Regression}{105}%
\contentsline {subsection}{\numberline {13.1.3}Polynomial Regression}{105}%
\contentsline {section}{\numberline {13.2}Logistic Regression}{106}%
\contentsline {subsection}{\numberline {13.2.1}Softmax Regression}{108}%
\contentsline {section}{\numberline {13.3}Nearest Neighbors}{108}%
\contentsline {subsection}{\numberline {13.3.1}Overview}{109}%
\contentsline {subsubsection}{\numberline {13.3.1.1}Distance metric}{109}%
\contentsline {subsection}{\numberline {13.3.2}K-Nearest Neighbors}{109}%
\contentsline {subsubsection}{\numberline {13.3.2.1}Regression Considerations}{109}%
\contentsline {subsection}{\numberline {13.3.3}Considering Imbalanced Data}{110}%
\contentsline {subsubsection}{\numberline {13.3.3.1}Weighted K-Nearest Neighbors}{110}%
\contentsline {subsubsection}{\numberline {13.3.3.2}Distance Weighted K-Nearest Neighbors}{110}%
\contentsline {subsection}{\numberline {13.3.4}Considerations}{110}%
\contentsline {subsubsection}{\numberline {13.3.4.1}Memory}{110}%
\contentsline {subsection}{\numberline {13.3.5}Other Variations}{110}%
\contentsline {section}{\numberline {13.4}Support Vector Machines (SVM)}{111}%
\contentsline {subsection}{\numberline {13.4.1}Maximizing Geometric Margin}{111}%
\contentsline {subsubsection}{\numberline {13.4.1.1}Sequential Minimal Optimization}{111}%
\contentsline {subsection}{\numberline {13.4.2}Kernel SVM}{111}%
\contentsline {subsubsection}{\numberline {13.4.2.1}The `Kernel Trick'}{111}%
\contentsline {section}{\numberline {13.5}Naive Bayes}{111}%
\contentsline {subsection}{\numberline {13.5.1}Bayes' Theorem}{112}%
\contentsline {section}{\numberline {13.6}Decision Trees}{112}%
\contentsline {subsection}{\numberline {13.6.1}Criterion -- Maximizing Information Gain}{113}%
\contentsline {subsubsection}{\numberline {13.6.1.1}Gini Impurity}{113}%
\contentsline {subsubsection}{\numberline {13.6.1.2}Entropy}{113}%
\contentsline {paragraph}{\numberline {13.6.1.2.1}Information Gain}{113}%
\contentsline {subsubsection}{\numberline {13.6.1.3}Classification Error}{114}%
\contentsline {subsubsection}{\numberline {13.6.1.4}ID3 Algorithm}{114}%
\contentsline {subsubsection}{\numberline {13.6.1.5}C4.5 Algorithm}{114}%
\contentsline {subsubsection}{\numberline {13.6.1.6}CART Algorithm}{114}%
\contentsline {subsection}{\numberline {13.6.2}Pruning}{114}%
\contentsline {subsubsection}{\numberline {13.6.2.1}Pre-pruning}{115}%
\contentsline {subsubsection}{\numberline {13.6.2.2}Post-pruning}{115}%
\contentsline {section}{\numberline {13.7}Random Forests}{115}%
\contentsline {chapter}{\numberline {14}Artificial Neural Networks}{117}%
\contentsline {section}{\numberline {14.1}Perceptron}{117}%
\contentsline {subsection}{\numberline {14.1.1}History}{117}%
\contentsline {subsection}{\numberline {14.1.2}Overview}{118}%
\contentsline {subsection}{\numberline {14.1.3}Activation Function Basics}{118}%
\contentsline {subsection}{\numberline {14.1.4}Limitations}{119}%
\contentsline {subsection}{\numberline {14.1.5}Extending to model linearly inseparable data}{119}%
\contentsline {subsubsection}{\numberline {14.1.5.1}Kernelization}{119}%
\contentsline {subsubsection}{\numberline {14.1.5.2}Directed Graph}{119}%
\contentsline {subsection}{\numberline {14.1.6}Notes}{120}%
\contentsline {section}{\numberline {14.2}Artificial Neural Networks (ANN)}{120}%
\contentsline {subsection}{\numberline {14.2.1}Multi-layer Perceptron}{120}%
\contentsline {subsection}{\numberline {14.2.2}Architecture}{120}%
\contentsline {subsection}{\numberline {14.2.3}Components}{121}%
\contentsline {subsubsection}{\numberline {14.2.3.1}Nodes / units}{121}%
\contentsline {paragraph}{\numberline {14.2.3.1.1}Initialization}{121}%
\contentsline {subsubsection}{\numberline {14.2.3.2}Activation Function}{121}%
\contentsline {subsubsection}{\numberline {14.2.3.3}Why Non-linear}{123}%
\contentsline {subsubsection}{\numberline {14.2.3.4}Popular Activation Functions}{123}%
\contentsline {paragraph}{\numberline {14.2.3.4.1}Smooth Non-linear}{123}%
\contentsline {subparagraph}{Sigmoid}{123}%
\contentsline {subparagraph}{ELU}{123}%
\contentsline {subparagraph}{Softplus}{123}%
\contentsline {paragraph}{\numberline {14.2.3.4.2}Not Smooth Non-linear}{127}%
\contentsline {subparagraph}{ReLU}{127}%
\contentsline {subparagraph}{Leaky ReLU}{127}%
\contentsline {subparagraph}{ReLU6}{127}%
\contentsline {subparagraph}{PReLU}{128}%
\contentsline {subsection}{\numberline {14.2.4}Characterization}{129}%
\contentsline {subsubsection}{\numberline {14.2.4.1}Types: Feed-forward vs Feedback}{129}%
\contentsline {paragraph}{\numberline {14.2.4.1.1}Feed-forward}{129}%
\contentsline {subparagraph}{Layered networks}{129}%
\contentsline {subparagraph}{General topologies}{130}%
\contentsline {paragraph}{\numberline {14.2.4.1.2}Feedback}{130}%
\contentsline {subsubsection}{\numberline {14.2.4.2}Terminology}{130}%
\contentsline {subsection}{\numberline {14.2.5}Learning: Backpropagation}{131}%
\contentsline {subsubsection}{\numberline {14.2.5.1}Forward pass}{131}%
\contentsline {subsubsection}{\numberline {14.2.5.2}Backward pass}{131}%
\contentsline {subsubsection}{\numberline {14.2.5.3}Back-propagation efficiency}{132}%
\contentsline {subsubsection}{\numberline {14.2.5.4}Chain Rule}{132}%
\contentsline {subsection}{\numberline {14.2.6}Autodiff}{132}%
\contentsline {subsubsection}{\numberline {14.2.6.1}Manual differentiation}{132}%
\contentsline {subsubsection}{\numberline {14.2.6.2}Finite difference approximation}{133}%
\contentsline {subsubsection}{\numberline {14.2.6.3}Forward-mode autodiff}{133}%
\contentsline {subsubsection}{\numberline {14.2.6.4}Reverse-mode autodiff}{133}%
\contentsline {section}{\numberline {14.3}Feed-forward}{133}%
\contentsline {section}{\numberline {14.4}Feedback or Recurrent}{133}%
\contentsline {subsection}{\numberline {14.4.1}Foundation}{134}%
\contentsline {subsection}{\numberline {14.4.2}Simple RNN and Recurrent Neuron}{134}%
\contentsline {subparagraph}{Overview}{134}%
\contentsline {section}{\numberline {14.5}Common Problems}{134}%
\contentsline {subsection}{\numberline {14.5.1}Maintaining States}{134}%
\contentsline {subsection}{\numberline {14.5.2}Addressing Vanishing and Exploding Gradients}{134}%
\contentsline {chapter}{\numberline {15}Applied Neural Networks}{137}%
\contentsline {section}{\numberline {15.1}Single Modality}{137}%
\contentsline {subsection}{\numberline {15.1.1}N-Dimensional Structure}{137}%
\contentsline {subsubsection}{\numberline {15.1.1.1}One-Dimensional Structure (\textit {e.g.} sequences, text, etc.)}{137}%
\contentsline {paragraph}{\numberline {15.1.1.1.1}Sequence to Sequence}{137}%
\contentsline {subparagraph}{Overview}{137}%
\contentsline {subparagraph}{advancements}{138}%
\contentsline {paragraph}{\numberline {15.1.1.1.2}Sequence to Vector}{138}%
\contentsline {subparagraph}{Overview}{138}%
\contentsline {paragraph}{\numberline {15.1.1.1.3}Vector to Sequence}{138}%
\contentsline {subparagraph}{Overview}{138}%
\contentsline {paragraph}{\numberline {15.1.1.1.4}Delayed Sequence to Sequence}{138}%
\contentsline {subsubsection}{\numberline {15.1.1.2}Two-Dimensional Structure (\textit {e.g.} Imagery)}{139}%
\contentsline {subsubsection}{\numberline {15.1.1.3}N-Dimensional Structure (\textit {e.g.} Video)}{139}%
\contentsline {section}{\numberline {15.2}Multimodal}{139}%
\contentsline {subsection}{\numberline {15.2.1}N-Dimensional Structure}{139}%
\contentsline {chapter}{\numberline {16}Unsupervised}{141}%
\contentsline {subsection}{\numberline {16.0.1}TODO}{141}%
\contentsline {section}{\numberline {16.1}Clustering}{142}%
\contentsline {subsection}{\numberline {16.1.1}Common Algorithms}{142}%
\contentsline {subsubsection}{\numberline {16.1.1.1}K-means}{142}%
\contentsline {paragraph}{\numberline {16.1.1.1.1}Local Optima}{143}%
\contentsline {paragraph}{\numberline {16.1.1.1.2}Selecting K}{143}%
\contentsline {paragraph}{\numberline {16.1.1.1.3}Elbow Method}{143}%
\contentsline {subsubsection}{\numberline {16.1.1.2}Hierarchical Clustering}{143}%
\contentsline {subsubsection}{\numberline {16.1.1.3}DBSCAN}{144}%
\contentsline {subsubsection}{\numberline {16.1.1.4}HDBSCAN}{144}%
\contentsline {subsection}{\numberline {16.1.2}Evaluating}{144}%
\contentsline {subsubsection}{\numberline {16.1.2.1}Silhouette Coefficient}{144}%
\contentsline {section}{\numberline {16.2}Dimensionality Reduction}{144}%
\contentsline {subsection}{\numberline {16.2.1}Principal Component Analysis}{145}%
\contentsline {subsubsection}{\numberline {16.2.1.1}Linear}{146}%
\contentsline {paragraph}{\numberline {16.2.1.1.1}Incremental PCA}{146}%
\contentsline {paragraph}{\numberline {16.2.1.1.2}Sparse PCA}{146}%
\contentsline {subsubsection}{\numberline {16.2.1.2}Nonlinear}{146}%
\contentsline {paragraph}{\numberline {16.2.1.2.1}Kernel PCA}{146}%
\contentsline {subsection}{\numberline {16.2.2}Singular value decomposition (SVD)}{146}%
\contentsline {subsection}{\numberline {16.2.3}Random Projection}{146}%
\contentsline {subsubsection}{\numberline {16.2.3.1}Gaussian Random Projection}{147}%
\contentsline {subsubsection}{\numberline {16.2.3.2}Sparse Random Projection}{147}%
\contentsline {section}{\numberline {16.3}Nonlinear dimensionality reduction}{147}%
\contentsline {subsection}{\numberline {16.3.1}Isomap}{147}%
\contentsline {subsection}{\numberline {16.3.2}Multidimensional Scaling (MDS)}{147}%
\contentsline {subsection}{\numberline {16.3.3}Locally Linear Embedding (LLE)}{147}%
\contentsline {subsection}{\numberline {16.3.4}t-Distributed Stochastic Neighbor Embedding (t-SNE)}{148}%
\contentsline {section}{\numberline {16.4}Non-geometric, no distance metric}{148}%
\contentsline {subsection}{\numberline {16.4.1}Dictionary Learning}{148}%
\contentsline {subsection}{\numberline {16.4.2}Independent Component Analysis (ICA)}{148}%
\contentsline {subsection}{\numberline {16.4.3}TODO: others}{148}%
\contentsline {subsection}{\numberline {16.4.4}Autoencoders}{149}%
\contentsline {paragraph}{\numberline {16.4.4.0.1}Undercomplete vs Overcomplete}{149}%
\contentsline {subsection}{\numberline {16.4.5}Generative Adversarial Networks}{149}%
\contentsline {subsection}{\numberline {16.4.6}Hidden Markov Model}{149}%
\contentsline {chapter}{\numberline {17}Semi-supervised}{151}%
\contentsline {section}{\numberline {17.1}Semi-Supervised}{151}%
\contentsline {subsection}{\numberline {17.1.1}Examples}{151}%
\contentsline {subsection}{\numberline {17.1.2}TODO}{151}%
\contentsline {chapter}{\numberline {18}Common Architectures}{153}%
\contentsline {subsection}{\numberline {18.0.1}Spatial Data}{153}%
\contentsline {subsubsection}{\numberline {18.0.1.1}Convolutional Approaches}{153}%
\contentsline {subsubsection}{\numberline {18.0.1.2}Image Classification}{153}%
\contentsline {paragraph}{\numberline {18.0.1.2.1}skip connections}{154}%
\contentsline {subsection}{\numberline {18.0.2}Object Detection}{154}%
\contentsline {subsection}{\numberline {18.0.3}Segmentation}{155}%
\contentsline {section}{\numberline {18.1}Text: Natural Language Processing}{155}%
\contentsline {section}{\numberline {18.2}Structured data: Tablular}{155}%
\contentsline {section}{\numberline {18.3}Other Common Architectures}{155}%
\contentsline {chapter}{\numberline {19}Model ``Compression'' or ``Distillation''}{157}%
\contentsline {section}{\numberline {19.1}Quantization}{157}%
\contentsline {subsection}{\numberline {19.1.1}When}{157}%
\contentsline {subsubsection}{\numberline {19.1.1.1}Initialization}{157}%
\contentsline {subsubsection}{\numberline {19.1.1.2}during training}{158}%
\contentsline {subsubsection}{\numberline {19.1.1.3}post-training}{158}%
\contentsline {section}{\numberline {19.2}Weight Pruning}{158}%
\contentsline {subsubsection}{\numberline {19.2.0.1}What to remove}{158}%
\contentsline {paragraph}{\numberline {19.2.0.1.1}relationship of nodes to nodes pruned}{158}%
\contentsline {subparagraph}{unstructured}{158}%
\contentsline {subparagraph}{structured}{158}%
\contentsline {paragraph}{\numberline {19.2.0.1.2}relationship of nodes to architecture}{158}%
\contentsline {subparagraph}{local}{158}%
\contentsline {subparagraph}{global}{158}%
\contentsline {subsubsection}{\numberline {19.2.0.2}Deciding what to remove}{159}%
\contentsline {subsubsection}{\numberline {19.2.0.3}When to prune}{159}%
\contentsline {subsubsection}{\numberline {19.2.0.4}Pruning mechanism}{159}%
\contentsline {section}{\numberline {19.3}Topology}{159}%
\contentsline {subsection}{\numberline {19.3.1}Distillation}{159}%
\contentsline {subsubsection}{\numberline {19.3.1.1}Student Teacher}{159}%
\contentsline {paragraph}{\numberline {19.3.1.1.1}Knowledge Transfer}{160}%
\contentsline {subsection}{\numberline {19.3.2}Tensor Decomposition}{160}%
\contentsline {subsection}{\numberline {19.3.3}The Lottery Ticket Hypothesis}{160}%
\contentsline {section}{\numberline {19.4}Sparsity}{160}%
\contentsline {chapter}{\numberline {20}External Memory}{161}%
\contentsline {section}{\numberline {20.1}Neural Turning Machines}{161}%
\contentsline {section}{\numberline {20.2}Other}{161}%
\contentsline {chapter}{\numberline {21}Training Dynamics}{163}%
\contentsline {chapter}{\numberline {22}Adversarial Machine Learning}{165}%
\contentsline {section}{\numberline {22.1}Attacks}{165}%
\contentsline {section}{\numberline {22.2}Defenses}{166}%
\contentsline {section}{\numberline {22.3}Implications}{166}%
\contentsline {section}{\numberline {22.4}Backdoor Learning}{166}%
\contentsline {section}{\numberline {22.5}Uses (non-exploitive) for Backdoor}{166}%
\contentsline {section}{\numberline {22.6}To Include}{166}%
\contentsline {chapter}{\numberline {23}Graph Neural Networks}{167}%
\contentsline {chapter}{\numberline {24}Generative}{169}%
\contentsline {section}{\numberline {24.1}Generative Adversarial Networks (GANs)}{169}%
\contentsline {section}{\numberline {24.2}Variational AutoEncoder}{169}%
\contentsline {chapter}{\numberline {25}Curriculum Learning}{171}%
\contentsline {part}{V\hspace {1em}Architectures}{173}%
\contentsline {section}{\numberline {25.1}Dense}{175}%
\contentsline {section}{\numberline {25.2}Convolutions}{175}%
\contentsline {section}{\numberline {25.3}Pooling}{177}%
\contentsline {section}{\numberline {25.4}Recurrent Cells}{177}%
\contentsline {subsection}{\numberline {25.4.1}Cell Advancements}{178}%
\contentsline {subsubsection}{\numberline {25.4.1.1}LSTM}{178}%
\contentsline {paragraph}{\numberline {25.4.1.1.1}variants}{178}%
\contentsline {paragraph}{\numberline {25.4.1.1.2}other directions}{178}%
\contentsline {paragraph}{\numberline {25.4.1.1.3}Fully Connected Layers}{179}%
\contentsline {subparagraph}{Main}{179}%
\contentsline {subparagraph}{Forget}{179}%
\contentsline {subparagraph}{Input}{179}%
\contentsline {subparagraph}{Output}{181}%
\contentsline {paragraph}{\numberline {25.4.1.1.4}Other}{181}%
\contentsline {subparagraph}{Peephole Connections}{181}%
\contentsline {subsubsection}{\numberline {25.4.1.2}GRU}{181}%
\contentsline {subsection}{\numberline {25.4.2}Notes -- add}{182}%
\contentsline {section}{\numberline {25.5}Capsule Networks}{182}%
\contentsline {section}{\numberline {25.6}MLP-Mixer}{182}%
\contentsline {section}{\numberline {25.7}Mixture of Experts (MoE)}{182}%
\contentsline {chapter}{\numberline {26}Attention}{185}%
\contentsline {section}{\numberline {26.1}Compatibility/Scoring/Attention Functions}{186}%
\contentsline {subsection}{\numberline {26.1.1}Additive}{186}%
\contentsline {subsection}{\numberline {26.1.2}Dot-Product}{186}%
\contentsline {subsection}{\numberline {26.1.3}Scaled Dot-Product}{186}%
\contentsline {subsection}{\numberline {26.1.4}Other}{186}%
\contentsline {section}{\numberline {26.2}softmax}{186}%
\contentsline {section}{\numberline {26.3}Reducing Complexity}{187}%
\contentsline {subsection}{\numberline {26.3.1}Low-Rank}{187}%
\contentsline {subsection}{\numberline {26.3.2}Compression}{187}%
\contentsline {subsubsection}{\numberline {26.3.2.1}Memory (keys and Values)}{187}%
\contentsline {subsubsection}{\numberline {26.3.2.2}Query}{187}%
\contentsline {subsection}{\numberline {26.3.3}Structural Sparsity}{187}%
\contentsline {subsubsection}{\numberline {26.3.3.1}Global}{187}%
\contentsline {paragraph}{\numberline {26.3.3.1.1}External}{187}%
\contentsline {paragraph}{\numberline {26.3.3.1.2}Internal}{187}%
\contentsline {subsubsection}{\numberline {26.3.3.2}Local}{187}%
\contentsline {paragraph}{\numberline {26.3.3.2.1}Band}{188}%
\contentsline {paragraph}{\numberline {26.3.3.2.2}Dialated}{188}%
\contentsline {paragraph}{\numberline {26.3.3.2.3}Block Local}{188}%
\contentsline {subsubsection}{\numberline {26.3.3.3}Random}{188}%
\contentsline {subsubsection}{\numberline {26.3.3.4}Combination}{188}%
\contentsline {chapter}{\numberline {27}Multi-Headed Attention}{189}%
\contentsline {chapter}{\numberline {28}Positional Encodings}{191}%
\contentsline {section}{\numberline {28.1}Positional Values}{191}%
\contentsline {paragraph}{\numberline {28.1.0.0.1}Positional Encoding Value(s)}{192}%
\contentsline {subsection}{\numberline {28.1.1}Fixed vs Learned and Relative vs Absolute}{192}%
\contentsline {subsection}{\numberline {28.1.2}Absolute}{193}%
\contentsline {subsubsection}{\numberline {28.1.2.1}Fixed}{193}%
\contentsline {paragraph}{\numberline {28.1.2.1.1}Sinusoidal}{193}%
\contentsline {subsubsection}{\numberline {28.1.2.2}Learned}{193}%
\contentsline {subsection}{\numberline {28.1.3}Relative}{193}%
\contentsline {subsubsection}{\numberline {28.1.3.1}Fixed}{193}%
\contentsline {subsubsection}{\numberline {28.1.3.2}Learned}{193}%
\contentsline {subsubsection}{\numberline {28.1.3.3}Use in Linear Transformers}{193}%
\contentsline {section}{\numberline {28.2}Hybrid approaches}{194}%
\contentsline {section}{\numberline {28.3}TO INCLUDE}{194}%
\contentsline {section}{\numberline {28.4}Including Positional Information}{194}%
\contentsline {subsection}{\numberline {28.4.1}Additive}{194}%
\contentsline {subsection}{\numberline {28.4.2}Concatenation}{194}%
\contentsline {subsection}{\numberline {28.4.3}Multiple inclusions}{194}%
\contentsline {section}{\numberline {28.5}Beyond One Dimension}{195}%
\contentsline {subsection}{\numberline {28.5.1}Two Dimensional}{195}%
\contentsline {chapter}{\numberline {29}Transformer}{197}%
\contentsline {section}{\numberline {29.1}Normalization}{197}%
\contentsline {subsection}{\numberline {29.1.1}Normalization Function}{197}%
\contentsline {subsection}{\numberline {29.1.2}Normalization Placement}{198}%
\contentsline {subsubsection}{\numberline {29.1.2.1}Post-LN}{198}%
\contentsline {subsubsection}{\numberline {29.1.2.2}Pre-LN}{198}%
\contentsline {section}{\numberline {29.2}Positional Transformation}{198}%
\contentsline {subsection}{\numberline {29.2.1}Replacement}{198}%
\contentsline {subsection}{\numberline {29.2.2}non-linearity}{198}%
\contentsline {part}{VI\hspace {1em}Ensembling}{199}%
\contentsline {chapter}{\numberline {30}Ensemble Methods}{201}%
\contentsline {section}{\numberline {30.1}Overview}{201}%
\contentsline {subsection}{\numberline {30.1.1}Approaches to Creating Ensembles}{201}%
\contentsline {subsubsection}{\numberline {30.1.1.1}Bagging}{201}%
\contentsline {subsubsection}{\numberline {30.1.1.2}Boosting}{202}%
\contentsline {paragraph}{\numberline {30.1.1.2.1}Examples}{202}%
\contentsline {subparagraph}{AdaBoost}{202}%
\contentsline {subsubsection}{\numberline {30.1.1.3}Bagging Vs Boosting}{203}%
\contentsline {subsection}{\numberline {30.1.2}Stacking}{203}%
\contentsline {subsection}{\numberline {30.1.3}Examples}{203}%
\contentsline {subsubsection}{\numberline {30.1.3.1}Random Forests}{203}%
\contentsline {chapter}{\numberline {31}Term dump}{205}%
\contentsline {chapter}{\numberline {32}Self-Supervised Learning (SSL)}{207}%
\contentsline {section}{\numberline {32.1}Pretext Tasks}{207}%
\contentsline {section}{\numberline {32.2}Contrastive Learning}{208}%
\contentsline {subsection}{\numberline {32.2.1}Explicit Contrastive Methods}{208}%
\contentsline {subsection}{\numberline {32.2.2}``Non-Contrastive'' Methods}{208}%
\contentsline {subsubsection}{\numberline {32.2.2.1}Cluster}{208}%
\contentsline {subsubsection}{\numberline {32.2.2.2}Distillation}{209}%
\contentsline {part}{VII\hspace {1em}Multiple Tasks and Datasets}{211}%
\contentsline {chapter}{\numberline {33}Mutiple objectives}{213}%
\contentsline {chapter}{\numberline {34}Transfer Learning}{215}%
\contentsline {subsubsection}{\numberline {34.0.0.1}Potential downsides of TL}{216}%
\contentsline {chapter}{\numberline {35}Multi-Task Learning}{217}%
\contentsline {section}{\numberline {35.1}Overview}{217}%
\contentsline {subsection}{\numberline {35.1.1}Same loss function, different data distribution}{217}%
\contentsline {subsection}{\numberline {35.1.2}Different loss function}{218}%
\contentsline {subsection}{\numberline {35.1.3}training network}{218}%
\contentsline {subsection}{\numberline {35.1.4}Conditioning task descriptor}{218}%
\contentsline {subsection}{\numberline {35.1.5}optimizing the objective}{219}%
\contentsline {section}{\numberline {35.2}Relationships}{221}%
\contentsline {subsection}{\numberline {35.2.1}Architecture}{221}%
\contentsline {subsection}{\numberline {35.2.2}loss}{221}%
\contentsline {subsection}{\numberline {35.2.3}Training Dynamics}{221}%
\contentsline {section}{\numberline {35.3}interesting considerations}{221}%
\contentsline {section}{\numberline {35.4}Parameter Sharing}{222}%
\contentsline {subsection}{\numberline {35.4.1}Hard parameter sharing}{222}%
\contentsline {subsection}{\numberline {35.4.2}Soft parameter sharing}{222}%
\contentsline {subsection}{\numberline {35.4.3}Other}{222}%
\contentsline {section}{\numberline {35.5}Mechanisms}{223}%
\contentsline {chapter}{\numberline {36}Meta Learning}{225}%
\contentsline {section}{\numberline {36.1}Overview}{225}%
\contentsline {subsection}{\numberline {36.1.1}taxonomy}{225}%
\contentsline {section}{\numberline {36.2}Outer Optimization}{226}%
\contentsline {section}{\numberline {36.3}Inner Optimization}{226}%
\contentsline {subsection}{\numberline {36.3.1}Embedding Functions / Metric-based Meta-Learning}{226}%
\contentsline {subsubsection}{\numberline {36.3.1.1}Implementations}{226}%
\contentsline {paragraph}{\numberline {36.3.1.1.1}Siamese Neural Networks}{226}%
\contentsline {paragraph}{\numberline {36.3.1.1.2}Matching Networks}{226}%
\contentsline {paragraph}{\numberline {36.3.1.1.3}Prototypical Networks}{227}%
\contentsline {paragraph}{\numberline {36.3.1.1.4}Relation Networks}{227}%
\contentsline {paragraph}{\numberline {36.3.1.1.5}Attentive Recurrent Comparators}{227}%
\contentsline {paragraph}{\numberline {36.3.1.1.6}Graph neural Netwosk}{227}%
\contentsline {paragraph}{\numberline {36.3.1.1.7}TADAM}{227}%
\contentsline {paragraph}{\numberline {36.3.1.1.8}TEAM}{227}%
\contentsline {subsection}{\numberline {36.3.2}Model-based Meta-Learning -- Memory Enhanced}{227}%
\contentsline {subsubsection}{\numberline {36.3.2.1}Implementations}{227}%
\contentsline {paragraph}{\numberline {36.3.2.1.1}Memory-augmented Neural networks (MANNs)}{227}%
\contentsline {paragraph}{\numberline {36.3.2.1.2}Meta networks}{228}%
\contentsline {paragraph}{\numberline {36.3.2.1.3}Simple Neural Attentive Meta-Learner (SNAIL)}{228}%
\contentsline {subsection}{\numberline {36.3.3}Model-based Meta-Learning -- No additional memory}{228}%
\contentsline {subsubsection}{\numberline {36.3.3.1}Implementations}{228}%
\contentsline {paragraph}{\numberline {36.3.3.1.1}Conditional Neural Processes}{228}%
\contentsline {paragraph}{\numberline {36.3.3.1.2}Neural Statistician}{228}%
\contentsline {subsection}{\numberline {36.3.4}Optimization-based Meta-Learning}{228}%
\contentsline {subsubsection}{\numberline {36.3.4.1}Implementations}{228}%
\contentsline {subsubsection}{\numberline {36.3.4.2}Implementations}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.1}Model-Agnostic Meta-Learning (MAML) and FOMAML}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.2}Meta-learning with implicit Gradients (iMAML)}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.3}Follow the Meta Leader (FTML)}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.4}Meta-SGD}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.5}Reptile}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.6}Latent Embedding Optimizaiton (LEO)}{229}%
\contentsline {paragraph}{\numberline {36.3.4.2.7}Uncertainty}{229}%
\contentsline {subparagraph}{LLAMA (Laplace Approximations for Meta-Aadaptation)}{229}%
\contentsline {subparagraph}{PLATIPUS (Probabilistic LATent model for Incorporating Priors and Uncertainty in few-Shot learning)}{229}%
\contentsline {subparagraph}{Bayesian MAML (BMAML)}{230}%
\contentsline {paragraph}{\numberline {36.3.4.2.8}ANIL (Almost No Inner Loop)}{230}%
\contentsline {paragraph}{\numberline {36.3.4.2.9}BOIL (Body Only update in Inner Loop)}{230}%
\contentsline {section}{\numberline {36.4}Other}{230}%
\contentsline {subsection}{\numberline {36.4.1}k-shot learning}{230}%
\contentsline {section}{\numberline {36.5}Other / To include}{231}%
\contentsline {chapter}{\numberline {37}Continual learning}{233}%
\contentsline {section}{\numberline {37.1}Catastrophic Forgetting}{233}%
\contentsline {subsection}{\numberline {37.1.1}Overcoming catastrophic forgetting}{233}%
\contentsline {subsubsection}{\numberline {37.1.1.1}Regularization-based}{233}%
\contentsline {subsubsection}{\numberline {37.1.1.2}Expansion-based}{233}%
\contentsline {subsubsection}{\numberline {37.1.1.3}Replay-based Methods}{234}%
\contentsline {part}{VIII\hspace {1em}Interpretability}{235}%
\contentsline {chapter}{\numberline {38}Interpretability, Analysis}{237}%
\contentsline {section}{\numberline {38.1}overview}{237}%
\contentsline {section}{\numberline {38.2}dump space}{237}%
\contentsline {section}{\numberline {38.3}Memorization}{237}%
\contentsline {part}{IX\hspace {1em}Model Releases: Using+Monitoring Models}{239}%
\contentsline {chapter}{\numberline {39}Applied}{241}%
\contentsline {section}{\numberline {39.1}Speed up}{241}%
\contentsline {chapter}{\numberline {40}Experimental Design}{243}%
\contentsline {section}{\numberline {40.1}Overview}{243}%
\contentsline {chapter}{\numberline {41}Model Reporting}{245}%
\contentsline {chapter}{\numberline {42}Deployment}{247}%
\contentsline {chapter}{\numberline {43}Monitoring}{249}%
\contentsline {chapter}{\numberline {44}Papers to read}{251}%
\contentsline {part}{X\hspace {1em}Ethics}{253}%
\contentsline {chapter}{\numberline {45}Ethics}{255}%
\contentsline {section}{\numberline {45.1}Overview}{255}%
\contentsline {section}{\numberline {45.2}Pieces to fit together}{255}%
\contentsline {section}{\numberline {45.3}Should something be published}{255}%
\contentsline {section}{\numberline {45.4}Further information}{255}%
\contentsline {section}{\numberline {45.5}bias}{256}%
\contentsline {section}{\numberline {45.6}Thoughts}{256}%
\contentsline {section}{\numberline {45.7}To add}{256}%
\contentsline {section}{\numberline {45.8}examples}{256}%
\contentsline {part}{XI\hspace {1em}My Opinions: Moving Forward}{257}%
\contentsline {chapter}{\numberline {46}My opinions}{259}%
\contentsline {part}{XII\hspace {1em}Applications}{261}%
\contentsline {chapter}{\numberline {47}EndToEnd}{263}%
\contentsline {section}{\numberline {47.1}Structured}{264}%
\contentsline {subsection}{\numberline {47.1.1}Linear Regression}{264}%
\contentsline {subsection}{\numberline {47.1.2}KNN}{264}%
\contentsline {section}{\numberline {47.2}Image}{264}%
\contentsline {subsection}{\numberline {47.2.1}Image Classification}{264}%
\contentsline {subsection}{\numberline {47.2.2}Image Segmentation}{264}%
\contentsline {subsection}{\numberline {47.2.3}Adversarial Exmaples}{264}%
\contentsline {subsection}{\numberline {47.2.4}AutoEncoder}{264}%
\contentsline {section}{\numberline {47.3}TimeSeries}{264}%
\contentsline {section}{\numberline {47.4}Text}{264}%
\contentsline {subsection}{\numberline {47.4.1}Sentiment Analysis}{264}%
\contentsline {section}{\numberline {47.5}Audio}{264}%
\contentsline {subsection}{\numberline {47.5.1}Audio to Text}{264}%
\contentsline {section}{\numberline {47.6}Recommendation Systems}{264}%
\contentsline {chapter}{\numberline {48}Interesting (to me) Applications}{265}%
\contentsline {section}{\numberline {48.1}Fit Somewhere}{265}%
\contentsline {part}{XIII\hspace {1em}Brief Reference}{267}%
\contentsline {chapter}{\numberline {49}Mistakes and Bloopers}{269}%
\contentsline {chapter}{\numberline {50}Environment}{271}%
\contentsline {chapter}{\numberline {51}Common Libraries}{273}%
\contentsline {part}{XIV\hspace {1em}appendix}{275}%
\contentsline {chapter}{\numberline {52}TODO:}{277}%
\contentsline {section}{\numberline {52.1}Time Zones}{277}%
\contentsline {section}{\numberline {52.2}Types of missingness}{277}%
\contentsline {paragraph}{\numberline {52.2.0.0.1}Missing at Random (MAR)}{277}%
\contentsline {paragraph}{\numberline {52.2.0.0.2}Missing Completely at Random}{277}%
\contentsline {paragraph}{\numberline {52.2.0.0.3} Ignorable (structurally missing)}{277}%
\contentsline {paragraph}{\numberline {52.2.0.0.4}Nonignorable (missing not at random)}{278}%
\contentsline {section}{\numberline {52.3}Imputing Missing Values}{278}%
\contentsline {subsection}{\numberline {52.3.1}Imputation}{278}%
\contentsline {subsection}{\numberline {52.3.2}Interpolation}{278}%
\contentsline {section}{\numberline {52.4}Anomaly Detection}{278}%
\contentsline {subsection}{\numberline {52.4.1}methods}{278}%
\contentsline {subsubsection}{\numberline {52.4.1.1}PCA}{278}%
\contentsline {section}{\numberline {52.5}labeling data}{279}%
\contentsline {section}{\numberline {52.6}Group Segmentation}{279}%
\contentsline {section}{\numberline {52.7}Time Series Concepts}{279}%
\contentsline {part}{XV\hspace {1em}Dump Space}{281}%
\contentsline {subsection}{\numberline {52.7.1}Restricted Boltzmann Machines}{286}%
\contentsline {subsection}{\numberline {52.7.2}Greek letters}{288}%
\contentsline {subsection}{\numberline {52.7.3}Basic Log Math}{288}%
\contentsline {subsection}{\numberline {52.7.4}Basic Matrix Math}{288}%
\contentsline {subsection}{\numberline {52.7.5}Basic Linear Algebra}{288}%
\contentsline {subsection}{\numberline {52.7.6}Representation Learning}{288}%
\contentsline {subsection}{\numberline {52.7.7}others}{288}%
\contentsline {section}{\numberline {52.8}research to include}{289}%
\contentsline {part}{XVI\hspace {1em}Appendix}{293}%
\contentsline {chapter}{\numberline {53}Augmentation Techniques}{295}%
\contentsline {section}{\numberline {53.1}Data Imbalance or Unbalanced Data}{296}%
\contentsline {subsection}{\numberline {53.1.1}Methods}{296}%
\contentsline {subsubsection}{\numberline {53.1.1.1}Data}{297}%
\contentsline {subsubsection}{\numberline {53.1.1.2}Model}{297}%
\contentsline {paragraph}{\numberline {53.1.1.2.1}Losses}{297}%
\contentsline {subsubsection}{\numberline {53.1.1.3}Post processing}{297}%
\contentsline {paragraph}{\numberline {53.1.1.3.1}Calibration}{297}%
\contentsline {subsection}{\numberline {53.1.2}Evaluation}{297}%
\contentsline {chapter}{\numberline {54}Statsy Stuff}{299}%
\contentsline {section}{\numberline {54.1}Experimental Design}{299}%
\contentsline {section}{\numberline {54.2}``tests''}{299}%
\contentsline {subsection}{\numberline {54.2.1}T test}{300}%
\contentsline {subsection}{\numberline {54.2.2}Chi-Squared Test}{300}%
\contentsline {section}{\numberline {54.3}Power Analysis}{300}%
\contentsline {section}{\numberline {54.4}Analysis of Variance (ANOVA)}{301}%
\contentsline {section}{\numberline {54.5}Transforms}{301}%
\contentsline {section}{\numberline {54.6}Variance Reduction Methods}{301}%
\contentsline {section}{\numberline {54.7}Distributions}{303}%
\contentsline {subsection}{\numberline {54.7.1}Common Distributions}{303}%
\contentsline {subsubsection}{\numberline {54.7.1.1}Normal Distribution / Gaussian Distribution}{303}%
\contentsline {subsubsection}{\numberline {54.7.1.2}Uniform Distribution}{303}%
\contentsline {subsubsection}{\numberline {54.7.1.3}Bernoulli Distribution}{303}%
\contentsline {subsubsection}{\numberline {54.7.1.4}Poisson Distribution}{303}%
\contentsline {chapter}{\numberline {55}Appendix}{305}%
\contentsline {section}{\numberline {55.1}Minor Explanations}{305}%
\contentsline {subsection}{\numberline {55.1.1}Convolution vs Cross-Correlation}{305}%
