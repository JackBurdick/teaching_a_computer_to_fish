\chapter{Foundational Methods}

%% Maybe (Foundational Methods --- supervised)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Regression
\input{./foundations/regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Logistic Regression
\input{./foundations/logistic_regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% KNN
\input{./foundations/nearest_neighbor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Support Vector Machines
\input{./foundations/svm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Naive Bayes
\input{./foundations/naive_bayes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Decision Trees
\input{./foundations/decision_trees}


\chapter{Artificial Neural Networks}

\textcolor{blue}{If a perceptron is analogous to a single neuron, an artificial neural network (either feedforward or feedback) would be analogous to a brain.}

\r{powerful and general framework for representing non-linear mappings (function approximation) from input features to outputs, where the form of the mapping is controlled by adjustable parameters (weights and biases). Determining the values for these parameters is the ``learning'' or training.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% perceptron
\input{./foundations/perceptron}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% overview
\input{./foundations/ann_overview}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% feedforward
\input{./foundations/feedforward}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% feedback
\input{./foundations/feedback}

\chapter{Common Operations/Components}

\input{./operations/overview}

\chapter{Applied Neural Networks}

\input{./applied/overview}


\chapter{Unsupervised}

\r{The reality is that data is not always labeled.}

\r{unsupervised learning is capable of finding hidden patterns in the underlying structure of hte data.}

\r{attempts to represent data with increaingly fewer parameters}

\r{Discovering hidden structures or patterns in unlabeled training data.}

\TD{Neighborhood-Based Methods \ref{nearest_neighbors} \r{lazy learners  -- learn how to label new instances based on proximity to existing instances}}

% TODO: placement / may need to rename+restructure sections
\textcolor{blue}{unsupervised methods may be commonly used in two main settings:}
\begin{enumerate}[noitemsep,topsep=0pt]
	\item Data Exploration
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Visualization (clustering \textcolor{red}{local ref})
	\end{itemize}
	\item Preprocessing (e.g. prior to a supervised method): unsupervised pretraining may be considered a form of regularization
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Compressing (dimensionality reduction \textcolor{red}{local ref})
		\item Creating new/different representations
	\end{itemize}
\end{enumerate}

\r{regularization, feature engineering, detecting outliers -- also used for detecting how different new (incoming) training data is from the current distribution.}

\r{popular applications --- anomaly detection, group segmentation, preprocessing (dimensionality reduction)}

\subsection{TODO}

\TD{evaluating unsupervised learning systems are harder to evaluate than supervised learning systems.}

\r{evaluating supervised methods is often subjective. An approach to skirt this is issue may be to label the test set manually and then use any desired supervised learning metric.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% clustering
\input{./foundations/unsupervised/clustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Dimensionality Reduction
\input{./foundations/unsupervised/dimensionality_reduction}


% TODO: format / other
%\input{./foundations/unsupervised/applications}


\chapter{Semi-supervised}

\input{./foundations/semi_supervised}


\chapter{Common Architectures}
%TODO: should this also include a ``history''?

\input{./foundations/common_architectures}


% chapter: model compression
\input{./foundations/model_compression}

% External memory
\input{./foundations/external_memory}

% chapter: training dynamics
\input{./foundations/training_dynamics}

\input{./foundations/adversarial_examples}


\input{./foundations/graph_neuralnetworks}


\input{./foundations/generative}

\input{./foundations/curriculum_learning}
