\chapter{Improving Generalizability}

\r{The methods shown in the upcoming sections aim to reduce overfitting. That is, these methods aim to prevent the model from becoming too specialized to the training dataset in hopes that it will generalize to data that it has not specifically seen during training (e.g from the ``test'' set).}

\r{By implementing some of these methods (e.g. reducing the model capacity), the model often has less ability to model the training set as well as it might otherwise be able to. This is ok, high performance on the test set is the ultimate goal.}

%  some of the methods aren't used before they are necessary \TD{section on determining overfitting}

% TODO: index overfitting
\r{overfitting: a practical definition may include observing the training loss to improve while the validation loss degrades. \TD{possibly mention \\cite{Nakkiran2020DeepDD}}}

\r{Overfitting --- too complex --- Occam's razor --- hypothesis with the fewest assumptions is best}

\r{A specific instance of improving generalization might be accounting for imblance. Either in the labels or in the features.  Section \ref{app_data_imbalance} discusses this topic and strategies in more detail.}

\r{Typicaly types of modifications that are made to improve generalization.}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Data
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Increase ammount of data
		\item Augmentation
		\item Sampling
	\end{itemize}
	\item Architecture --- Reduce complexity of model e.g. applying parameter constraints, and/or reduce overall number of parameters
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Reduce complexity/number of parameters
		\item Ensembling
		\item Constraints
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Directly on parameters
			\item Through additional losses/tasks
		\end{itemize}
	\end{itemize}
	\item Training Pattern
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Early stopping
		\item Stochastic Behavior
	\end{itemize}
\end{itemize}


\section{Data}

\subsection{Data Collection}

\r{Arguably the best way to increase generalizability of a model is to train the model on more data. However, as readers may already be aware, this is not always easy. Collecting more data may not be time/cost effective, or even possible.}

\r{``free'' data in that the ``cost'' is minor computation}

\subsubsection{Data Labeling}

%TODO: later sections likely belong in an appendix

\r{Labeling unlabed data}

\begin{itemize}[noitemsep,topsep=0pt]
	\item semi-supervised
	\item active learning
	\item weak supervision
\end{itemize}

\paragraph{Semi-supervised}

\TD{label propagation}

\TD{Book~\cite{chapelle2010semi}}

\TD{using GANs: Improved Techniques for Training GANs~\cite{DBLP:journals/corr/SalimansGZCRC16}}

\TD{Temporal Ensembling for Semi-Supervised Learning~\cite{DBLP:journals/corr/LaineA16}}

\paragraph{Active Learning}

\TD{A Survey of Deep Active Learning~\cite{DBLP:journals/corr/abs-2009-00236}}


% TODO: this reminds me of another paper I read that was counter intuitive at first... maybe on data echoing?
\TD{Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering~\cite{DBLP:journals/corr/abs-2107-02331}  Reminds me a bit of the paper on penalizing confident outputs~\cite{DBLP:journals/corr/PereyraTCKH17} as well as ``Are all negatives created equal in contrastive instance discrimination?''~\cite{DBLP:journals/corr/abs-2010-06682}}

\TD{intelligently sample data. Select instances that would be most informative for training}

\TD{Intelligent sampling could use a few different methods}

\TD{life cycle could include: taking unlabeled data, using the active learning sampler to pick instances, using a human annotator for these points, then using this new labeled set for or in addition to the current training set for training}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Margin Sampling
	\item Cluster Based Sampling
	\item Query-by-committee
	\item Region-based Sampling
\end{itemize}

\subparagraph{Margin Sampling}

\r{Select instances that are nearest to the decision boundary (margin) e.g. the most uncertain and train on these points}

\subparagraph{Cluster Based Sampling}

\r{sample from the well formed clusters}

\subparagraph{Query-by-Committee}

\r{train and ensemble of models and sample from the data points that the models disagree on.}

\subparagraph{Region-based Sampling}

\r{Run several algorithms (from above) on different portions of the space}


\paragraph{Weak Supervision}

\TD{Weak supervision: https://ai.stanford.edu/blog/weak-supervision/}

\TD{Snorkel: Rapid Training Data Creation with Weak Supervision~\cite{DBLP:journals/corr/abs-1711-10160}}


\subsection{Augmentation}

\r{Dataset augmentation is \textcolor{green}{TODO}}

\r{adds examples that are similar to real}

\TD{Usupervised data augmentation: UDA}

\r{Please note, augmentation must be done responsibly. For example, if performing digit recognition, it would not be wise to perform rotational or flip transformations on the data since, depending on the specific data, a 6, rotated 180 or flipped vertically may now appear as a 9.}


\r{invariances in the data}

\r{For specific techniques, see~\ref{app_aug_techniques}}

\TD{Beyond improving generalization, augmentation may be used in other contexts as well, such as in helping quantify uncertainty -- \TD{see ref ---\TD{Augmenting the test set. A simple augmentation (horizontal filliping) was performed on the test set in \cite{simonyan2014very} -- where the prediction of the original and augmented images are averaged to obtain the final output score.} }}


\subsection{Sampling}

\r{The line between the techniques described here and ``augmentation'' might be a little blurred, in that sampling might technically be considered a augmentation technique (and I'm not even sure ``sampling'' is the appropriate title). But the intended distinction is that in augmentation, we are diliberately altering something (e.g. the input data) and in sampling, we are altering the number of times an architecture sees a particular instance in a training dataset.}

\TD{see appendix section for methods}



\section{Architecture}

\section{Training Pattern}

\subsection{Early Stopping}

\r{see p.243 of DL, papers Bishop 1995 and Sjoberg and Ljung 1995}

% TODO: note about regularization --- the smaller the value, the stronger the regularization.


\subsection{Stochastic Behavior}

\subsubsection{Dropout}

\r{``Dropout'' as a node in a computational graph may be considered an architectural structure change, but the method itself affects the training pattern in possibly not obvious ways. }

% TODO: explain dropout

\r{Dropout -- ref original paper (Hinton? -- intuitive, inspired by bank -- that defrauding the bank would require cooperation between employees to defraud the bank \TD{cite})}.

\r{Dropout (proposed in ``Improving Neural Networks by Preventing Co-Adaption of Feature Dectors''~\cite{DBLP:journals/corr/abs-1207-0580}, and popularized by Nitish et.al in ``Dropout: a Simple Way to Prevent Nerual Networks from Overfitting''~\cite{JMLR:v15:srivastava14a}}

\r{It is important to note that dropout is only present during training. i.e. dropout does not occur during test/evaluation if using dropout in the ``standard way''. However dropout is occassionally used for evaluation in attempt to quantify model uncertainty \TD{CITATION}}

\r{keeps a neuron active by a hyperparameterized probability.}

\r{used in any/all neurons in the network (other than the output neruons).}

\r{think about where dropout is used. That is when you use dropout at any given nueron the upstream paths transversing that particular neuron are also affected (in this case, ``turned off''), as well downstream connections (but often only modified, not entirely turned off since they often still have other inputs) }

\r{Forces the network to learn mappings even in the absence of all the information, that is the network is forced to consider the values of other values and can't rely on a smaller number of values or groups of values. Said another way, the network is prevented from becoming too dependent on certain inputs or features.}

\r{In this way, dropout can be thought of as sort of an ensembling method. When dropout is in use during training, each loop technically produces a different network that is then trained for the given task. During the next loop, a different network is used. As Aurélien Géron~\cite{geron2019hands} describes, if you train for 10,000 training steps (where dropout is used), you will have likely (almost certainly) trained 10,000 different neural networks. It's true that each network is not indpendant (they share weights), but they are different. More generally, a network with $N$ activations with dropout present, there exist $2^N$ possible networks ($2$ since each activation/neuron/value can have either an `on` or `off` state.) and thus, the use of all of these networks at once can be considered an ensembling of sorts.}

\TD{create figure of this ensemble of many networks.}


% TODO: find recent paper I saw mentioned on twitter.... (4July) it may be in my pocket

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil
	\caption{\TD{Graph of an example function including dropout. three separate training iterations and how the network changes}}
	\label{fig:regularization_dropout_overview_training}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\caption{\TD{Same graph during test --- no dropout applied}}
	\label{fig:regularization_dropout_overview_test}
\end{figure}

\r{It is worth pointing out that since dropout is only applied at training time, comparing the loss curve of training and inference (validation splits) will be a bit misleading since the full ensemble network is used for calculating the validation loss/metrics and only the component \TD{is there a better word than this?} networks are used for the training set.}

\r{Additionally, if you run the training set through multiple times, you may find slightly different results. Again, this is because while dropout is on, you'll find that a slightly different network is used. \TD{This idea can be exploited at inference time to get uncertainty estimates.}}

\r{some important notes about the implementation. The outputs at test time should be equivalent to their expected outputs at training time (which is altered due to the application of dropout).}

\r{Couple solutions}
\begin{itemize}[noitemsep,topsep=0pt]
	\item scale the outputs during inference
	\item
\end{itemize}

\r{One potential solution to this problem is to scale the outputs during inference in a way that compensates for the dropout probability.  For example, if the dropout rate was set to $0.5$, then it would become necessary to halve the neurons outputs at test time in order to keep the expected output the neurons have learned during training.  However, this may not be ideal in practice since it would require scaling all the neuron outputs at test time (where performance is often critical and more important).}

\r{at test time, multiply the values by the expectation, not the on/off mask}

\r{Another, perhaps more desirable solution, would be to use \IDI{inverted dropout}. The cs231n~\cite{cs231n} course provides a concise explaination and example code on this topic.}

\r{This applies the same principal as outlined above, only the scaling occurs at training time rather that at test time. That is, during training, any neuron whose activation was not turned off, has the output divided by the dropout rate before being propagation to the next layer.  This way, at test time, no scaling is required.}

% helps learn ``multiple paths''/simulates ensembles
\TD{link to ensemble section}

\subsubsection{Others}

\TD{``during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function'' --- Deep Networks with Stochastic Depth \cite{DBLP:journals/corr/HuangSLSW16}}

\TD{DropConnect~\cite{wan2013regularization} is similar to dropout, except that individual weights are disabled, not entire individual nodes and can be considered a generalization of dropout.}

\TD{figure showing difference}

% `drop block''?
\TD{investigate more structured dropout.}


\TD{structured --- ``contiguous region of a feature map are dropped together'' DropBlock  \cite{DBLP:journals/corr/abs-1810-12890}}


\TD{alpha dropout\cite{DBLP:journals/corr/KlambauerUMH17}}



\subsection{Parameter Regularization}

\r{Collection of techniques used to help generalize a model -- which may help prevent overfitting. Typically regularization penalizes complexity of a model.}


% TODO: figure of loss plot showing a steep training and shallow+divergent val/test loss

\r{imposes a penalty on the parameters}

\r{Helps prevent the model from memorizing noise in the training data.}

\r{Discourages the learned mapping/function/model from becoming too complex}


\subsubsection{Types of Regularization}

\textcolor{blue}{Regularization is an active area of research.}

% more information on L1/L2 http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/

\begin{itemize}[noitemsep,topsep=0pt]
	\item Early Stopping (implementation: \textcolor{red}{local ref})
	\item Parameter Norm Penalties (implementation: \textcolor{red}{local ref})
	\begin{itemize}[noitemsep,topsep=0pt]
		\item L1 (Lasso) Regularization
		\item L2 (Ridge) Regularization
		\item Elastic Nets
	\end{itemize}
	\item Dataset Augmentation (implementation: \textcolor{red}{local ref})
	\item Noise Robustness
	\item Sparse Representations
	\item Dropout (implementation: \textcolor{red}{local ref})
	\item Ensemble methods (implementation: \textcolor{red}{local ref})
	\item Adversarial Training
\end{itemize}



\subsubsection{Parameter Norm Penalties}

\r{key difference is the penalty term}

\TD{TODO: DIGRAM OF L2 + L1 + elastic nets}

\paragraph{L2 Regularization}

\TD{TODO: DIAGRAM OF L2}

\r{L2, ({Ridge regression}\index{Ridge regression}) may also be known as {Tikhonov regularization}\index{Tikhonov regularization}}

\r{penalizes model parameters that become too large. Will force most of the parameters to be small, but still non-zero}

\r{square of the absolute value of the coefficient}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil\\
	\medskip
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil
	\caption{\TD{Top: NN output decision boundary on 2D dataset Bottom: weight params distribution from tensorboard... from LtoR = same arch with varying degrees of L2 regularization (0.01, 0.1 and 1.0)}}
	\label{fig:basics_regularization_l2_example}
\end{figure}


% p91(71) of mastering ML w SKL says "when lambda is equal to zero, ridge regression is equal to linear regression"

\paragraph{L1 Regularization}

\TD{TODO: DIAGRAM OF L1}

\r{LASSO (\textbf{L}east \textbf{A}bsolute \textbf{S}hrinkage and \textbf{S}election \textbf{O}perator) --- produces sparse parameters. This will force coefficients to zero and cause the model to depend on a small subset of the features.}

\r{absolute value of the weight coefficient}

\r{use only a small subset of the input features and can become resistant to noisy inputs.}

\r{It could be argued that using L1 regularization may help to make a model more interpretable, by using less (presumably more important/relevant) features when making predictions.}

\r{The use of L1 regularization for feature selection}


\paragraph{Elastic Net Regularization}

\r{Linearly combines the $L^1$ (feature selection) and $L^2$ (generalizability) penalties used by both LASSO and ridge regression. The cost is having two parameters (as opposed to just one when using either L1 or L2).}

\TD{TODO: figure}.



\subsection{Ensemble Methods}

\r{see \textcolor{red}{local ref} for more information on ensemble basics and see \textcolor{red}{local ref} for implementation details.}

% TODO: find Breiman 1994 paper referenced in p249 of Deep Learning
\r{As described in \textcolor{red}{local ref} ensemble methods act as a form of regularization by combining several different models \TD{Breiman 1994}. This often improves generalizability since the included models will often make independent, different, errors on the data.}

\subsection{Adversarial Training}

% TODO:



% TODO: this likely does not belong here...
\subsection{Normalization}

\r{Normalizations role on training dynamics is actively studied and can behave differently in different modalities and architectures.  For example, though batch norm is largely considered ``helpful'' in feed forward networks, batch norm, it's not as clear in recurrent neural networks and has lead to altered formulations such as ``sequence-wise normalization''~\cite{Laurent2016BatchNR}  ``recurrent batch normaliziation''~\cite{DBLP:journals/corr/CooijmansBLC16}}

% TODO: Analysis on normalization layers 
\TD{Beyond BatchNorm: Towards a General Understanding of Normalization  in Deep Learning~\cite{DBLP:journals/corr/abs-2106-05956}}

\TD{TODO: overview para + importance}

\TD{TODO: figure showing differences}



\paragraph{Proxy Normalization}

\TD{Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence~\cite{DBLP:journals/corr/abs-2106-03743}}


\subsubsection{Activation-Based Layers}

\r{BatchNorm, LayerNorm, Intance Norm, Group Norm, Filter Response Normalization Variance Normalization, EvoNormBO, EvoNoRMSO}

\r{act on the activations of the layers}

\paragraph{Instance normalization}

\TD{Instance Normalization: The Missing Ingredient for Fast Stylization~\cite{DBLP:journals/corr/UlyanovVL16}}

\paragraph{Layer Normalization}

\TD{Layer Normalization \cite{Ba2016LayerN}}

\paragraph{Group Normalization}

\TD{Group Normalization \cite{DBLP:journals/corr/abs-1803-08494}}

\paragraph{Filter Response Normalization}

\TD{Filter Response Normalization Layer: Eliminating Batch Dependence
	in the Training of Deep Neural Networks~\cite{DBLP:journals/corr/abs-1911-09737}}

\paragraph{Variance Normalization}

\TD{Variance Normalization~\cite{Daneshmand2020TheoreticalUO}}


\paragraph{EvoNorm BO and EvoNoRMSO}
% TODO: Read this
\TD{Evolving Normalization-Activation Layers \cite{DBLP:journals/corr/abs-2004-02967}}




\paragraph{Batch normalization}

% TODO: Read this
\TD{Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs \cite{DBLP:journals/corr/abs-2003-00152}}

\TD{Show / explain}

\TD{Batch Normalization: Accelerating Deep Network Training by Reducing	Internal Covariate Shift \cite{DBLP:journals/corr/IoffeS15}}

\r{similar to dropout \ALR, the behavior of batch norm is different at training time and inference time.}

\r{normalizes values across a batch of data. Where the normalization is controlled by two learned parameters. The ``center'' and ``scale''.}

\r{Standard implementation is to calculate the population values using an exponential moving average (EMA).}

%TODO: here!

\TD{{Rethinking "Batch" in BatchNorm}~\cite{Wu2021RethinkingI} concludes that using EMA as the method for calculating the population statistics is not ideal. They show that during the early epochs, the xxxxxxx.}

\r{An adaptive re-parameterization.}

\r{reduce sensitivity to hyperparameterization.}

\TD{TODO: transfer learning considerations --- will likely have to unfreeze these params}

% HUGO talk
\r{``making the optimization easier''. batch norm is not effective in RNNs -- more so layer norm}

\r{seems to help when both under and over fitting.}

\r{order, up for debate and often described as either pre-activation operation, then activation, then batch norm, or pre-activation operation, then batch norm, then activation.}

\r{$\gamma$ and $\beta$ parameters that are learned parameters. These params could effectively undo the normalization caused (if ``learned'' to do so.)}


\begin{enumerate}[noitemsep,topsep=0pt]
	\item batch statistics
	\begin{itemize}[noitemsep,topsep=0pt]
		\item mean
		\item variance
	\end{itemize}
	\item normalize the pre-activation
	\item $\gamma$ and $\beta$ --- learned rescalling
\end{enumerate}

\TD{Rethinking "Batch" in BatchNorm \cite{DBLP:journals/corr/abs-2105-07576}}

% TODO: haven't read this paper yet 9Oct21 (I don't think...)
\TD{How Does Batch Normalization Help Optimization? \cite{Santurkar2018HowDB}}


% Graham Taylor talk
\r{paper~\cite{DBLP:journals/corr/abs-1905-02161}}
\begin{itemize}[noitemsep,topsep=0pt]
	\item turn down other regularization
	\item fixes first and second moments which may suppress information in these moments.
\end{itemize}

\TD{work related to adversarial spheres. --- with batch norm, the result was more reflective of the batch, not the entire dataset (which makes sense, right?)}



\r{Lubana et al.~\cite{DBLP:journals/corr/abs-2106-05956} aggregate beneficial properties of batch normalization as follows}
\begin{itemize}[noitemsep,topsep=0pt]
	\item propagating informative activation patterns in deeper layers
	\item reduced dependence on initialization
	\item faster convergence via removeal of outlier eigenvalues
	\item auto-tuning of leraning rates
	\item equivalent to modern adaptive optimizers
	\item smoothing of loss landscape
\end{itemize}

\r{Lubana et al.~\cite{DBLP:journals/corr/abs-2106-05956} aggregate potential areas where batch normalization doesn't work as well}
\begin{itemize}[noitemsep,topsep=0pt]
	\item small batch-sizes
	\item shifts in distributions from original training
	\item in meta-learning, can lead to transdutive inference
	\item in adversarial learning, can affect accuracy by estimating incorrect statistics
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%
%% batch normalization

\TD{Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks~\cite{Daneshmand2020TheoreticalUO}}

\TD{The Shattered Gradients Problem~\cite{DBLP:journals/corr/BalduzziFLLMM17}}

\TD{Batch Normalization Biases Deep Residual Networks Towards Shallow Paths~\cite{DBLP:journals/corr/abs-2002-10444}}

\TD{Is normalization indispensable for training deep neural network?~\cite{shao2020normalization}}

\TD{Fixup Initialization: Residual Learning Without Normalization~\cite{DBLP:journals/corr/abs-1901-09321}}

\TD{The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks~\cite{Karakida2019TheNM}}

\TD{Understanding Batch Normalization~\cite{DBLP:journals/corr/abs-1806-02375}}

\TD{Theoretical Analysis of Auto Rate-Tuning by Batch Normalization~\cite{DBLP:journals/corr/abs-1812-03981}}

\TD{Noether's Learning Dynamics: The Role of Kinetic Symmetry Breaking
	in Deep Learning~\cite{DBLP:journals/corr/abs-2105-02716}}

\TD{Adaptive Gradient Methods, Normalization, and Weight Decay~\cite{grosseadaptive}}

\TD{Gradient Centralization~\cite{DBLP:journals/corr/abs-2004-01461}}

\TD{Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized
	Models~\cite{DBLP:journals/corr/Ioffe17}}

\TD{Four Things Everyone Should Know to Improve Batch Normalization~\cite{DBLP:journals/corr/abs-1906-03548}}

\TD{Transferable normalization: Towards improving transferability of deep neural network~\cite{wang2019transferable}}

\TD{TaskNorm: Rethinking Batch Normalization for Meta-Learning~\cite{Bronskill2020TaskNormRB}}

\TD{Batch Normalization is a Cause of Adversarial Vulnerability~\cite{DBLP:journals/corr/abs-1905-02161}}

\TD{Adversarial Examples Improve Image Recognition~\cite{DBLP:journals/corr/abs-1911-09665}}
%%%%%%%%%%%%%%%%%%%%%



\subsubsection{Parametric Layers}

\paragraph{Weight Normalization}

\TD{Weight Normalization~\cite{DBLP:journals/corr/SalimansK16}}


\paragraph{Scaled Weight Standardization}

% TODO: read this
\TD{Scaled Weight Standardization~\cite{DBLP:journals/corr/abs-2101-08692} paper also introduced Signal Propagation Plots (SPPs)}

\r{built as an improvement to}
\TD{Weight Standardization~\cite{DBLP:journals/corr/abs-1903-10520}}

\TD{Centered weight normalization in accelerating training of deep neural networks~\cite{huang2017centered}}


\section{Output regularization}

\r{confidence penalty on predictions that are extrememly confident\cite{pereyra2017regularizing}. Originally an RL idea to promote expoloration. In SL, we would prefer fast convergence i) anneal confidence penalty ii) only penalize at a certain confidence threshold (lower entropy threshold). Intuitive (or not), can improve generalization.}

%TODO:
\r{label smoothing\cite{szegedy2016rethinking}}

\r{Adding label noise\cite{xie2016disturblabel}}

\r{smooth labels -- either via a ``teacher model''\cite{hinton2015distilling} or using it's own distribution\cite{reed2014training}}

\r{virtual adversarial training\cite{miyato2018virtual}}