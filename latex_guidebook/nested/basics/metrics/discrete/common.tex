\subsubsection{Confusion Matrix}
\textcolor{blue}{A confusion matrix (sometimes referred to as a table of confusion, or contingency table) XXXXXXXX}

%% Confusion matrix
\begin{table}
	\centering
	\begin{tabular}{l|l|c|c|}
		\multicolumn{2}{c}{}&\multicolumn{2}{c}{Ground Truth}\\ 
		\cline{3-4}
		\multicolumn{2}{c|}{}&Positive&Negative\\ 
		\cline{2-4}
		\multirow{2}{*}{\rotatebox{90}{Pred}}& Positive & $TP$ & $FP$ \\ 
		\cline{2-4}
		& Negative & $FN$ & $TN$ \\ 
		\cline{2-4}
	\end{tabular}
	\caption{Example confusion matrix}
	\label{tab:sample_conf_matrix}
\end{table}

\textcolor{blue}{From the confusion matrix:}

% TODO: index type-II and type-II
\begin{itemize}[noitemsep,topsep=0pt]
	\item \textit{TP (True Positive)}: ``hit'', correct positive prediction. The ground truth is positive and the prediction is positive.
	
	\item \textit{TN (True Negative)}: correct rejection. The ground truth is negative and the prediction is negative.
	
	\item \textit{FP (False Positive)}: False alarm or Type-I error\index{Type I error}. The ground truth is negative, but the prediction is positive.
	
	\item \textit{FN (False Negative)}: Miss or Type-II error\index{Type II error}. The ground truth is positive, but the prediction is negative.
\end{itemize}

\subsubsection{Classification Metrics}

\textcolor{blue}{The below measures of performance are calculated with the indicated equation with values obtained from the confusion matrix XXXXXXXX}


% TODO: these may belong in an appendix
\begin{itemize}[noitemsep,topsep=0pt]
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Accuracy (ACC)}, (Eq.~\ref{eq:accuracy}): the ratio of correct predictions to the total number of predictions. \textcolor{blue}{this is typically the ``go to metric'', however, accuracy may give a false sense of XXXXX and is particularly not very informative if dealing with skewed (unbalanced data) --- see example in \textcolor{red}{local ref?}}

\begin{equation}
{\frac{TP+TN}{TP+TN+FP+FN}}
\label{eq:accuracy}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Misclassification rate}, (Eq.~\ref{eq:misclassification_def}): \textcolor{blue}{the ``opposite'' of accuracy}.

\begin{equation}
{\frac{FP+FN}{TP+TN+FP+FN}}
\label{eq:misclassification_def}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Sensitivity (recall, hit rate, true positive rate (TPR))}, (Eq.~\ref{eq:sensitivity}): the ratio of true positives that are correctly identified.

\begin{equation}
{\frac{TP}{TP+FN}}
\label{eq:sensitivity}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Specificity (true negative rate (TNR))}, (Eq.~\ref{eq:specificity}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{TN}{TN+FP}}
\label{eq:specificity}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Precision (positive predictive value (PPV))}, (Eq.~\ref{eq:precision}): the ratio of positives that are, in fact, positive. If the classifier predicts positive, how often is is correct?

\begin{equation}
{\frac{TP}{TP+FP}}
\label{eq:precision}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Negative Predictive Value (NPV)}, (Eq.~\ref{eq:npv}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{TN}{TN+FN}}
\label{eq:npv}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Miss Rate (False Negative Rate (FNR))}, (Eq.~\ref{eq:miss_rate}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FN}{FN+TP}}
\label{eq:miss_rate}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{False Positive Rate (FPR) (Fall-Out, false alarm rate)}, (Eq.~\ref{eq:fall_out}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FP}{FP+TN}}
\label{eq:fall_out}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{False Discovery Rate (FDR)}, (Eq.~\ref{eq:false_discovery}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FP}{FP+TP}}
\label{eq:false_discovery}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{False Omission Rate (FOR)}, (Eq.~\ref{eq:false_omission}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FN}{FN+TN}}
\label{eq:false_omission}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO: define harmonic mean somewhere
\item \textit{F-1 Score}, (Eq.~\ref{eq:f1_metric}): \textcolor{blue}{F1 is the \textcolor{red}{harmonic mean} of precision and sensitivity XXXXXXXXXX. The F1 score will penalize classifiers more as the difference between the precision and sensitivity increases.}.

\TD{harmonic mean is like taking average, but places emphasis on the lower number}

% F1 = 2 * (precision * recall) / (precision + recall)
% https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html

\begin{equation}
{\frac{2TP}{2TP+FP+FN}}
\label{eq:f1_metric}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Matthews Correlation Coefficient (MCC)}, (Eq.~\ref{eq:mcc_metric}): \textcolor{blue}{MCC is  an alternative to the F1 score for evaluating binary classifiers. MCC is useful even when the ratio of class in the data is severely imbalanced. The output is $[-1,1]$, where 1 would be considered a perfect prediction and -1 an imperfect and 0 being random.}.

\begin{equation}
{\frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}}
\label{eq:mcc_metric}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Informedness (Bookmaker Informedness (BM))}, (Eq.~\ref{eq:informed_metric}): \textcolor{blue}{Informedness is the XXXXXXXXXX}.

\begin{equation}
{\frac{TP}{TP+FN}+\frac{TN}{TN+FP}-1}
\label{eq:informed_metric}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Markedness (MK)}, (Eq.~\ref{eq:markedness_metric}): \textcolor{blue}{Markedness is the XXXXXXXXXX}.

\begin{equation}
{\frac{TP}{TP+FP}+\frac{TN}{TN+FN}-1}
\label{eq:markedness_metric}
\end{equation}


\end{itemize}

\subsubsection{AUC (Area Under the Curve)}

\TD{ROC (Receiver Operating Characteristics) curve --- predicting the probability of a binary outcome}
%TODO: index all metrics

\TD{create visualization for two distributions and create the ROC curve figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\caption{\TD{AUC dists}}
	\label{fig:auc_dist}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\caption{\TD{three AUC curves}}
	\label{fig:auroc_curves}
\end{figure}

\TD{when considering a multi-class (e.g. $n$ class) model, $n$ curves can be produced using a OvR or OvA strategy}

\r{plot of ``positive rates'' --- that is where the False positive rate is on the x-axis and the true positive rate is on the y-axis}

False Positive Rate (FPR) (Fall-Out, false alarm rate)), (Eq.~\ref{eq:fall_out}) [x-axis] vs true positive rate (TPR) (sensitivity, recall, hit rate), (Eq.~\ref{eq:sensitivity}) [y-axis]

\begin{equation}
\textmd{FPR} \textmd{ vs } \textmd{TPR} =	{\frac{FP}{FP+TN}} \textmd{vs} {\frac{TP}{TP+FN}} 
	\label{eq:roc}
\end{equation}

\r{AUC is a single value representing the area under an ROC curve. Though generally referred to as the AUC, the term is correctly abbreviated AUROC, specifying that the curve is an ROC curve. The larger the auROC, the better. Useful metric for summarizing how the model is performing at different thresholds}

\TD{select the best threshold from ROC curve that gives the desired balance between false positives and false negatives.}

\TD{interpretation}
\begin{itemize}[noitemsep,topsep=0pt]
	\item $0$: --- The model is exceptionally bad (but if you flip the output, is it actually exceptionally good), likely something is wrong
	\item $0 - 0.5$: --- If no mistakes are made, the mode is doing worse than random guessing.
	\item $0.5$: --- The model is making random guesses
	\item $0.5 - 1$: --- interpretation here is highly specific to the problem being worked on. The model is doing better than random guessing but less than perfect.
	\item $1$: --- The inverse of the value $0$, the model is exceptionally good, likely something is wrong
\end{itemize}

\r{It's worth noting, that really, the distance from 0.5 is desired. That is a model that produces a $0.1$ is potentially better than a model that produces a $0.6$, that is because if the prediction was ``flipped'', the $0.1$ value would be $0.9$.}

\r{often used as an important metric for binary classification tasks, though isn't necessarily ``the'' metric of interest (\TD{see sec: ref --- sensitivity/precision may be more important})}


\subsubsection{Precision-Recall curve}
% TODO: read https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves

\TD{Diagram}

\r{choice of the threshold to use moving forward}

% https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/
\r{auROC curves may be more informative when there is roughly the same number of classes, whereas PR curves may be more informative when there is a large class imbalance.\cite{davis2006relationship}}

\r{auROC \ALR used}