\section{Data Preparation}

\TD{TODO: direct and indirect importance of datapreparation. indirect: feature engineering allows for more elegant and efficient solutions (e.g. calculating the time with a CNN of a clock face vs engineered features for the angles of the large and small hand.)}

\subsection{Data Pre-processing}

\r{Data is rarely obtained in a form that is necessary for optimal performance of a learning algorithm. Data can be missing, can contain a mix of categorical and quantitative, can contain values on vastly different scales, etc.}

\r{Building a good representation of your data -- feature extraction/engineering}

\r{It is important to note that any parameters related to data pre-processing, such as feature scaling and dimensionality reduction, are obtained solely from observing the training set. The parameters for these methods obtained on the training set are then later applied to the test set. This is important since if these preprocessing parameters were obtained on the entire dataset and included the test set, the model performance may be overoptimistic since then when applying the methods to the unseen data.}

\r{make sure to use data that is relevant to the objective/problem.}

% TODO: case study example

\r{make sure the data can be known at prediction time. e.g. will there be a delay between activity and being able to access that data at the data repository?}

% TODO: case study example

% TODO: expand
\r{features should be numeric and have a meaningful magnitude (see below). In addition to being meaningful, the magnitude should be on a similar (small) range. e.g. if some data has one feature on a range 70,000-500,000 (estimated home value) and another feature on a 0-5 range (maybe number of bathrooms in a house), the higher values, in addition to being viewed as more meaningful (higher weights), large gradient updates may result that don't allow the network to converge (the updates are not fine grained enough).}

% TODO: case study example

\r{pre-processing may be used to reduce the dimensionality of the data, before training.}

\r{make sure ``enough'' examples exist for the particular feature}

\r{Don't mix magic values with values you already have. For example, if we have ratings 0-10, don't include a new column for rated vs not rated.}

\subsection{Handling Missing Data}

\subsubsection{Filtering Out}

\textcolor{blue}{Simply removing any entries that are missing data. This is convenient and easy but may not be practical -- any time data is being removed, potentially useful information is lost and too much data may be removed.}

\textcolor{green}{TODO: Code in jupyter on how to do this with pandas and dropna -- key params - how, thresh, subset}

\subsubsection{Filling In}

\r{Estimating the missing data}

\r{Imputation, interpolation, More detailed imputation information can be found in \ref{appendix:imputing_missing_values}}


\subsubsection{Handling Categorical Data}

\r{cardinality -- levels}

\r{unknown category}

\r{continuous -- binning}

% TODO: rare category
\TD{including an additional category for a rare category, a category that acts as a catch-all (meaning, it may actually be composed of many categories), but was not originally seen in the data at the time of preprocessing. This category may also be called or considered an unknown category.}

\paragraph{Encoding}

\TD{storage as a sparse datatype. -- \TD{example showing the storage saving}}

\textcolor{blue}{It may seem intuitive to represent categorical data with an integer value. However, this may not always be best. Let's pretend we're representing United States with arbitrary integer values and we assign values alphabetically; Alabama: 0, Alaska: 1, Arizona: 2, Arkansas: 3, California: 4, ..., Wisconsin: 48, Wyoming: 49. So far so good. \textcolor{red}{However, these values indirectly imply a relationship (that may not actually exist)} -- and imply that some states are more similar to one another than others (for instance, this encoding seemingly indicates that Alabama as most related to Alaska.) }

\r{{one-hot encoding}\index{one-hot encoding} or one-of-k encoding\index{one-of-k encoding} is a method of encoding which represents each explanatory variable as a binary feature}

\r{one-hot encoding reduces the relationship}

\TD{TODO: show example of one hot encoding}

\r{will lead to {sparse vectors}\index{sparse vectors} -- high dimensional vectors. This is memory intensive (some libraries have methods to address this --- SciPy and Pandas).}

\subparagraph{binarization}

%TODO: index
\TD{rather than using one hot encoding, features could be binarized.}


\subparagraph{Target Encoding}

\TD{convert a categorical value to the mean target variable associated with the given category (calculating the mean from the training split).}

\subsubsection{Feature Scaling, Normalization}

% TODO: this section needs to be restructured to present ideas in a logical flow/building on one another and the cost(time)/benefit

\textcolor{blue}{Ensuring variables exist on a similar scale and variance is important. If one variable is orders of magnitude larger than others, the variable may dominate the learning algorithm and prevent influence from the other variables.}

\textcolor{blue}{Generally would like to get each feature into the $-1 \le X_i \le 1$ or the $-0.5 \le X_i \le 0.5$ range.}

\textcolor{blue}{"OK" if not exact, different people have different rules of thumb for when it is necessary to scale a feature into this range.}

\textcolor{blue}{Additionally, some learning algorithms converge more slowly.}

% hard clipping/capping

% log scalling -- good for when the data has a huge range

% TODO: Give an example


\paragraph{Mean Normalization}

\r{feature $x$ is replaced by $x - \mu$ to create a zero mean}

% TODO: 2D figure showing a circle vs oval contour plot and how gradient descent may take longer on the "oval"

\paragraph{Min-Max scaling (Normalization)}

\textcolor{blue}{values are shifted and rescaled so they end up on a [0,1] range}

\paragraph{Standardization}

\textcolor{blue}{(Eq.~\ref{eq:preprocess_standardization}) first, subtract the sample mean, then divide by standard deviation variance}

\textcolor{blue}{pros: unlike min-max, not bound to specific range}

\textcolor{blue}{standardized values always have a zero mean and unit variance, a standard deviation of 1.}

\textcolor{blue}{gives our data the property of a standard normal distribution}

\begin{equation}
{X' = \frac{X - \mu}{\sigma}}
\label{eq:preprocess_standardization}
\end{equation}

\textcolor{green}{TODO: create code sample - numpy, and sklearn methods}

\subparagraph{Robust Scaler}

\textcolor{blue}{In order to reduce the effect of large outliers, a \textcolor{blue}{RobustScalar} may be used. Rather than subtract the mean and divide by the standard deviation, the median is subtracted and then the data is divided by the {interquartile range}\index{interquartile range} -- \textcolor{red}{see local ref?}}


\subsubsection{Others}

\paragraph{Removing Duplicates}

\paragraph{Outliers}

\textcolor{blue}{rather than remove outliers, the values may be capped.}
% TODO: example, in the California housing data, there are datapoints with 50 rooms per house

\paragraph{Discretization and Binning}

\r{An example for this may be housing prices and latitude.}

% ? tf.feature_column.bucketized_column

\r{rather than store a floating point, bins could be created. would like to keep the information (latitude may be useful, but the magnitude here is not useful (even if normalized) and so binning may be a great option)}

\r{treat numerical features as categorical.}

% TODO: figure for latitude (housing prices) and values pre/post binning

\subsubsection{Where to do preprocessing}

\textcolor{blue}{1. at execution time/with the TF graph}
\textcolor{blue}{2. apache beam ``in front'' of the graph (can use time windows)}
\textcolor{blue}{3. }



\textcolor{blue}{Batch vs Streaming}

% calculating rolling average

\subsubsection{Feature Engineering}

\r{feature engineering is the process of altering the values of the data such that the problem becomes easier to solve as a result of expressing the data in a simplier/more relevant way. this usually required domain knowledge.}

\r{Any transformation (normalization, creation of new features, or alteration) of a feature may be considered feature engineering.}

\TD{TODO: p.102 of Keras book fchollete uses a clock example that is very nice.}

\TD{Date and time transformations}

\TD{aggregation}

\TD{statistics --- mean, median, max, min, unique, skew}

\subsection{Data Types}

% TODO: think this through a bit...


\subsection{Imagery}

\subsection{Time series}

% TODO: https://github.com/blue-yonder/tsfresh

\TD{upsamping and downsampling -- often to match other sampling frequency, but may be for any number of reasons. When upsampling, this can often be framed as a ``missing data'' problem \ALR}

\TD{smoothing --- \TD{exponential smoothing}}

 \TD{smoothing--- Holt's method (with a trend), Holt-Winters smoothing (with a trend and seasonality)}
 
 \TD{other smoothing techniques, more computationally expensive --- Kalman filters, LOESS (locally estimated scater plot smoothing)}
 
 \TD{Seasonality -- a recurring trend in which the frequency is predictable/stable. cyclical is used to describe cycles in which the frequency is unpredicable, that is the cycles will have an uncertain duration.}
 
 % TODO: smoothing the first values

\subsection{Text data}


