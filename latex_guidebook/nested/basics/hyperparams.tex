\section{Hyper-Parameters}

\TD{A disciplined approach to neural network hyper-parameters: Part 1 \cite{DBLP:journals/corr/abs-1803-09820}}


\subsection{Parameters: "tuning knobs"}

\subsubsection{Learning Rate}
\label{hp_learning_rate}

\TD{TODO: Learning rate overview}

% TODO: Learning rate practical advice

% TODO: figure showing cost vs iteration for a LR that is too small, just right, and too large

% TODO: Learning rate figure showing how if the learning rate is too high, you'll likely see the cost diverage when plotted vs iterations

% TODO: schedules

\r{In general, if the LR is too small, convergence (with something like gradient descent) may be slow.  If LR is too large, then convergence may not occur and the reduction in error may oscillate wildly or may even diverge.}

\TD{The large learning rate phase of deep learning: the catapult mechanism \cite{Lewkowycz2020TheLL}}

\paragraph{Schedule}

\r{Rather than keep the same learning rate during all of training, the learning rate is adjusted during training according to a ``schedule''.}

\paragraph{Descriminative/Differential}

\r{FastAI -- ``discriminative'' however, typically shows up as ``differential'' learning rate. Rather than use the same learning rate for all layers/components, different layers/components use different learning rates. (e.g. use a lower learning rate for pretrained layers and a normal/higher learning rate for the customlayers that follow.)}

\paragraph{research}

% TODO: this section may not belong here - may belong in an "advanced section"

%%%% learning rates
\textcolor{blue}{cyclic learning rate~\cite{smith2017cyclical}}

\textcolor{blue}{sgdr: stochastic gradient descent with restarts~\cite{loshchilov2016sgdr} (SGDR). The learning rate is decreased from the max value along a curve (cosine, shown in Eq.\ref{eq:sgdr_def}, where $n_{max}^i$ and $n_{min}^i$ are ranges for the learning rate, $T_i$ represents epochs, $T_{cur}$ is how many epochs have been performed since the last restart). The authors also suggest making each next cycle longer than the previous cycle by a constant $T_mul$ may be beneficial.}

\r{LR annealing, Cosine anealing ($1/2$ cosine curve)}

% \TD{`differential learning rate'/different learning rates at different levels of the network} blog: https://blog.slavv.com/differential-learning-rates-59eff5209a4f

\begin{equation}
{n_t = n_{min}^i + 1/2(n_{max}^i - n_{min}^i)(1 + cos(\frac{T_{cur}}{T_i}\pi))}
\label{eq:sgdr_def}
\end{equation}

\subsubsection{Batch size}

\textcolor{green}{TODO: batch size overview}

\r{anywhere from a single instance to the entire training set size.}

\textcolor{blue}{optimal batch size is problem dependent}

\textcolor{blue}{TODO: notebook and plots showing how the smoothness is affected when comparing batch sizes of 1 vs 10 vs 20 etc.}

% related to shuffling - the gradients are computed on a batch and so a batch should be representative of the data

%%%%% small batch size
\r{small minibatch sizes (between 2 and 32) may be better than large batch sizes~\cite{masters2018revisiting}.}

\r{``generalization gap'' may not be due to large mini-batches, rather, due to the number of updates made to the system~\cite{hoffer2017train}}

%%%% minibatch
\r{Batch training is almost always slower to converge than on-line/mini-batch training, which is likely due to the fact that on-line/mini-batches learning will follow the error surface, allowing for larger learning rates, and thus faster convergence~\cite{wilson2003general}.}

% incrementing batchsize over time
\r{Increasing the batch size may achieve similar benefits to decaying the learning rate ~\cite{smith2017don} -- which could lead to use of larger batch sizes, reducing the number of parameter updates and therefore reducing training time.}

\r{minibatches use the hardware more efficiently}


\subsection{Hyper-Parameter Optimization}

% \r{opinion: perfomed last to eek out extra performance}



\subsubsection{Coordinate Descent}

All hyper-parameters remain fixed, except for the hyper-parameter of interest. The hyper-parameter of interest is then adjusted such that the validation error is minimized.

\r{bayesian $>$ random $>$ grid}

\subsubsection{Grid Search}

\textcolor{blue}{{Grid search}\index{Grid search} Exhaustive search that trains+evaluates a model for each combination of specified hyperparameter configurations and combinations defined by a Cartesian product of the sets of possible values for each hyperparameter.}

\subsubsection{Randomized Search}

\r{{Randomized search}\index{Randomized search} }

\r{TODO: figure demonstrating difference between grid and randomized search}

\TD{TODO: grid vs random search figure}

\subsubsection{Other Methods: Automated / Model-based Methods}

\textcolor{blue}{See \textcolor{red}{local ref? --- advanced methods and research}}

\paragraph{Bayesian Methods}

\TD{todo:}


