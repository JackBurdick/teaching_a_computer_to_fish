\section{Qualitative Evalutation: Performance Metrics}

% NOTE: the term for this section should be ``performance metrics'' that are metrics related to model performance

% performance metrics are typcially directly related to business goals

% TODO: this para needs to be merged with the prev section and moved to where it is decided it best fits
\emph{Cost} is frequently used interchangeably with loss. Technically, loss refers to the error on a single example and cost is the average of the loss across the entire training set.


\TD{Need to rethink this definition and placement.}
\r{here I'm defining (perhaps inappropriately) metics as not differentiable --- at least without any clever manipulations.  That is all ``losses'' (found in \TD{section}) could be considered metrics, but it is unlikely that the metrics in this section would be used as a loss function (again, without any clever manipulation \TD{ref more later}).}

% % https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213
\TD{BLEU Score\cite{papineni2002bleu}}

\TD{case study on the importance of metrics -- disease example.}


\subsection{discrete}

\r{classification}


\subsubsection{Common Metrics}

\input{./nested/basics/metrics/discrete/common}

% TODO: continuous probability rank score.

\subsection{continuous}

\TD{does this section even belong here --- wouldn't this, by default, belong in the discrete section?}

\r{It is important to note that regression performance metrics must ignore the direction of the error, otherwise the positive and negative errors would cancel each other out and the overall score would appear artificially optimistic \textcolor{blue}{see local figure}. This is typically corrected for by either taking the absolute value or the square of the value. An important consideration will be how severely outliers should be penalized, as a squared component will result in a larger penalization than an absolute value.}

\textcolor{green}{todo: Figure showing $\pm$errors and how direction is important}

\subsubsection{Common Metrics}

 \input{./nested/basics/metrics/continuous/common}



\subsubsection{Additional Metrics}


\paragraph{Linear Evaluation}

\TD{TODO: overview of linear evaluation metrics}

% TODO: index
\r{Coefficient of Determination (R-squared ($R^2$)) quantifies how close the data is to a \textcolor{red}{hyperplane} -- a line in a 2-Dimensional space.}

\TD{Note that it is possible for R-squared to be negative.}


% TODO: really?
\textcolor{red}{Several methods exist to calculate R-squared}

% p42(30) of Mastering ML w/scikit
\textcolor{blue}{Pearson product-moment correlation coefficient (PPMCC), or {Pearson's R}\index{Pearson's R} results in a positive number between 0 and 1.}

\textcolor{blue}{NOTE: R-squared is particularly sensitive to outliers.}

\textcolor{blue}{R-squared can spuriously increase when features are added}

\paragraph{Distance Metrics}

\textcolor{blue}{There are four basic requirements for the distance metric:}

\begin{itemize}
	\item Non-negativity: the value must be greater or equal to 0
	\item Identity: if the distance metric between $a$ and $b$ is zero, the two values must be at the same location
	\item Symmetry: the distance metric from $a$ to $b$ must be the same as the distance metric from $b$ to $a$
	\item Triangular inequality: metric($a$,$b$) $\le$ metric($a$,$c$) $+$ metric($b$,$c$)
\end{itemize}

\textcolor{blue}{When calculating the nearest neighbors the terms \textit{distance} and \textit{similarity} may be used interchangeably -- it is important to keep in mind that though they are the ``same'', they are different terms in that the lowest value for distance is ``best'' and the highest value for similarity is ``best''.}

\textcolor{blue}{The default distance metric is the Euclidean distance}

\textcolor{blue}{both the Euclidean and Manhattan distances are special cases of the Minkowski distance}

% see p184 of FofMLforpred data analytics
\textcolor{green}{TODO: more about the Minkowski distance def here}

\textcolor{blue}{Minkowski-based Euclidean distance -- a straight line between two points (Eq~\ref{eq:euclidean_distance_def})}

\begin{equation}
{\sqrt{\sum_{i=1}^{m}{{(a[i] - b[i])}^2}}}
\label{eq:euclidean_distance_def}
\end{equation}


\textcolor{blue}{Manhattan distance (Eq.~\ref{eq:manhattan_distance_def}) -- may also be called the taxi-cab distance, since it is similar to how a driver would have to drive from one point to another on a grid based road system (\textit{e.g.} like Manhattan).}

\begin{equation}
{\sum_{i=1}^{m}{abs(a[i] - b[i])}}
\label{eq:manhattan_distance_def}
\end{equation}

\textcolor{blue}{When implementing a nearest neighbor using Euclidean distance, the feature space is partitioned into {Voronoi tessellation}\index{Voronoi tessellation}. New points are assigned to a {Voronoi region}\index{Voronoi region}.}

% see p214 of FofMLforpred data analytics
\textcolor{green}{TODO: More about other similarity measures}




\paragraph{Multi-label Classification}

\textit{Intersection over union (IOU)}, (Eq.~\ref{eq:iou_def}): \r{intersection over union, with perfect overlap, the value is equal to 1, with no overlap the value is 0.}.

\TD{show examples}

\begin{equation}
	{\textrm{IOU} = \frac{\hat{y} \cap y)}{\hat{y} \cup y}}
	\label{eq:iou_def}
\end{equation}


\TD{include additional metrics like JI, DC, others}

\TD{Jaccard Similarity}

% p125[113] of Mastering ML with SKL
\TD{Hamming Loss}





\subsubsection{Choosing the ``right'' metrics}

\textcolor{blue}{TODO: paras on choosing the right metrics -- need to consider balance, others}



