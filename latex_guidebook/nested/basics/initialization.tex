\section{Initialization}

\TD{initialization --- how we define/set the initial values of parameters}

\TD{largely focused on neuralnetworks initialization}

\TD{TODO: initialization methods and importance}

\TD{figure showing the importance of initialization strategies for different architectures after \textit{n} layers}

\r{motivated partially by reducing the possibility of exploding or vanishing gradients.}

\TD{AutoInit: Analytic Signal-Preserving Weight Initialization for Neural Networks \cite{Bingham2021AutoInitAS}}

\TD{basic idea: initialize with small random values, typically from uniform or gaussian --- more advanced: hueristics based on characteristics --- motivation: }


\subsection{Parameter types (the initialization of)}

\TD{TODO: different types of parameters may benefit from different strategies}

\paragraph{Weights}

\TD{TODO: fully connected, convolution}

Break symmetry -- two things:
\begin{itemize}[noitemsep,topsep=0pt]
	\item \r{Non-zero}
	\item \r{some diversity}
\end{itemize}

\paragraph{Biases}

% HUGO talk
\TD{initializing with negative values may encourage sparsity?}

\TD{fan in and fan out}

\subsection{Normal Vs Uniform}


\subsection{Strategies}

% TODO: Nice write up: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/
% also possibly useful: https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/
\TD{write up\cite{brownlee2021WeightInit}}

\TD{TODO: strategies overview}


\TD{fixup initialization \cite{zhang2019fixup}}

\TD{LeCun \cite{lecun2012efficient}}

\subsubsection{Glorot or Xavier}

\TD{\cite{glorot2010understanding}}

\r{xavier: derived based on linear activations (which isn't true for modern architectures)}



\subsubsection{he}

\TD{Kaiming initialization \cite{he2015delving}}

\subsubsection{Implementation}

