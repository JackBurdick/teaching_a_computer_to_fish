\chapter{Transfer Learning}

\TD{What is being transferred in transfer learning?~\cite{DBLP:journals/corr/abs-2008-11687}}

% TODO: is this useful?
\TD{On the Theory of Transfer Learning: The Importance of Task Diversity~\cite{DBLP:journals/corr/abs-2006-11650}}

%TODO: read this survey
\TD{A Survey on Deep Transfer Learning \cite{DBLP:journals/corr/abs-1808-01974}}

\TD{A Comprehensive Survey on Transfer Learning~\cite{DBLP:journals/corr/abs-1911-02685}}

\TD{How transferable are features in deep neural networks? \cite{DBLP:journals/corr/YosinskiCBL14}}
\TD{CNN Features off-the-shelf: an Astounding Baseline for Recognition \cite{DBLP:journals/corr/RazavianASC14}}

% TODO: haven't read this one (I don't think), but looks relevant
\TD{Learning and transferring mid-level image representations using convolutional neural networks\cite{oquab2014learning}}
\TD{Pay attention to features, transfer learn faster CNNs\cite{wang2019pay}}

% TODO: is this talked about anywhere else? this is probably the best place for it.

\TD{TODO: transfer learning, using -- explanation}

\TD{tool that may sometimes be efficient way of getting to potentially more accurate approximations, faster. \TD{citations}}

% TODO: index
\r{using parameters or pre-trained components from a model/task for a new model/task.  In practice, this often amounts to running inputs through a network that has been previously trained, and obtaining ``embeddings'' from this model (sometimes at an abitrary layer in the network), and then using these ``embeddings'' as input to train an additional model on the desired task. The process of adapting these components to a new model/task is called fine-tuning}


\TD{Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning~\cite{Evci2022Head2ToeUI}}


\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example layer hierarchy and where/when to transfer/freeze params -- this will be 1-2 figures and include many sub-figures \textcolor{green}{TODO}}
	\label{fig:transfer_learning_subfigs_a}
\end{figure}

\textcolor{green}{{freezing}\index{freezing} parameters or a layer means preventing the parameters from being updated during training. This is often controlled by a parameter called ``trainable''.}

% In relation to transfer learning and freezing, mention the difficulty of propagating updates though a large network

\TD{Scaling Laws for Transfer \cite{DBLP:journals/corr/abs-2102-01293}}

\r{One difficulty of fine tuning is knowing where and by how much to either freeze or learn. That is should you freeze the first $n\%$ of the network, why not $m\%$?. Maybe you should leave the entire network trainable? But if the entire network is trianable, the previously learned (and presumably useful features), may be erased by the updates. Aside from selecting where to make the distinction, the main method used to combat these issues is to modify the learning rate. There are two core methods to adjusting the learning rate to address these issues.}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Learning rate schedule
	\item Layer-wise learning rates
\end{itemize}

\TD{These methods are described in more detail in section ~\ref{hp_learning_rate}}


\TD{Adversarially robust transfer learning \cite{DBLP:journals/corr/abs-1905-08232}}

% TODO: check this paper out
\TD{DT-LET: Deep Transfer Learning by Exploring where to Transfer \cite{Lin2020DTLETDT}}

\subsubsection{Potential downsides of TL}

\TD{biases, attacks}

\TD{A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning \cite{DBLP:journals/corr/abs-1904-04334}}
